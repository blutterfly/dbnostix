{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>Precision Analytics, Powerful Results</p> <p>DBNOSTIX specializes in state-of-the-art analytics and diagnostics, empowering businesses with actionable insights to enhance performance and fuel growth.</p>"},{"location":"index.html#why-choose-us","title":"Why Choose Us?","text":"<ul> <li>Cutting-Edge Technology: Advanced AI-driven solutions for precise analytics.</li> <li>Proven Track Record: Trusted partner delivering measurable business impact.</li> <li>Customized Solutions: Tailored services addressing your unique challenges.</li> </ul>"},{"location":"index.html#our-services","title":"Our Services","text":""},{"location":"index.html#about-us","title":"About Us","text":"<p>DBNOSTIX was founded with the mission to simplify complex data and empower organizations through insightful, real-time diagnostics. Our experienced team leverages state-of-the-art analytics to deliver impactful results, driving efficiency, clarity, and growth.</p>"},{"location":"index.html#contact-us","title":"Contact Us","text":"<p>Ready to leverage powerful analytics for your business? Reach out today:</p> Contact DBNOSTIX \u00a9 2025 DBNOSTIX | All Rights Reserved"},{"location":"algotrade/algotrade.html","title":"Algotrade","text":"<p>A deep dive into algorithmic trading concepts.</p>"},{"location":"algotrade/extract.html","title":"Extract","text":"<p>Techniques and tools for extracting financial data.</p>"},{"location":"algotrade/load.html","title":"Load","text":"<p>Loading financial data into storage systems for further processing.</p>"},{"location":"algotrade/transform.html","title":"Transform","text":"<p>Transforming and cleaning financial data for analysis.</p>"},{"location":"apps/dti.html","title":"Debt To Income","text":"<p>Below is a concise \u201cstarter payload\u201d you can use for a Debt-to-Income calculator. Each key is the dictionary field name the app expects from the user (or will populate after the computation); the value shown is just a placeholder illustrating the data type.</p> <pre><code>dti_payload = {\n    # --- identity / metadata (optional but handy) ---\n    \"user_id\": \"\",                # str \u2013 internal identifier\n    \"first_name\": \"\",             # str \u2013 optional display name\n    \"last_name\": \"\",              # str\n\n    # --- income ---\n    \"gross_monthly_income\": 0.0,  # float \u2013 pre-tax income for the month\n\n    # --- recurring monthly debt payments ---\n    \"monthly_rent_or_mortgage\": 0.0,\n    \"auto_loan_payments\": 0.0,\n    \"student_loan_payments\": 0.0,\n    \"credit_card_payments\": 0.0,  # total of all cards\u2019 minimums\n    \"personal_loan_payments\": 0.0,\n    \"child_support_alimony\": 0.0,\n    \"other_debt_payments\": 0.0,   # catch-all for anything not listed above\n\n    # --- output ---\n    \"dti_ratio\": None             # float \u2013 calculated later (% or decimal)\n}\n</code></pre>"},{"location":"apps/dti.html#why-these-keys","title":"Why these keys?","text":"Group Key(s) Reason Identity <code>user_id</code>, <code>first_name</code>, <code>last_name</code> Lets you cache results per user and show friendly messages. Income <code>gross_monthly_income</code> DTI is based on gross (pre-tax) income; easier for users to supply monthly rather than annual. Debts Each payment category Lenders usually include these items in DTI screens. Splitting them out helps with validation and future analytics (e.g., \u201cwhere do users carry most debt?\u201d). Output <code>dti_ratio</code> Store the computed result in the same dict so downstream steps (e.g., scoring, UI display, DB insert) can treat it as a single object. <p>Feel free to add or drop fields (e.g., separate <code>primary_income</code> and <code>secondary_income</code>, or break out multiple mortgages) as your use-case evolves, but this list will get a basic calculator up and running.</p>"},{"location":"cloud/oracle_cloud.html","title":"Oracle cloud","text":"<p>Here is a concise summary of the article \"How I Set Up a Free Server That I\u2019ll Never Have to Pay For\" by Frost from Medium, along with actionable steps and relevant code blocks:</p>"},{"location":"cloud/oracle_cloud.html#summary","title":"\ud83e\uddfe Summary","text":"<p>Frost explains how to set up a permanently free Linux server using Oracle Cloud's \"Always Free\" tier, useful for hosting websites, running projects, or experimenting.</p>"},{"location":"cloud/oracle_cloud.html#how-to-guide","title":"\u2699\ufe0f How-To Guide","text":""},{"location":"cloud/oracle_cloud.html#step-1-sign-up-on-oracle-cloud","title":"\u2705 Step 1: Sign Up on Oracle Cloud","text":"<ol> <li>Go to https://signup.cloud.oracle.com</li> <li>Fill in your details (name, email, and payment info \u2014 no charges apply for free tier).</li> <li>Complete the signup and log in.</li> </ol>"},{"location":"cloud/oracle_cloud.html#step-2-create-a-free-instance","title":"\u2705 Step 2: Create a Free Instance","text":"<ol> <li> <p>In the Oracle Cloud dashboard:</p> </li> <li> <p>Navigate to Compute &gt; Instances</p> </li> <li>Click \"Create Instance\"</li> <li>Enable the \"Always Free resources only\" option.</li> </ol>"},{"location":"cloud/oracle_cloud.html#step-3-save-your-ssh-keys","title":"\u2705 Step 3: Save Your SSH Keys","text":"<ul> <li>During instance creation, Oracle generates an SSH key pair.</li> <li>Download and save both the public and private keys to your system.</li> </ul>"},{"location":"cloud/oracle_cloud.html#step-4-connect-to-your-server-via-ssh","title":"\u2705 Step 4: Connect to Your Server via SSH","text":"<p>Use this command in your terminal (adjust <code>&lt;key-file-name&gt;</code> and <code>your-public-ip</code> accordingly):</p> <pre><code>ssh -i &lt;key-file-name&gt; ubuntu@your-public-ip\n</code></pre> <p>Example:</p> <pre><code>ssh -i ~/.ssh/oracle_free.pem ubuntu@132.145.75.23\n</code></pre>"},{"location":"cloud/oracle_cloud.html#use-cases","title":"\ud83d\udca1 Use Cases","text":"<p>Once connected, you can:</p> <ul> <li>Host a website or web application</li> <li>Deploy and test code</li> <li>Run databases, backend services, or custom scripts</li> </ul> <p>Here's a Bash automation script to streamline the setup of your Oracle Cloud free server, including SSH config and initial connection. This assumes you're manually creating the instance via the dashboard (Oracle doesn't support full automation of free-tier VM creation via CLI for new accounts).</p>"},{"location":"cloud/oracle_cloud.html#setup_oracle_free_serversh","title":"\ud83d\udee0 <code>setup_oracle_free_server.sh</code>","text":"<pre><code>#!/bin/bash\n\n# Constants (Edit as needed)\nKEY_NAME=\"oracle_free\"\nKEY_DIR=\"$HOME/.ssh\"\nKEY_PATH=\"$KEY_DIR/$KEY_NAME.pem\"\nUSER=\"ubuntu\"\n\n# Prompt for public IP\nread -p \"Enter your Oracle public IP address: \" PUBLIC_IP\n\n# 1. Generate SSH Key (skip if you already downloaded it)\necho \"Ensure your private key is saved as $KEY_PATH\"\nif [ ! -f \"$KEY_PATH\" ]; then\n    echo \"\u274c Private key not found. Please download it from Oracle dashboard.\"\n    exit 1\nfi\n\n# 2. Set correct permissions\nchmod 400 \"$KEY_PATH\"\necho \"\u2705 Key permissions set to 400\"\n\n# 3. Add to SSH config\nSSH_CONFIG=\"$KEY_DIR/config\"\nif ! grep -q \"$PUBLIC_IP\" \"$SSH_CONFIG\" 2&gt;/dev/null; then\n    echo \"\ud83d\udd27 Adding SSH config entry...\"\n    cat &lt;&lt;EOF &gt;&gt; \"$SSH_CONFIG\"\n\nHost oracle-free\n    HostName $PUBLIC_IP\n    User $USER\n    IdentityFile $KEY_PATH\n    IdentitiesOnly yes\nEOF\n    echo \"\u2705 SSH config updated.\"\nelse\n    echo \"\u2139\ufe0f SSH config for this IP already exists.\"\nfi\n\n# 4. Test SSH connection\necho \"\ud83c\udf10 Connecting to Oracle server...\"\nssh oracle-free\n</code></pre>"},{"location":"cloud/oracle_cloud.html#instructions","title":"\ud83d\udd27 Instructions","text":"<ol> <li>Save this as <code>setup_oracle_free_server.sh</code></li> <li>Run it with:</li> </ol> <p><pre><code>chmod +x setup_oracle_free_server.sh\n./setup_oracle_free_server.sh\n</code></pre> 3. Follow the prompts.</p> <p>Great \u2014 here's an enhanced version of the script that, once connected to your Oracle Free Tier server, automatically installs common development tools:</p>"},{"location":"cloud/oracle_cloud.html#setup_oracle_free_serversh-with-auto-install","title":"\ud83d\udd27 <code>setup_oracle_free_server.sh</code> (with auto-install)","text":"<pre><code>#!/bin/bash\n\n# Constants\nKEY_NAME=\"oracle_free\"\nKEY_DIR=\"$HOME/.ssh\"\nKEY_PATH=\"$KEY_DIR/$KEY_NAME.pem\"\nUSER=\"ubuntu\"\n\n# Prompt for public IP\nread -p \"Enter your Oracle public IP address: \" PUBLIC_IP\n\n# Step 1: Ensure private key exists\nif [ ! -f \"$KEY_PATH\" ]; then\n    echo \"\u274c Private key not found at $KEY_PATH. Please download it from Oracle and place it there.\"\n    exit 1\nfi\n\n# Step 2: Set permissions\nchmod 400 \"$KEY_PATH\"\necho \"\u2705 Set permissions for key file\"\n\n# Step 3: Add SSH config for convenience\nSSH_CONFIG=\"$KEY_DIR/config\"\nif ! grep -q \"$PUBLIC_IP\" \"$SSH_CONFIG\" 2&gt;/dev/null; then\n    echo \"\ud83d\udd27 Updating SSH config...\"\n    cat &lt;&lt;EOF &gt;&gt; \"$SSH_CONFIG\"\n\nHost oracle-free\n    HostName $PUBLIC_IP\n    User $USER\n    IdentityFile $KEY_PATH\n    IdentitiesOnly yes\nEOF\n    echo \"\u2705 SSH config updated.\"\nfi\n\n# Step 4: Connect and install tools\necho \"\ud83d\ude80 Connecting and installing packages...\"\n\nssh oracle-free &lt;&lt;'EOF'\necho \"\ud83d\udd27 Updating package index...\"\nsudo apt update -y\n\necho \"\ud83d\udce6 Installing common packages...\"\nsudo apt install -y nginx git ufw fail2ban htop curl\n\necho \"\ud83d\udd25 Configuring UFW firewall...\"\nsudo ufw allow OpenSSH\nsudo ufw allow 'Nginx Full'\nsudo ufw --force enable\n\necho \"\u2705 Setup complete. Installed: nginx, git, ufw, fail2ban, htop, curl\"\nEOF\n</code></pre>"},{"location":"cloud/oracle_cloud.html#what-it-does","title":"\u2705 What It Does","text":"<ul> <li>Checks key and SSH config</li> <li>Connects to the server via SSH</li> <li> <p>Installs:</p> </li> <li> <p><code>nginx</code> (web server)</p> </li> <li><code>git</code> (version control)</li> <li><code>ufw</code> (firewall)</li> <li><code>fail2ban</code> (basic security)</li> <li><code>htop</code> (system monitor)</li> <li><code>curl</code> (network utility)</li> <li>Enables firewall and opens necessary ports</li> </ul> <p>Perfect! Here\u2019s the final version of your setup script \u2014 it not only installs essential packages but also creates a custom Nginx homepage with your name or project information.</p>"},{"location":"cloud/oracle_cloud.html#setup_oracle_free_serversh-full-setup-with-homepage","title":"\ud83d\udda5 <code>setup_oracle_free_server.sh</code> (Full Setup with Homepage)","text":"<pre><code>#!/bin/bash\n\n# Constants\nKEY_NAME=\"oracle_free\"\nKEY_DIR=\"$HOME/.ssh\"\nKEY_PATH=\"$KEY_DIR/$KEY_NAME.pem\"\nUSER=\"ubuntu\"\n\n# Prompt for public IP and homepage title\nread -p \"Enter your Oracle public IP address: \" PUBLIC_IP\nread -p \"Enter a title for your homepage (e.g., 'Welcome to My Server'): \" HOMEPAGE_TITLE\nread -p \"Enter a short description (e.g., 'Hosted on Oracle Cloud Free Tier'): \" HOMEPAGE_DESC\n\n# Step 1: Ensure private key exists\nif [ ! -f \"$KEY_PATH\" ]; then\n    echo \"\u274c Private key not found at $KEY_PATH. Please download it from Oracle and place it there.\"\n    exit 1\nfi\n\n# Step 2: Set permissions\nchmod 400 \"$KEY_PATH\"\necho \"\u2705 Set permissions for key file\"\n\n# Step 3: Add SSH config\nSSH_CONFIG=\"$KEY_DIR/config\"\nif ! grep -q \"$PUBLIC_IP\" \"$SSH_CONFIG\" 2&gt;/dev/null; then\n    echo \"\ud83d\udd27 Adding SSH config entry...\"\n    cat &lt;&lt;EOF &gt;&gt; \"$SSH_CONFIG\"\n\nHost oracle-free\n    HostName $PUBLIC_IP\n    User $USER\n    IdentityFile $KEY_PATH\n    IdentitiesOnly yes\nEOF\n    echo \"\u2705 SSH config updated.\"\nfi\n\n# Step 4: Connect and set up server\necho \"\ud83d\ude80 Connecting to server and configuring...\"\n\nssh oracle-free &lt;&lt;EOF\necho \"\ud83d\udd04 Updating packages...\"\nsudo apt update -y\n\necho \"\ud83d\udce6 Installing tools...\"\nsudo apt install -y nginx git ufw fail2ban htop curl\n\necho \"\ud83d\udd25 Setting up UFW firewall...\"\nsudo ufw allow OpenSSH\nsudo ufw allow 'Nginx Full'\nsudo ufw --force enable\n\necho \"\ud83c\udf10 Creating custom Nginx homepage...\"\nsudo bash -c 'cat &gt; /var/www/html/index.nginx-debian.html &lt;&lt;EOPAGE\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;$HOMEPAGE_TITLE&lt;/title&gt;\n    &lt;style&gt;\n        body { font-family: sans-serif; text-align: center; margin-top: 10%; }\n        h1 { font-size: 2.5em; color: #333; }\n        p { font-size: 1.2em; color: #666; }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;$HOMEPAGE_TITLE&lt;/h1&gt;\n    &lt;p&gt;$HOMEPAGE_DESC&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nEOPAGE'\n\nsudo systemctl reload nginx\necho \"\u2705 Setup complete! Visit http://$PUBLIC_IP\"\nEOF\n</code></pre>"},{"location":"cloud/oracle_cloud.html#result","title":"\ud83e\uddea Result","text":"<p>After you run this script:</p> <ul> <li>SSH is set up and secured.</li> <li>A firewall is configured with necessary rules.</li> <li>Your server has useful packages pre-installed.</li> <li>Your custom homepage is live at <code>http://&lt;your-public-ip&gt;</code></li> </ul>"},{"location":"database/database.html","title":"Database","text":"<p>General overview of databases and their importance.</p>"},{"location":"database/db2.html","title":"Db2","text":"<p>Introduction to IBM Db2 and its features.</p>"},{"location":"database/duckdb.html","title":"Duckdb","text":"<p>Introduction to DuckDB, an in-memory analytical database.</p>"},{"location":"database/duckdb_intro.html","title":"DuckDb Intro","text":"<p>Here\u2019s a summarized breakdown of the key takeaways from the article \u201cIntroducing DuckDB and Python\u201d along with relevant code blocks:</p>"},{"location":"database/duckdb_intro.html#key-takeaways","title":"\ud83d\udd11 Key Takeaways","text":""},{"location":"database/duckdb_intro.html#what-is-duckdb","title":"\u2705 What is DuckDB?","text":"<ul> <li>An in-process analytical SQL database optimized for OLAP.</li> <li>Requires no server, no setup, and works inside Python scripts.</li> <li>Excellent for large datasets (CSV, Parquet, JSON) and memory-efficient querying.</li> </ul>"},{"location":"database/duckdb_intro.html#benefits-over-pandas","title":"\u2705 Benefits Over pandas","text":"<ul> <li>Handles large files without loading into memory.</li> <li>Enables SQL queries on local files and <code>pandas</code> DataFrames.</li> <li>Supports efficient filtering, joins, and write operations directly to disk.</li> </ul>"},{"location":"database/duckdb_intro.html#installation","title":"\u2705 Installation","text":"<pre><code>pip install duckdb\n</code></pre>"},{"location":"database/duckdb_intro.html#example-usage-patterns","title":"\ud83d\udca1 Example Usage Patterns","text":""},{"location":"database/duckdb_intro.html#query-a-csv-file-directly","title":"\ud83d\udccc Query a CSV File Directly","text":"<pre><code>import duckdb\n\nresult = duckdb.query(\"SELECT * FROM 'users.csv' LIMIT 5\").to_df()\nprint(result)\n</code></pre>"},{"location":"database/duckdb_intro.html#join-across-multiple-files","title":"\ud83d\udccc Join Across Multiple Files","text":"<pre><code>query = \"\"\"\nSELECT u.id, u.name, o.total\nFROM 'users.csv' u\nJOIN 'orders.csv' o ON u.id = o.user_id\nWHERE o.total &gt; 100\n\"\"\"\ndf = duckdb.query(query).to_df()\n</code></pre>"},{"location":"database/duckdb_intro.html#query-a-pandas-dataframe","title":"\ud83d\udccc Query a pandas DataFrame","text":"<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"score\": [85, 92, 78]\n})\n\nresult = duckdb.query(\"SELECT name FROM df WHERE score &gt; 80\").to_df()\nprint(result)\n</code></pre>"},{"location":"database/duckdb_intro.html#query-a-parquet-file-with-filter","title":"\ud83d\udccc Query a Parquet File (with filter)","text":"<pre><code>df = duckdb.query(\"SELECT * FROM 'data/2023_logs.parquet' WHERE status = 'error'\").to_df()\n</code></pre>"},{"location":"database/duckdb_intro.html#create-a-sql-view-over-file-paths","title":"\ud83d\udccc Create a SQL View Over File Paths","text":"<pre><code>duckdb.query(\"CREATE VIEW logs AS SELECT * FROM 'data/*.parquet'\")\n</code></pre>"},{"location":"database/duckdb_intro.html#write-query-results-to-disk","title":"\ud83d\udccc Write Query Results to Disk","text":"<pre><code>duckdb.query(\"\"\"\nCOPY (SELECT * FROM 'users.csv' WHERE country = 'US')\nTO 'us_users.csv'\n\"\"\")\n</code></pre>"},{"location":"database/duckdb_intro.html#sql-based-pipeline-in-python","title":"\ud83d\udccc SQL-Based Pipeline in Python","text":"<pre><code>duckdb.query(\"CREATE TEMP TABLE sales AS SELECT * FROM 'sales_2024.csv'\")\n\nduckdb.query(\"\"\"\nCREATE TEMP TABLE filtered AS\nSELECT * FROM sales WHERE amount &gt; 1000\n\"\"\")\n\nresult = duckdb.query(\"SELECT region, COUNT(*) FROM filtered GROUP BY region\").to_df()\n</code></pre>"},{"location":"database/duckdb_intro.html#parameterized-queries","title":"\ud83d\udccc Parameterized Queries","text":"<pre><code>region = 'West'\nresult = duckdb.query(\"SELECT * FROM 'sales.csv' WHERE region = ?\", [region]).to_df()\n</code></pre>"},{"location":"database/duckdb_intro.html#when-not-to-use-duckdb","title":"\ud83d\udeab When Not to Use DuckDB","text":"<ul> <li>For transactional workloads, multi-user access, or relational constraints.</li> <li>Stick with PostgreSQL or MySQL for persistent, transactional applications.</li> </ul> <p>Would you like a comparison cheat sheet between DuckDB, pandas, and SQLite for specific tasks?</p>"},{"location":"database/duckdb_motherduck.html","title":"Duckdb and MotherDuck","text":"<p>Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage</p>"},{"location":"database/duckdb_motherduck.html#overview","title":"Overview","text":"<p>This project explores how to leverage the strengths of DuckDB and MotherDuck to build a robust data processing and storage solution. DuckDB excels at fast in-memory analytics, while MotherDuck provides a scalable and cost-effective cloud data warehouse. By combining these technologies, you can achieve optimal performance for both local and cloud-based data operations.</p>"},{"location":"database/duckdb_motherduck.html#environment-settings","title":"Environment settings","text":"<pre><code>import polars as pl\nimport duckdb as db\nimport glob\nExtraction from data sources\n- csv files\ncsv_files = glob.glob('./datasets/*.csv')\nlist(enumerate(csv_files))\n- json files\njson_files = glob.glob('./datasets/*.json')\nlist(enumerate(json_files))\n- database tables\ndb_files = glob.glob('datasets/*.db')\nlist(enumerate(db_files))\n\nData warehouse creation\nconn = db.connect('my_database.db')\nData warehouse load\nconn.sql(f\"create or replace table water_collection as \n        select * from '{csv_files[0]}' \")\nconn.sql(f\"create or replace table contains_null as\n        select * from '{csv_files[1]}' \")\nconn.sql(f\"create or replace table sales_info as\n        select * from '{csv_files[2]}' \")\nconn.sql(f\"create or replace table cdmx_subway as\n        select * from '{csv_files[3]}' \")\nconn.sql(f\"create or replace table airports as\n        select * from '{csv_files[4]}' \")\nconn.sql(f\"create or replace table colors as\n        select * from '{csv_files[5]}' \")\nconn.sql(f\"create or replace table sets as\n        select * from '{csv_files[6]}' \")\nconn.sql(f\"create or replace table appl_stock as\n        select * from '{csv_files[7]}' \")\nconn.sql(f\"create or replace table sales as\n        select * from '{csv_files[8]}' \")\nconn.sql(f\"create or replace table prevalencia as\n        select * from '{json_files[0]}' \")\nconn.sql(f\"create or replace table people as\n        select * from '{json_files[1]}' \")\nretail = db.connect('./datasets/retail_db.db')\nretail.sql('show tables')\nretail_sales_pl = retail.sql('select * from retail_sales').pl()\nconn.execute(\"create or replace table retail_sales as from retail_sales_pl\");\nrestaurants = db.connect('./datasets/restaurants.db')\nrestaurants.sql('show tables')\nrestaurants_pl = restaurants.sql('select * from restaurants').pl()\nconn.execute(\"create or replace table restaurants as from restaurants_pl\");\n\n# Data retrieval\nconn.sql('show databases')\nconn.sql('show tables')\nconn.sql('select * from restaurants limit 5').pl()\n\n# Cloud Data Warehouse with MotherDuck\ndw = db.connect('md')\ndw.sql('select current_database()').show()\n\n# Convert queries from local database to polars\nairports_pl = conn.sql('select * from airports').pl()\nappl_stock_pl = conn.sql('select * from appl_stock').pl()\ncdmx_subway_pl = conn.sql('select * from cdmx_subway').pl()\ncolors_pl = conn.sql('select * from colors').pl()\ncontains_null_pl = conn.sql('select * from contains_null').pl()\npeople_pl = conn.sql('select * from people').pl()\nprevalencia_pl = conn.sql('select * from prevalencia').pl()\nrestaurants_pl = conn.sql('select * from restaurants').pl()\nretail_sales_pl = conn.sql('select * from retail_sales').pl()\nsales_pl = conn.sql('select * from sales').pl()\nsales_info_pl = conn.sql('select * from sales_info').pl()\nsets_pl = conn.sql('select * from sets').pl()\nwater_collection_pl = conn.sql('select * from water_collection').pl()\n\n# Upload dataframes to MotherDuck\ndw.sql(f\"create or replace table airports as select * from airports_pl\");\ndw.sql(f\"create or replace table appl_stock as\n      select * from appl_stock_pl\");\ndw.sql(f\"create or replace table cdmx_subway as\n      select * from cdmx_subway_pl\");\ndw.sql(f\"create or replace table colors as select * from colors_pl\");\ndw.sql(f\"create or replace table contains_null as\n      select * from contains_null_pl\");\ndw.sql(f\"create or replace table people as select * from people_pl\");\ndw.sql(f\"create or replace table prevalencia as\n      select * from prevalencia_pl\");\ndw.sql(f\"create or replace table restaurants as\n      select * from restaurants_pl\");\ndw.sql(f\"create or replace table retail_sales as\n      select * from retail_sales_pl\");\ndw.sql(f\"create or replace table sales as\n      select * from sales_pl\");\ndw.sql(f\"create or replace table sales_info as\n      select * from sales_info_pl\");\ndw.sql(f\"create or replace table sets as\n      select * from sets_pl\");\ndw.sql(f\"create or replace table water_collection as\n      select * from water_collection_pl\");\n# Check uploaded tables\ndw.sql('show tables')\n\nClose all database connections\nconn.close()\nretail.close()\nrestaurants.close()\ndw.close()\n</code></pre>"},{"location":"database/duckdb_motherduck.html#conclusions","title":"Conclusions","text":"<p>By combining DuckDB\u2019s in-memory processing capabilities with MotherDuck\u2019s cloud-based data warehousing, you can create a powerful and flexible data processing and storage solution. This approach allows you to efficiently handle both local and cloud-based data operations, optimize performance, and scale your data infrastructure as needed.</p>"},{"location":"database/postgresql.html","title":"Postgresql","text":"<p>A guide to PostgreSQL and its use cases.</p>"},{"location":"dbnostix/about.html","title":"About","text":"<p>Information about the Dbnostix platform, its history, and goals.</p>"},{"location":"dbnostix/contact.html","title":"Contact","text":"<p>Contact details and ways to reach out for inquiries and support.</p>"},{"location":"dbnostix/contents.html","title":"Contents","text":"<p>An overview of Dbnostix, its purpose, and key features.</p> <p>See top navigation bar above for a list of topics covered.</p>"},{"location":"dbnostix/help.html","title":"Help","text":""},{"location":"dbnostix/services.html","title":"Services","text":"<p>A summary of services offered by Dbnostix.</p>"},{"location":"gpt/00_gpt.html","title":"Contents","text":"<p>Generative Pre-trained Transformer</p> <ul> <li>GPT</li> <li>MCP</li> <li>LLM</li> <li>Offline LLM</li> </ul>"},{"location":"gpt/01_llm_mcp.html","title":"LLM and MCP","text":""},{"location":"gpt/01_llm_mcp.html#explain","title":"Explain","text":""},{"location":"gpt/01_llm_mcp.html#1-what-is-a-large-language-model-llm","title":"1. What is a Large Language Model (LLM)?","text":"Aspect Key points Definition An LLM is a neural-network language model trained\u2014largely with self-supervision\u2014on trillions of tokens so it can predict the next token in a sequence. The dominant family today are generative-pre-trained transformers (GPT-type) models. (en.wikipedia.org) How it works \u2460 Transformer layers use self-attention to weigh every token against every other token, letting the model capture long-range dependencies. \u2461 During pre-training it learns statistical patterns of language; during in-context learning it follows instructions or solves tasks given in the prompt. (techradar.com) Scale &amp; \u201cChinchilla\u201d law Empirical scaling laws show that performance improves roughly as a power law with respect to parameter count, training tokens, and compute\u2014hence today\u2019s multi-hundred-billion-parameter systems. (en.wikipedia.org) Typical capabilities Text generation, summarization, classification, information extraction, code completion, multi-modal reasoning (when fine-tuned with images/audio), and tool invocation (\u201cfunction calling\u201d). Strengths Few-shot adaptability, broad world knowledge, multilingual ability, rapid deployment via APIs rather than bespoke ML pipelines. Limitations \u2022 \u201cHallucination\u201d (confident but false statements) \u2022 Training-data bias \u2022 High inference cost and latency \u2022 Context-window limits \u2022 Need for guard-rails and alignment techniques such as RLHF or constitutional AI. (en.wikipedia.org, arxiv.org)"},{"location":"gpt/01_llm_mcp.html#2-what-is-the-model-context-protocol-mcp","title":"2. What is the Model Context Protocol (MCP)?","text":"Aspect Key points Purpose MCP is an open, JSON-RPC 2.0\u2013based application-layer protocol that standardises how LLMs (or \u201cagent\u201d clients) request structured context and execute tool calls against external systems\u2014databases, SaaS APIs, local files, code repos, etc. Think of it as \u201cUSB-C for AI apps.\u201d (en.wikipedia.org, arstechnica.com) Origins &amp; Timeline Introduced by Anthropic in Nov 25 2024 and open-sourced the same day; since March\u2013April 2025 OpenAI, Google, and Microsoft have officially announced support, making it the de-facto industry standard for agentic integrations. (theverge.com, techcrunch.com, techcrunch.com) Core components \u2022 MCP Client (usually the LLM runtime or an orchestrator library) sends JSON-RPC requests such as <code>list_tools</code>, <code>call_tool</code>, or <code>get_context</code>.\u2022 MCP Server wraps the data/tool source and returns context chunks, tool manifests, or results.\u2022 Schema: all messages follow a versioned JSON schema with explicit auth scopes and token budgeting metadata. (en.wikipedia.org) Typical workflow 1\ufe0f\u20e3 LLM receives a user prompt (\u201cSummarise this policy PDF\u201d). 2\ufe0f\u20e3 It calls <code>list_tools</code>; the server advertises a \u201cfile-reader\u201d tool. 3\ufe0f\u20e3 LLM issues <code>call_tool</code> with the file path, gets the text back, and produces the answer\u2014all inside one conversational turn. Benefits Uniformity (one integration covers many models), security (fine-grained permissions instead of full file-system access), streamlined agent chaining (the same protocol passes intermediate results between multiple specialised tools), and ecosystem momentum (SDKs in Python, TypeScript, Rust). Challenges \u2022 Still evolving spec (v0.6 as of June 2025) \u2022 Governance and version-compatibility \u2022 Need for robust auth/secret management \u2022 Potential vendor-specific extensions if not carefully standardised."},{"location":"gpt/01_llm_mcp.html#3-how-llm-mcp-fit-together","title":"3. How LLM + MCP fit together","text":"<ol> <li>LLM alone is great at reasoning over the text you give it, but it\u2019s stateless and blind to private data or live systems.</li> <li>MCP bridges that gap\u2014the model can now pull \u201ccontext blocks\u201d or trigger functions in a consistent, auditable way.</li> <li> <p>This unlocks agentic patterns such as:</p> </li> <li> <p>Customer-support bots that pull order data from an ERP via an MCP server.</p> </li> <li>Programming copilots that read current project files, run unit tests, and commit code.</li> <li>Knowledge workers letting the assistant query DuckDB, generate a chart, then draft a summary email\u2014all mediated through one protocol stack.</li> </ol>"},{"location":"gpt/01_llm_mcp.html#4-why-it-matters-to-practitioners","title":"4. Why it matters to practitioners","text":"<p>For developers\u2002\u2192\u2002write one MCP wrapper around your data/tool and instantly make it accessible to multiple LLM vendors; avoid bespoke plug-ins. For companies\u2002\u2192\u2002enforce consistent governance: every tool call is logged, permissioned, and revocable. For researchers\u2002\u2192\u2002simplifies benchmarking of tool-augmented reasoning by abstracting away bespoke glue code.</p> <p>In short, LLMs supply the reasoning engine; MCP supplies the safe connector. Together they enable the next wave of AI systems that don\u2019t just talk about helping\u2014you can let them act on your real data with confidence.</p>"},{"location":"gpt/20_mcp_ollama_build.html","title":"Build LLM and MCP","text":"<p>\u201cOllama + MCP Servers from Scratch\u201d, including key code blocks:</p>"},{"location":"gpt/20_mcp_ollama_build.html#step-by-step-implementation","title":"\u2705 Step-by-step Implementation","text":""},{"location":"gpt/20_mcp_ollama_build.html#1-install-dependencies","title":"\ud83e\uddf1 1. Install Dependencies","text":"<pre><code>uv add fastmcp ollama\n</code></pre>"},{"location":"gpt/20_mcp_ollama_build.html#2-project-structure","title":"\ud83d\udcc1 2. Project Structure","text":"<pre><code>your-folder/\n\u251c\u2500\u2500 server.py      # defines the MCP server &amp; tool\n\u2514\u2500\u2500 client.py      # launches server &amp; connects Ollama\n</code></pre>"},{"location":"gpt/20_mcp_ollama_build.html#3-create-mcp-server","title":"\ud83d\udee0 3. Create MCP Server","text":"<pre><code># server.py\nfrom fastmcp import FastMCP\n\nmcp = FastMCP(\"TestServer\")\n\n@mcp.tool()\ndef magicoutput(obj1: str, obj2: str) -&gt; int:\n    return \"WomboWombat\"\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre>"},{"location":"gpt/20_mcp_ollama_build.html#4-start-connect-client","title":"\ud83d\udd0c 4. Start &amp; Connect Client","text":"<pre><code># client.py\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nimport asyncio, threading\nfrom pathlib import Path\n\nclass OllamaMCP:\n    def __init__(self, server_params):\n        self.server_params = server_params\n        self.tools = []\n        self.initialized = threading.Event()\n\n    def _run_background(self):\n        asyncio.run(self._async_run())\n\n    async def _async_run(self):\n        async with stdio_client(self.server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                await session.initialize()\n                tools_result = await session.list_tools()\n                self.tools = tools_result.tools\n                self.initialized.set()\n\nif __name__ == \"__main__\":\n    server_params = StdioServerParameters(\n        command=\"uv\", args=[\"run\", \"python\", \"server.py\"], cwd=str(Path.cwd())\n    )\n    client = OllamaMCP(server_params)\n    client._run_background()\n</code></pre>"},{"location":"gpt/20_mcp_ollama_build.html#5-convert-tools-to-pydantic-models","title":"\ud83d\udce6 5. Convert Tools to Pydantic Models","text":"<pre><code>from pydantic import BaseModel, create_model, Field\n\ndef convert_json_type(json_type):\n    return {\n        \"string\": (str, ...),\n        \"integer\": (int, ...),\n        \"number\": (float, ...),\n        \"boolean\": (bool, ...)\n    }.get(json_type, (str, ...))\n\ndef create_response_model(tools):\n    dynamic_models = {}\n    for tool in tools:\n        fields = {\n            name: convert_json_type(info.get(\"type\", \"string\"))\n            for name, info in tool.inputSchema[\"properties\"].items()\n        }\n        dynamic_models[tool.name.capitalize()] = create_model(tool.name.capitalize(), **fields)\n\n    # Combine all tool models into one\n    AllTools = Union[tuple(dynamic_models.values())]\n    return create_model(\"Response\", response=(str, ...), tool=(Optional[AllTools], Field(None)))\n\n# Usage\nclient.response_model = create_response_model(client.tools)\n</code></pre>"},{"location":"gpt/20_mcp_ollama_build.html#6-background-thread-queues-for-calling-tools","title":"\ud83e\uddf5 6. Background Thread + Queues for Calling Tools","text":"<pre><code>import queue\n\nclass OllamaMCP:\n    ...\n    def __init__(...):\n        ...\n        self.request_queue = queue.Queue()\n        self.response_queue = queue.Queue()\n        self.thread = threading.Thread(target=self._run_background)\n        self.thread.start()\n\n    def call_tool(self, name, args):\n        self.request_queue.put((name, args))\n        return self.response_queue.get()\n\n    async def _async_run(...):\n        ...\n        while True:\n            name, args = self.request_queue.get()\n            if name is None: break\n            result = await session.call_tool(name, args)\n            self.response_queue.put(result)\n</code></pre>"},{"location":"gpt/20_mcp_ollama_build.html#7-send-messages-to-ollama-and-trigger-tool-execution","title":"\ud83e\udd16 7. Send Messages to Ollama and Trigger Tool Execution","text":"<pre><code>from ollama import chat\n\nasync def ollama_chat(self, messages):\n    format_schema = self.response_model.model_json_schema()\n    response = chat(model=\"gemma3:latest\", messages=messages, format=format_schema)\n\n    obj = self.response_model.model_validate_json(response.message.content)\n    if obj.tool:\n        func_name = obj.tool.__class__.__name__.lower()\n        func_args = obj.tool.model_dump()\n        return await asyncio.to_thread(self.call_tool, func_name, func_args)\n    return obj.response\n</code></pre>"},{"location":"gpt/20_mcp_ollama_build.html#8-main-runner","title":"\ud83c\udfc1 8. Main Runner","text":"<pre><code>async def main():\n    ...\n    session = OllamaMCP(params)\n    session.create_response_model()\n    messages = [\n        {\"role\": \"system\", \"content\": \"Use the available tools to respond.\"},\n        {\"role\": \"user\", \"content\": \"Get the magic output for Wombat and Dog.\"}\n    ]\n    result = await session.ollama_chat(messages)\n    print(\"Result:\", result)\n    session.shutdown()\n</code></pre> <p>Would you like this scaffold bundled into a downloadable repo or ZIP?</p>"},{"location":"gpt/20_mcp_server_build.html","title":"Build MCP Server","text":"<p>Summarized step-by-step guide from the article \"How to build MCP server with Authentication in Python using FastAPI\" by Miki Makhlevich, including key code blocks:</p>"},{"location":"gpt/20_mcp_server_build.html#steps","title":"Steps","text":""},{"location":"gpt/20_mcp_server_build.html#step-0-install-required-dependencies","title":"\u2705 Step 0: Install Required Dependencies","text":"<pre><code>pip install uvicorn fastapi fastapi-mcp\n</code></pre>"},{"location":"gpt/20_mcp_server_build.html#step-1-create-a-basic-fastapi-app","title":"\u2705 Step 1: Create a Basic FastAPI App","text":"<pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"MCP is super cool\"}\n</code></pre> <p>Run the server:</p> <pre><code>uvicorn main:app --reload\n</code></pre>"},{"location":"gpt/20_mcp_server_build.html#step-2-mount-the-mcp-server","title":"\u2705 Step 2: Mount the MCP Server","text":"<pre><code>from fastapi import FastAPI\nfrom fastapi_mcp import FastApiMCP\n\napp = FastAPI()\nmcp = FastApiMCP(app)\nmcp.mount()\n</code></pre> <p>MCP will now be accessible at: <code>http://127.0.0.1:8000/mcp</code></p>"},{"location":"gpt/20_mcp_server_build.html#step-3-add-authentication","title":"\u2705 Step 3: Add Authentication","text":""},{"location":"gpt/20_mcp_server_build.html#option-1-bearer-token-in-authorization-header","title":"\ud83d\udd10 Option 1: Bearer Token in Authorization Header","text":"<p>Example CLI call with token:</p> <pre><code>{\n  \"mcpServers\": {\n    \"remote-example\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"http://localhost:8000/mcp\",\n        \"--header\",\n        \"Authorization:${AUTH_HEADER}\"\n      ]\n    },\n    \"env\": {\n      \"AUTH_HEADER\": \"Bearer &lt;your-token&gt;\"\n    }\n  }\n}\n</code></pre> <p>Protecting with FastAPI-MCP:</p> <pre><code>from fastapi import FastAPI, Depends\nfrom fastapi.security import HTTPBearer\nfrom fastapi_mcp import FastApiMCP, AuthConfig\n\ntoken_auth_scheme = HTTPBearer()\napp = FastAPI()\n\nmcp = FastApiMCP(\n    app,\n    name=\"Protected MCP\",\n    auth_config=AuthConfig(\n        dependencies=[Depends(token_auth_scheme)],\n    ),\n)\nmcp.mount()\n</code></pre>"},{"location":"gpt/20_mcp_server_build.html#option-2-full-oauth-with-auth0-recommended","title":"\ud83d\udd10 Option 2: Full OAuth with Auth0 (Recommended)","text":"<p>Step 1: Create <code>.env</code></p> <pre><code>AUTH0_DOMAIN=your-tenant.auth0.com\nAUTH0_AUDIENCE=https://your-tenant.auth0.com/api/v2/\nAUTH0_CLIENT_ID=your-client-id\nAUTH0_CLIENT_SECRET=your-client-secret\n</code></pre> <p>Step 2: Load settings</p> <pre><code>from pydantic_settings import BaseSettings\n\nclass Settings(BaseSettings):\n    auth0_domain: str\n    auth0_audience: str\n    auth0_client_id: str\n    auth0_client_secret: str\n\n    class Config:\n        env_file = \".env\"\n\nsettings = Settings()\n</code></pre> <p>Step 3: Get and convert JWK public key</p> <pre><code>import jwt\nimport requests\nfrom cryptography.hazmat.primitives import serialization\n\ndef _get_public_key(url: str):\n    response = requests.get(url)\n    jwks = response.json()\n    return jwks[\"keys\"][0]\n\ndef _convert_key_to_pem(jwk_key) -&gt; str:\n    public_key = jwt.algorithms.RSAAlgorithm.from_jwk(jwk_key)\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo,\n    )\n    return pem.decode(\"utf-8\")\n\njwks_key = _get_public_key(f\"https://{settings.auth0_domain}/.well-known/jwks.json\")\npem_key = _convert_key_to_pem(jwks_key)\n</code></pre> <p>Step 4: Create token verifier</p> <pre><code>async def verify_auth(request: Request) -&gt; dict[str, Any]:\n    return jwt.decode(\n        token,\n        pem_key,\n        algorithms=[\"RS256\", \"HS256\"],\n        audience=settings.auth0_audience,\n        issuer=f\"https://{settings.auth0_domain}/\",\n        options={\"verify_signature\": True},\n    )\n</code></pre> <p>Step 5: Secure MCP with OAuth2</p> <pre><code>from fastapi import Depends\nfrom fastapi_mcp import FastApiMCP, AuthConfig\n\nmcp = FastApiMCP(\n    app,\n    name=\"MCP With Auth0\",\n    auth_config=AuthConfig(\n        issuer=f\"https://{settings.auth0_domain}/\",\n        authorize_url=f\"https://{settings.auth0_domain}/authorize\",\n        oauth_metadata_url=f\"https://{settings.auth0_domain}/.well-known/openid-configuration\",\n        audience=settings.auth0_audience,\n        client_id=settings.auth0_client_id,\n        client_secret=settings.auth0_client_secret,\n        dependencies=[Depends(verify_auth)],\n        setup_proxies=True,\n    ),\n)\nmcp.mount()\n</code></pre>"},{"location":"mkdocs/admonitions.html","title":"Admonitions","text":"<p>In MkDocs Material, callouts are called Admonitions. You can implement them easily by following these steps:</p>"},{"location":"mkdocs/admonitions.html#topics","title":"Topics","text":""},{"location":"mkdocs/admonitions.html#modify-mkdocsyml","title":"Modify mkdocs.yml","text":"<p>To enable adminitions, ensure that your Material theme supports admonitions by adding these lines to your configuration file:</p> <pre><code>markdown_extensions:\n  - admonition\n  - pymdownx.details\n  - pymdownx.superfences\n</code></pre>"},{"location":"mkdocs/admonitions.html#syntax-for-callouts","title":"Syntax for Callouts","text":"<p>Here's a basic example:</p> <pre><code>!!! note \"Optional title\"\n    This is a simple note callout.\n</code></pre>"},{"location":"mkdocs/admonitions.html#types-of-admonitions","title":"Types of Admonitions","text":"<p>You can use various predefined types:</p> <p>Info</p> <ul> <li>note</li> <li>info</li> <li>tip</li> <li>warning</li> <li>danger</li> <li>success</li> <li>question</li> <li>example</li> <li>quote</li> <li>bug</li> </ul>"},{"location":"mkdocs/admonitions.html#examples","title":"Examples","text":"<pre><code>!!! info \"Important Information\"\n    Please read carefully before proceeding.\n</code></pre> <p>Important Information</p> <p>Please read carefully before proceeding.</p> <pre><code>!!! warning \"Be Careful!\"\n    You might lose your changes if you proceed without saving.\n</code></pre> <p>Be Careful!</p> <p>You might lose your changes if you proceed without saving.</p> <pre><code>??? tip \"Collapsible tip\"\n    This content is hidden until expanded by the user.\n</code></pre> Collapsible tip <p>This content is hidden until expanded by the user.</p>"},{"location":"mkdocs/collapsible_sections.html","title":"Collapsible Sections","text":"<p>How to create collapsible sections in MkDocs.</p>"},{"location":"mkdocs/emoji_guide.html","title":"Emoji Guide","text":"<p>ChatGpt</p>"},{"location":"mkdocs/mkdocs.html","title":"Overview","text":"<p>Getting started with MkDocs for documentation.</p>"},{"location":"options/contents.html","title":"Options","text":"<p>Collection of strategy, how-to and python script.</p>"},{"location":"options/strangle_strategy.html","title":"Strangle Strategy","text":""},{"location":"options/strangle_strategy.html#what-is","title":"What is","text":"<p>A Strangle is an options trading strategy that involves buying or selling both a call option and a put option with the same expiration date but different strike prices. It is used to take advantage of expected volatility in the underlying asset.</p>"},{"location":"options/strangle_strategy.html#option-strangle-strategy","title":"Option Strangle Strategy","text":""},{"location":"options/strangle_strategy.html#types-of-strangle-strategies","title":"Types of Strangle Strategies","text":"<ol> <li>Long Strangle (Buying a Strangle)</li> <li>Buy an out-of-the-money (OTM) call option.</li> <li>Buy an out-of-the-money (OTM) put option.</li> <li> <p>Used when a trader expects high volatility but is unsure of the direction.</p> </li> <li> <p>Short Strangle (Selling a Strangle)</p> </li> <li>Sell an out-of-the-money (OTM) call option.</li> <li>Sell an out-of-the-money (OTM) put option.</li> <li>Used when a trader expects low volatility and wants to collect premium.</li> </ol>"},{"location":"options/strangle_strategy.html#long-strangle-strategy","title":"Long Strangle Strategy","text":"<ul> <li>Objective: Profit from a significant price move in either direction.</li> <li>Max Loss: Limited to the total premium paid.</li> <li>Max Profit: Unlimited (if the stock moves significantly beyond the strike prices).</li> <li>Breakeven Points:</li> <li>Upper BEP = Call strike price + Premium paid.</li> <li>Lower BEP = Put strike price - Premium paid.</li> </ul> <p>\u2705 Best for: Volatile markets, earnings reports, major news events.</p> <p>\ud83d\udd34 Risk: If the stock remains within the strike prices, the options expire worthless.</p>"},{"location":"options/strangle_strategy.html#short-strangle-strategy","title":"Short Strangle Strategy","text":"<ul> <li>Objective: Profit from low volatility and time decay.</li> <li>Max Profit: Limited to the premium collected.</li> <li>Max Loss: Potentially unlimited if the stock moves sharply.</li> <li>Breakeven Points:</li> <li>Upper BEP = Call strike price + Premium received.</li> <li>Lower BEP = Put strike price - Premium received.</li> </ul> <p>\u2705 Best for: Stable or low-volatility markets.</p> <p>\ud83d\udd34 Risk: Large losses if the stock price moves sharply in either direction.</p>"},{"location":"options/strangle_strategy.html#example-of-a-long-strangle","title":"Example of a Long Strangle","text":"<p>Stock: XYZ trading at $100  </p> <ul> <li>Buy 110 Call at $2.00</li> <li>Buy 90 Put at $2.00  </li> <li>Total Cost: $4.00</li> </ul> <p>Breakeven Points:</p> <ul> <li>Upper BEP = $110 + $4 = $114</li> <li>Lower BEP = $90 - $4 = $86</li> </ul> <p>Profit Scenarios:</p> <ul> <li>If stock moves above $114, the call option gains value.</li> <li>If stock moves below $86, the put option gains value.</li> <li>If stock stays between $90 and $110, both options expire worthless, and you lose the $4 premium.</li> </ul>"},{"location":"options/strangle_strategy.html#example-of-a-short-strangle","title":"Example of a Short Strangle","text":"<p>Stock: XYZ trading at $100  </p> <ul> <li>Sell 110 Call for $2.00</li> <li>Sell 90 Put for $2.00  </li> <li>Total Premium Collected: $4.00</li> </ul> <p>Breakeven Points:</p> <ul> <li>Upper BEP = $110 + $4 = $114</li> <li>Lower BEP = $90 - $4 = $86</li> </ul> <p>Profit Scenarios:</p> <ul> <li>If stock stays between $90 and $110, both options expire worthless, and you keep the premium.</li> <li>If stock moves above $114 or below $86, you face unlimited risk.</li> </ul>"},{"location":"options/strangle_strategy.html#key-takeaways","title":"Key Takeaways","text":"<p>\u2714\ufe0f Long Strangle: Limited risk, unlimited profit potential, best for high volatility. \u2714\ufe0f Short Strangle: Limited profit, unlimited risk, best for low volatility. \u2714\ufe0f Breakeven Points: Critical to manage risk and set exit points. \u2714\ufe0f Implied Volatility (IV): Higher IV makes long strangles more expensive, while lower IV makes short strangles more profitable.  </p>"},{"location":"options/strangle_strategy.html#script-to-backtest-this-strategy","title":"Script to backtest this strategy \ud83d\ude80","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport yfinance as yf\n\n# Define parameters for the strangle strategy\nticker = \"AAPL\"\nstart_date = \"2023-01-01\"\nend_date = \"2024-01-01\"\nstrike_diff = 5  # Difference from ATM for Call and Put\npremium_paid = 4  # Estimated cost of buying both options\npremium_received = 4  # Estimated premium collected for short strangle\n\n# Fetch historical stock data\ndata = yf.download(ticker, start=start_date, end=end_date)\ndata[\"MidPrice\"] = (data[\"High\"] + data[\"Low\"]) / 2\n\n# Define breakeven points\ndata[\"Upper_BEP_Long\"] = data[\"MidPrice\"] + strike_diff + premium_paid\ndata[\"Lower_BEP_Long\"] = data[\"MidPrice\"] - strike_diff - premium_paid\ndata[\"Upper_BEP_Short\"] = data[\"MidPrice\"] + strike_diff + premium_received\ndata[\"Lower_BEP_Short\"] = data[\"MidPrice\"] - strike_diff - premium_received\n\n# Calculate long strangle profit/loss\ndata[\"Long_Strangle_PL\"] = np.where(\n    (data[\"MidPrice\"] &gt; data[\"Upper_BEP_Long\"]) | (data[\"MidPrice\"] &lt; data[\"Lower_BEP_Long\"]),\n    abs(data[\"MidPrice\"] - data[\"Upper_BEP_Long\"]) - premium_paid,\n    -premium_paid\n)\n\n# Calculate short strangle profit/loss\ndata[\"Short_Strangle_PL\"] = np.where(\n    (data[\"MidPrice\"] &gt; data[\"Upper_BEP_Short\"]) | (data[\"MidPrice\"] &lt; data[\"Lower_BEP_Short\"]),\n    -abs(data[\"MidPrice\"] - data[\"Upper_BEP_Short\"]) + premium_received,\n    premium_received\n)\n\n\n# Display first few rows using pprint\npprint(data[[\"MidPrice\", \"Long_Strangle_PL\", \"Short_Strangle_PL\"]].head())\n</code></pre>"},{"location":"pandas/advanced_techniques.html","title":"Advanced Techniques","text":"<p>Advanced Pandas Techniques for Data Processing and Performance</p>"},{"location":"pandas/advanced_techniques.html#1-data-chunking","title":"1. Data chunking","text":"<p>There are often scenarios we encounter where the size of the data is more than the available memory (RAM) we have. In such cases, it\u2019s a good idea to read data in chunks from the file so that the system doesn\u2019t run out of memory.</p> <pre><code>1 # Initialize an empty list to store the chunks\n2 private_rooms = []\n3\n4 # Read the CSV file in chunks\n5 for chunk in pd.read_csv('data/listings.csv', chunksize=1000):\n6     # Process each chunk (for example, filter listings where room_type is \"Private room\")\n7     processed_chunk = chunk[chunk[\"room_type\"] == \"Private room\"]\n8     \n9     # Append the processed chunk to the list\n10     private_rooms.append(processed_chunk)\n11\n12 # Combine all processed chunks into a dataframe\n13 private_rooms = pd.concat(private_rooms)\n</code></pre>"},{"location":"pandas/advanced_techniques.html#2-progress-bar","title":"2. Progress bar","text":"<p>Pandas\u2019 apply() functions, which are commonly used to perform element- wise operations on DataFrame columns or rows.</p> <pre><code>1 import time\n2 from datetime import datetime\n3\n4 from tqdm import tqdm\n5 tqdm.pandas()  # add this to integrate progress bar functionality to pandas\n6\n7\n8 def convert_to_datetime(date):\n9     # return date if type of date is not string\n10     if type(date) != str:\n11         return date\n12     \n13     # time.sleep() added to demonstrate the progress bar more effectively \n14     time.sleep(0.1)\n15\n16     # returns a datetime object\n17     return datetime.strptime(date, \"%Y-%m-%d\")\n18\n19 private_rooms[\"last_review\"] = private_rooms[\"last_review\"].progress_apply(convert_to_datetime)\n</code></pre>"},{"location":"pandas/advanced_techniques.html#3-populate-multiple-columns","title":"3. Populate Multiple Columns","text":"<p>Solution: apply() with result_type=\"expand\"</p> <pre><code>1 def get_date_and_month_name(last_review_date):\n2     # returns the day and month\n3     return last_review_date.day_name(), last_review_date.month_name()\n4\n5\n6 private_rooms[[\"Day\", \"Month\"]] = private_rooms.apply(\n7     lambda x : get_date_and_month_name(x[\"last_review\"]),\n8     axis=1,\n9     result_type=\"expand\"\n10 )\n</code></pre>"},{"location":"pandas/advanced_techniques.html#4-parallel-processing","title":"4. Parallel Processing","text":"<p>Use multiprocessing module.  (using np.array_split()) into multiple chunks (depending on the number of cores available in your system) Once the chunks are processed and the desired output for each chunk is obtained, it can be concatenated back to a single DataFrame.</p> <pre><code>1 import time\n2 import random\n3 from multiprocessing import Pool\n4\n5 import numpy as np\n6 import pandas as pd\n7 from tqdm import tqdm\n8\n9 tqdm.pandas()\n10\n11\n12 def predict_sentiment(review):\n13     time.sleep(0.1)  # simulating time taken by prediction model\n14     return \"Positive\" if random.randint(1,10) &gt; 5 else \"Negative\"\n15\n16\n17 def batch_predict_sentiment(review_df):\n18     review_df[\"sentiment\"] = review_df[\"comments\"].progress_apply(predict_sentiment)\n19     return review_df\n20\n21\n22 def fetch_sentiment_for_review():\n23     n_cores = 64\n24     reviews = pd.read_csv(\"data/reviews.csv\")\n25     review_batches = np.array_split(reviews, n_cores)  # split into same number of batches as n_c\n26     \n27     # Processing Parallely\n28     with Pool(n_cores) as pool:\n29         sentiment_prediction_batches = pool.map(batch_predict_sentiment, review_batches)\n30\n31     # Once all the batches are processed, concatenate list of DataFrames into a single DataFrame\n32     sentiment_prediction = pd.concat(sentiment_prediction_batches)\n33     return sentiment_prediction\n34\n35\n36 reviews_with_sentiment = fetch_sentiment_for_review()\n</code></pre> <p>From more than 13 hours to less than 13 minutes! Each batch consists of 7521 reviews and there are a total of 64 batches. In this scenario</p> <p>Tip: set n_cores more than the actual number of cores my system has.  This is because during the execution of time.sleep(0.1) the CPU remains idle and each process interleaves for other process to execute. If your process is CPU intensive, it is recommended to keep n_cores less than the actual number of cores your system has.</p>"},{"location":"pandas/advanced_techniques.html#5-complex-merging","title":"5. Complex Merging","text":"<p>Merging is quite a common operation performed by individuals who deal with data. However, sometimes it can get quite complicated to understand if any particular data points were lost during the merging process. It might be due to a plethora of reasons \u2014 the worst one being, malformed or faulty data.</p> <p>The indicator=True.  When enabled it creates a new column named _merge which can denote three different scenarios based on the type of merge operation performed.</p> <ul> <li>left_only \u2014 indicates that the row\u2019s key only exists in the left DataFrame and it couldn\u2019t find a match in the right DataFrame</li> <li>right_only \u2014 indicates that the row\u2019s key only exists in the right</li> <li>both \u2014 indicates that the row\u2019s key exists in both the DataFrames and data</li> </ul> <pre><code>merged_df = listings.merge(\n    reviews,\n    left_on=\"id\",\n    right_on=\"listing_id\",\n    how=\"outer\",\n    indicator=True\n)\nmerged_df.shape\n</code></pre>"},{"location":"pandas/advanced_techniques.html#6-data-segmentation","title":"6. Data Segmentation","text":"<p>pd.cut() is a powerful function that can be used when you need to segment data into multiple bins. It can also act as a way to convert continuous values into categorical values. One such scenario is demonstrated in the example below. We will be Segmenting the price of each listing into multiple price brackets (bins). We can set a predetermined number of price brackets \u2014 \u201c$0 \u2014 $100\u201d, \u201c$101 \u2014 $250\u201d, \u201c$251 \u2014 $500\u201d, \u201c$500 \u2014 $1000\u201d, and \u201c$1000+\u201d.</p> <p>Create bins and label for each bin <pre><code>bins = [0, 100, 250, 500, 1000, float('inf')]\nlabels = [\"$0 - $100\", \"$101 - $250\", \"$251 - $500\", \"$500 - $1000\", \"$1000+\"]\nlistings[\"price_bucket\"] = pd.cut(listings[\"price\"], bins=bins, labels=labels)\n</code></pre></p> <p>Please note here that the number of labels (5) is less than number of bins (6) by one. This is because the initial two values in the bin belong to the first label.</p>"},{"location":"pandas/advanced_techniques.html#7-cross-tabulation-analysis","title":"7. Cross-Tabulation Analysis","text":"<p>Using the above data, we can go one step further and perform a cross- tabulation between the price brackets and room types available for those price brackets. Here\u2019s where pd.crosstab() comes into play. It can be used to perform a simple cross-tabulation between price_bucket and room_type.</p> <pre><code>room_type_breakup = pd.crosstab(\n    listings[\"price_bucket\"],\n    listings[\"room_type\"],\n    margins=True  # This will add the row and column totals\n)\n</code></pre>"},{"location":"pandas/cheat_sheet.html","title":"Cheatsheet","text":""},{"location":"pandas/cheat_sheet.html#selecting-and-filtering-data","title":"Selecting and Filtering Data","text":"<p>Selecting and filtering data are fundamental operations when working with Pandas DataFrames. Pandas provides multiple methods for selecting specific rows and columns based on labels, positions, and conditions.</p>"},{"location":"pandas/cheat_sheet.html#1-selecting-data","title":"1. Selecting Data","text":""},{"location":"pandas/cheat_sheet.html#11-selecting-columns","title":"1.1 Selecting Columns","text":"<ul> <li>Using column names:</li> </ul> <pre><code>import pandas as pd\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    \"Age\": [25, 30, 35, 40],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n})\n\n# Select a single column\nprint(df[\"Name\"])\n\n# Select multiple columns\nprint(df[[\"Name\", \"City\"]])\n</code></pre>"},{"location":"pandas/cheat_sheet.html#12-selecting-rows","title":"1.2 Selecting Rows","text":"<ul> <li>Using <code>.loc[]</code> (label-based indexing):</li> </ul> <pre><code># Select a row by index label\nprint(df.loc[0])  # First row\n</code></pre> <ul> <li>Using <code>.iloc[]</code> (position-based indexing):</li> </ul> <pre><code># Select a row by position\nprint(df.iloc[2])  # Third row\n</code></pre> <ul> <li>Selecting multiple rows:</li> </ul> <pre><code>print(df.loc[0:2])  # Select rows 0 to 2\nprint(df.iloc[0:2])  # Select first two rows\n</code></pre>"},{"location":"pandas/cheat_sheet.html#2-filtering-data","title":"2. Filtering Data","text":"<p>Filtering is used to extract subsets of a DataFrame that meet specific conditions.</p>"},{"location":"pandas/cheat_sheet.html#21-filtering-rows-based-on-condition","title":"2.1 Filtering Rows Based on Condition","text":"<pre><code># Select rows where Age &gt; 30\nprint(df[df[\"Age\"] &gt; 30])\n</code></pre>"},{"location":"pandas/cheat_sheet.html#22-filtering-with-multiple-conditions","title":"2.2 Filtering with Multiple Conditions","text":"<p>Using <code>&amp;</code> (AND condition):</p> <pre><code># Select rows where Age &gt; 30 and City is \"Chicago\"\nprint(df[(df[\"Age\"] &gt; 30) &amp; (df[\"City\"] == \"Chicago\")])\n</code></pre> <p>Using <code>|</code> (OR condition):</p> <pre><code># Select rows where Age &lt; 30 or City is \"Houston\"\nprint(df[(df[\"Age\"] &lt; 30) | (df[\"City\"] == \"Houston\")])\n</code></pre> <p>Using <code>.isin()</code> for filtering specific values:</p> <pre><code># Select rows where City is in [\"New York\", \"Houston\"]\nprint(df[df[\"City\"].isin([\"New York\", \"Houston\"])])\n</code></pre> <p>Using <code>.between()</code> for range-based filtering:</p> <pre><code># Select rows where Age is between 25 and 35\nprint(df[df[\"Age\"].between(25, 35)])\n</code></pre> <p>Using <code>.query()</code> for SQL-like filtering:</p> <pre><code>print(df.query(\"Age &gt; 30 and City == 'Chicago'\"))\n</code></pre>"},{"location":"pandas/cheat_sheet.html#3-selecting-specific-rows-and-columns","title":"3. Selecting Specific Rows and Columns","text":"<pre><code># Select specific rows and columns\nprint(df.loc[df[\"Age\"] &gt; 30, [\"Name\", \"City\"]])\n</code></pre> <pre><code># Select first two rows and first two columns\nprint(df.iloc[0:2, 0:2])\n</code></pre>"},{"location":"pandas/cheat_sheet.html#mini-tutorial-filtering-a-real-dataset","title":"Mini Tutorial: Filtering a Real Dataset","text":"<p>Let's assume we have a dataset <code>data.csv</code> with employee information:</p>"},{"location":"pandas/cheat_sheet.html#step-1-load-the-dataset","title":"Step 1: Load the Dataset","text":"<pre><code>df = pd.read_csv(\"data.csv\")\n</code></pre>"},{"location":"pandas/cheat_sheet.html#step-2-explore-the-data","title":"Step 2: Explore the Data","text":"<pre><code>print(df.head())  # View the first few rows\nprint(df.info())  # Get summary information\n</code></pre>"},{"location":"pandas/cheat_sheet.html#step-3-apply-filtering","title":"Step 3: Apply Filtering","text":"<pre><code># Employees older than 40\nolder_employees = df[df[\"Age\"] &gt; 40]\n\n# Employees in the IT department\nit_employees = df[df[\"Department\"] == \"IT\"]\n\n# Employees with salary between 50K and 100K\nmid_salary_employees = df[df[\"Salary\"].between(50000, 100000)]\n\n# IT employees in New York\nny_it_employees = df[(df[\"Department\"] == \"IT\") &amp; (df[\"City\"] == \"New York\")]\n</code></pre>"},{"location":"pandas/cheat_sheet.html#step-4-save-the-filtered-data","title":"Step 4: Save the Filtered Data","text":"<pre><code>ny_it_employees.to_csv(\"filtered_data.csv\", index=False)\n</code></pre>"},{"location":"pandas/cheat_sheet.html#conclusion","title":"Conclusion","text":"<ul> <li>Use <code>.loc[]</code> and <code>.iloc[]</code> for row and column selection.</li> <li>Use conditional filtering with boolean operators (<code>&amp;</code>, <code>|</code>).</li> <li>Use <code>.query()</code> for SQL-like filtering.</li> <li>Use <code>.isin()</code> and <code>.between()</code> for specialized filtering.</li> </ul> <p>Would you like an example using PandasGUI for interactive filtering? \ud83d\ude80</p>"},{"location":"pandas/contents.html","title":"Contents","text":"<p>For data scientists, pandas is the most important python module in their repertoire.</p> <p>In this workspace,  the following topics will be covered:</p> <ul> <li>Cheatsheet</li> <li>Selecting and Filtering</li> </ul>"},{"location":"pandas/dataframe_creation.html","title":"Dataframe creation","text":""},{"location":"pandas/dataframe_creation.html#code-examples","title":"Code Examples","text":"<pre><code># A simple dataframe  with Name, Age, and City columns.\nimport pandas as pd\ndf = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\"],\n    \"Age\": [25, 30],\n    \"City\": [\"New York\", \"Los Angeles\"]\n})\nprint(df)\n\n# List of dictionaries\ndata = [\n    {\"Name\": \"Charlie\", \"Age\": 35, \"City\": \"Chicago\"},\n    {\"Name\": \"David\", \"Age\": 40, \"City\": \"Miami\"}\n]\ndf = pd.DataFrame(data)\nprint(df)\n\n# Converts a list of dictionaries to a DataFrame.\ndf = pd.DataFrame({\n    \"Name\": [\"Eve\", \"Frank\"],\n    \"Age\": [28, 34],\n    \"City\": [\"Boston\", \"Seattle\"]\n}, index=[\"Row1\", \"Row2\"])\nprint(df)\n\n# Uses custom labels for rows instead of default numeric index.\nCreating DataFrames\ndf_list = pd.DataFrame([\n    [\"Grace\", 29],\n    [\"Henry\", 32]\n], columns=[\"Name\", \"Age\"])\nprint(df_list)\n\n# Uses separate lists for each row and specifies column labels.\ndata_dict = {\"Name\": [\"Ivy\", \"Jack\"], \"Age\": [24, 36]}\ndf_dict = pd.DataFrame(data_dict)\nprint(df_dict)\n</code></pre>"},{"location":"pandas/dataframe_creation.html#importing-data-from-csv-files","title":"Importing data from CSV files","text":"<p>the transition from file-based storage to in-memory data manipulation efficiently. This method is straightforward and essential for handling large datasets typically stored in CSV format. <pre><code>df_csv = pd.read_csv(\"data.csv\")\nprint(df_csv)\n</code></pre></p>"},{"location":"pandas/dataframe_creation.html#search","title":"Search","text":"<pre><code>import pandas as pd Open in app\n# Create a DataFrame\ndf = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n})\n# Use .loc to select by label\nfirst_row = df.loc[0]  # Select the first row\nspecific_cell = df.loc[1, 'Age']  # Access a specific cell\nprint(\"First Row:\\n\", first_row)\nprint(\"Specific Cell (Row 1, Age):\", specific_cell)\n# Use .iloc to select by position\nfirst_column = df.iloc[:, 0]  # Select the first column\nspecific_rows = df.iloc[0:2]  # Retrieve the first two rows\nprint(\"First Column:\\n\", first_column)\nprint(\"Specific Rows:\\n\", specific_rows)\n# Apply boolean indexing to filter data\nadults = df[df['Age'] &gt; 25]  # Filter rows where age is greater than 25\nlos_angeles_residents = df[df['City'] == \"Los Angeles\"]\nprint(\"Adults:\\n\", adults)\nprint(\"Los Angeles Residents:\\n\", los_angeles_residents)\n</code></pre>"},{"location":"pandas/dataframe_creation.html#modifying-dataframes","title":"Modifying DataFrames","text":"<pre><code># Add a new column 'Height' to the DataFrame\ndf['Height'] = [165, 180, 175]  # Heights corresponding to each person\nprint(\"DataFrame with Height:\\n\", df)\n# Remove the 'City' column from the DataFrame\ndf = df.drop('City', axis=1)\nprint(\"DataFrame without City:\\n\", df)\n# Updating specific cell values in the DataFrame\ndf.loc[0, 'Age'] = 26\ndf.iloc[2, 1] = 36  # Update Charlie's age using integer position\nprint(\"Updated DataFrame:\\n\", df)\n</code></pre>"},{"location":"pandas/dataframe_creation.html#data-cleaning","title":"Data Cleaning","text":"<p>Example 1: Filling NaN with Mean</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Age': [25, np.nan, 30]}\ndf = pd.DataFrame(data)\n# Fill NaN values in 'Age' with the mean of the column\ndf['Age'] = df['Age'].fillna(df['Age'].mean())\nprint(df)\n</code></pre> <p>Example 2: Removing Rows with NaN</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Age': [25, np.nan, 30],\n        'City': [np.nan, 'New York', 'Los Angeles']}\ndf = pd.DataFrame(data)\n# Drop rows where any element is NaN\ndf_clean = df.dropna()\nprint(df_clean)\n</code></pre> <p>Example 3: Filling NaN with Forward Fill Method</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Age': [25, np.nan, np.nan],\n        'City': ['New York', np.nan, 'Los Angeles']}\ndf = pd.DataFrame(data)\n# Forward fill NaN values in the DataFrame\ndf.ffill(inplace=True)\nprint(df)\n</code></pre> <p>Example 4: Converting String to Integer <pre><code>import pandas as pd\n# Sample DataFrame\ndata = {'ID': ['101', '102', '103'],\n        'Amount': ['1000', '1500', '1200']}\ndf = pd.DataFrame(data)\n# Convert 'Amount' from string to integer\ndf['Amount'] = df['Amount'].astype(int)\nprint(df.dtypes)\n</code></pre></p> <p>Example 5: Converting Integer to Float <pre><code>import pandas as pd\n# Sample DataFrame\ndata = {'Product': ['A', 'B', 'C'],\n        'Price': [200, 150, 300]}\ndf = pd.DataFrame(data)\n# Convert 'Price' from int to float\ndf['Price'] = df['Price'].astype(float)\nprint(df.dtypes)\n</code></pre></p> <p>Example 6: Handling Conversion Errors</p> <pre><code>import pandas as pd\n# Sample DataFrame\ndata = {'Year': ['2020', 'NaN', '2021'],\n        'Value': ['100', '200.5', '300']}\ndf = pd.DataFrame(data)\ntry:\n    # Attempt to convert 'Year' to int\n    df['Year'] = df['Year'].astype(int)\nexcept ValueError:\n    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\nprint(df)\n</code></pre>"},{"location":"pandas/dataframe_tips.html","title":"Dataframe Tips","text":"<p>Pandas is one of the most popular Python libraries for data manipulation and analysis. Use these tips for efficiency</p>"},{"location":"pandas/dataframe_tips.html#1-use-at-and-iat","title":"1. Use .at and .iat","text":"<p>Accessing or modifying individual values in a DataFrame is something you\u2019ll do often. While .loc and .iloc are the most common methods, they aren\u2019t always the fastest. </p> <p>For single-value access, .at (label-based) and .iat (integer position-based) are optimized for speed. This can make a big difference in large datasets.</p> <pre><code>import pandas as pd\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n\n# Accessing a value\nvalue = df.at[0, 'A']  # Faster than df.loc[0, 'A']\n\n# Modifying a value\ndf.iat[1, 1] = 10      # Faster than df.iloc[1, 1] = 10\n</code></pre> <p>In simple terms: Use .at when you know the column and row labels, and .iat when you\u2019re working with index positions.</p>"},{"location":"pandas/dataframe_tips.html#2-use-query","title":"2. Use .query","text":"<p>Filtering rows with conditions is a daily task. While traditional boolean indexing works, it can get messy, especially with multiple conditions. Enter the query method, which lets you filter rows using SQL-like syntax for readability.</p> <pre><code>df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n\n# Instead of this\nfiltered = df[(df['A'] &gt; 1) &amp; (df['B'] &lt; 6)]\n\n# Use query for cleaner code\nfiltered = df.query('A &gt; 1 &amp; B &lt; 6')\n</code></pre> <p>When your filtering logic gets more complex, query makes your code much easier to read and maintain.</p>"},{"location":"pandas/dataframe_tips.html#3-use-vectorized-operations","title":"3. Use vectorized operations","text":"<p>Loops feel natural to use, but they\u2019re inefficient when working with Pandas. The library is built for vectorized operations, which are faster because they\u2019re executed in C under the hood. If you\u2019re using for loops or apply for simple column-wise calculations, you\u2019re doing it the hard way.</p> <p><pre><code># Slow loop\ndf['C'] = [a + b for a, b in zip(df['A'], df['B'])]\n\n# Fast vectorized operation\ndf['C'] = df['A'] + df['B']\n</code></pre> Not only is the vectorized approach faster, but it\u2019s also more readable. Think of your DataFrame as a single entity rather than individual rows and columns.</p>"},{"location":"pandas/dataframe_tips.html#4-use-assign-to-chain-operations","title":"4. Use .assign to chain operations","text":"<p>Data cleaning often involves multiple transformations. Instead of performing one operation, assigning it back to the DataFrame, and repeating, you can use .assign to chain them together. This makes your code more elegant and less error-prone.</p> <pre><code>df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n# Add or modify columns in one step\ndf = df.assign(\n    D=df['A'] * 2,\n    E=lambda x: x['B'] + 3\n)\n</code></pre> <p>Now you can add or modify multiple columns without interrupting the workflow. This is especially useful when chaining multiple operations together.</p>"},{"location":"pandas/dataframe_tips.html#5-use-map-and-applymap","title":"5. Use .map and .applymap","text":"<p>If you\u2019re transforming a single column, use .map instead of apply. It\u2019s faster and more concise. For element-wise transformations across an entire DataFrame, use .applymap. Reserve apply for row-wise or column-wise operations.</p> <pre><code># Use map for single columns\ndf['A'] = df['A'].map(lambda x: x * 2)\n# Use applymap for element-wise operations across the DataFrame\ndf = df.applymap(lambda x: x * 2)\n</code></pre> <p>This keeps your code focused and avoids unnecessary complexity.</p>"},{"location":"pandas/dataframe_tips.html#6-use-to_dict","title":"6. Use .to_dict","text":"<p>When working with APIs or exporting data, you\u2019ll often need to convert a DataFrame to a dictionary. Instead of writing custom code, use Pandas\u2019 built- in to_dict method. Choosing the correct orient argument can save you a lot of headaches.</p> <pre><code># Convert rows to a list of dictionaries\ndata_dict = df.to_dict(orient='records')\n</code></pre> <p>This method is perfect for JSON-style outputs, where each dictionary represents a row. It\u2019s a lifesaver when integrating with external systems.</p>"},{"location":"pandas/dataframe_tips.html#7-use-groupby","title":"7. Use .groupby","text":"<p>The .groupby function is a powerhouse for summarizing data, but did you know you can combine it with .agg to perform multiple custom aggregations at once? This makes it easy to produce complex summaries in just a few lines.</p> <pre><code>df = pd.DataFrame({'A': ['foo', 'foo', 'bar'], 'B': [1, 2, 3], 'C': [4, 5, 6]})\n# Group by column 'A' and compute custom aggregations\nresult = df.groupby('A').agg(\n    sum_B=('B', 'sum'),    # Sum of column B\n    mean_C=('C', 'mean')   # Mean of column C\n)\n</code></pre> <p>Instead of chaining multiple .groupby and aggregation calls, this allows you</p>"},{"location":"pandas/etl.html","title":"Etl","text":"<p>Open in app Search Write Five Common Python Libraries for ETL Processing Gen. David L. \u00b7 Follow 5 min read \u00b7 Dec 10, 2024 130 3 ETL is a critical concept in data warehousing technology, representing three main processing steps: Extract, Transform, and Load. ETL is commonly used for data integration, merging data from various sources into a unified view for analysis and reporting. ETL Detailed Process 1. Extract (E): This step involves extracting data from various sources, which could include relational databases, file systems, cloud storage, or APIs. The extraction process may involve copying or moving data and recognizing and parsing its format. 2. Transform (T): Once the data is extracted, it usually needs to be transformed into a format suitable for analysis. This process may include: Data cleansing (e.g., removing duplicates, correcting errors, and resolving inconsistencies). Data aggregation (e.g., calculating sums, averages, maximum/minimum values). Data normalization (e.g., converting data into a unified format or unit). Data enrichment (e.g., adding additional contextual information). The transformation step is the most critical part of ETL as it directly affects the quality and usability of the data in the final data warehouse. 3. Load (L): The loading step involves writing the transformed data into the target system, which could be a data warehouse, data lake, data mart, or another form of data storage. During this step, the data may be indexed, partitioned, or archived to optimize query performance and storage efficiency. ETL processes can be either batch-based or real-time. Batch ETL typically runs at scheduled intervals, such as daily, weekly, or monthly, whereas real- time ETL requires the system to continuously process and update data. Common Python ETL Libraries 1. Pandas Advantages: Pandas provides data structures and analytical tools that are highly suitable for data cleaning and transformation. It simplifies data manipulation tasks in the ETL process, such as adding new columns and filtering data. Disadvantages: Pandas may encounter performance bottlenecks when handling large datasets, as it primarily operates on in-memory data. Use Cases: Suitable for small to medium-sized datasets and for rapid prototyping. Code Example: import pandas as pd</p>"},{"location":"pandas/etl.html#read-data-from-a-csv-file","title":"Read data from a CSV file","text":"<p>df = pd.read_csv('my_example.csv')</p>"},{"location":"pandas/etl.html#data-transformation-such-as-adding-a-new-column","title":"Data transformation, such as adding a new column","text":"<p>df['new_A_column'] = df['existing_A_column'].apply(lambda x: x * 5.4)</p>"},{"location":"pandas/etl.html#save-the-transformed-data-to-a-new-csv-file","title":"Save the transformed data to a new CSV file","text":"<p>df.to_csv('transformed_example.csv', index=False) 2. Apache Spark (PySpark) Advantages: Apache Spark is a unified analytics engine designed for large- scale data processing. The PySpark API makes it simple to handle Spark jobs within Python workflows. Disadvantages: Compared to Pandas, Spark has a steeper learning curve and requires more resources for setup and operation. Use Cases: Ideal for processing large datasets, especially in distributed computing environments. Code Example: from pyspark.sql import SparkSession</p>"},{"location":"pandas/etl.html#initialize-a-spark-session","title":"Initialize a Spark session","text":"<p>spark = SparkSession.builder.appName('etl_example').getOrCreate()</p>"},{"location":"pandas/etl.html#read-data-from-a-csv-file_1","title":"Read data from a CSV file","text":"<p>df = spark.read.csv('input_example_data.csv', header=True, inferSchema=True)</p>"},{"location":"pandas/etl.html#data-transformation","title":"Data transformation","text":"<p>df = df.withColumn('new_A_column', df['existing_A_column'] * 5.4)</p>"},{"location":"pandas/etl.html#write-the-data-to-a-new-csv-file","title":"Write the data to a new CSV file","text":"<p>df.write.csv('output_example_data.csv') 3. Haul Advantages: Haul is a lightweight and modular Python library designed specifically for ETL tasks. It focuses on simplifying data extraction, transformation, and loading by providing clean, reusable components that integrate seamlessly into modern workflows. Disadvantages: Haul is relatively new and less feature-rich compared to more established ETL frameworks like Airflow or Luigi. It may not be suitable for highly complex ETL pipelines or large-scale distributed systems. Use Cases: Ideal for small to medium-sized ETL workflows, rapid prototyping, and scenarios requiring flexibility and simplicity. Code Example: from haul import Extractor, Transformer, Loader</p>"},{"location":"pandas/etl.html#define-an-extractor-to-load-data-from-a-csv-file","title":"Define an extractor to load data from a CSV file","text":"<p>extractor = Extractor.from_csv('input_example_data.csv')</p>"},{"location":"pandas/etl.html#define-a-transformer-to-add-a-new-column","title":"Define a transformer to add a new column","text":"<p>class MultiplyTransformer(Transformer):     def transform(self, row):         row['new_A_column'] = row['existing_A_column'] * 5.4         return row transformer = MultiplyTransformer()</p>"},{"location":"pandas/etl.html#define-a-loader-to-save-the-data-to-a-new-csv-file","title":"Define a loader to save the data to a new CSV file","text":"<p>loader = Loader.to_csv('output_example_data.csv')</p>"},{"location":"pandas/etl.html#execute-the-etl-process","title":"Execute the ETL process","text":"<p>etl_pipeline = extractor | transformer | loader etl_pipeline.run() 4. Luigi Advantages: Luigi is a Python library designed for building complex batch job pipelines. It handles dependency resolution, workflow management, visualization, and failure handling. Disadvantages: Luigi does not automatically synchronize tasks to worker nodes and lacks built-in scheduling, alerting, or monitoring features. Use Cases: Suitable for automating simple ETL workflows, such as log processing. Code Example: from luigi import Task, ExternalTask, Parameter, LocalTarget import pandas as pd  </p>"},{"location":"pandas/etl.html#define-a-task-to-extract-data-from-a-source-url","title":"Define a task to extract data from a source URL","text":"<p>class ExtractData(Task):     source_url = Parameter()  # Define a parameter for the source URL of the dat     def run(self):         # Read data from the source URL into a pandas DataFrame         df = pd.read_csv(self.source_url)         # Write the DataFrame to a CSV file at the specified output location         self.output().open('w').write(df.to_csv(index=False))     def output(self):         # Define the output target for this task (a local CSV file)         return LocalTarget('output_data_example.csv')</p>"},{"location":"pandas/etl.html#define-a-task-to-transform-the-data","title":"Define a task to transform the data","text":"<p>class TransformData(ExternalTask):     source_url = Parameter()  # Define a parameter for the source URL of the inp     def run(self):         # Read data from the source URL into a pandas DataFrame         df = pd.read_csv(self.source_url)         # Perform a hypothetical transformation: adding a new column         df['new_A_column'] = df['existing_A_column'] * 5.4         # Write the transformed DataFrame to a CSV file at the specified output          self.output().open('w').write(df.to_csv(index=False))     def output(self):         # Define the output target for this task (a transformed local CSV file)         return LocalTarget('transformed_example_data.csv')</p>"},{"location":"pandas/etl.html#main-entry-point-for-luigi-tasks","title":"Main entry point for Luigi tasks","text":"<p>if name == 'main':     # Run the TransformData task, which depends on 'example_data.csv' as input     luigi.build([TransformData('example_data.csv')]) 5. Airflow Advantages: Apache Airflow is a powerful workflow orchestration tool, allowing users to programmatically author, schedule, and monitor workflows. Its flexibility and modular architecture make it ideal for creating complex ETL pipelines that can handle dependencies and integrate with various data sources. Disadvantages: Airflow has a steep learning curve, especially for users unfamiliar with its DAG (Directed Acyclic Graph) paradigm. Additionally, it requires careful setup and maintenance, as scaling can introduce performance and resource challenges. Use Cases: Best suited for orchestrating ETL workflows in environments with multiple steps or dependencies. It is particularly effective for managing workflows in distributed or cloud-based systems. Code Example: from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime</p>"},{"location":"pandas/etl.html#define-the-etl-functions","title":"Define the ETL functions","text":"<p>def extract():     print(\"Extracting data...\")     print(\"more code here to do extract job\") def transform():     print(\"Transforming data...\")     print(\"more code here to do transform job\") def load():     print(\"Loading data...\")     print(\"more code here to do load job\")</p>"},{"location":"pandas/etl.html#initialize-the-dag","title":"Initialize the DAG","text":"<p>default_args = {     'owner': 'airflow',     'start_date': datetime(2024, 1, 1),     'retries': 1, } dag = DAG(     'example_etl',     default_args=default_args,     schedule_interval='@daily', )</p>"},{"location":"pandas/etl.html#define-the-tasks","title":"Define the tasks","text":"<p>extract_task = PythonOperator(task_id='extract', python_callable=extract, dag=da transform_task = PythonOperator(task_id='transform', python_callable=transform,  load_task = PythonOperator(task_id='load', python_callable=load, dag=dag)</p>"},{"location":"pandas/etl.html#set-task-dependencies","title":"Set task dependencies","text":"<p>extract_task &gt;&gt; transform_task &gt;&gt; load_task Summary These libraries and tools each have their strengths, and the choice of which to use depends on the specific project requirements, data scale, and the familiarity of the development team. The ETL process forms the foundation of data management and analysis, ensuring data consistency, accuracy, and availability. This enables data analysis and business intelligence (BI) tools to deliver valuable insights and support decision-making. With technological advancements, the ETL process continues to evolve, giving rise to variations like ELT (Extract, Load, Transform), where data is first loaded into the target system before transformation. This approach can improve efficiency when dealing with large-scale datasets. Thanks for your reading. Python Etl Etl Tool Etl Pipeline Python Data Preprocessing Written by Gen. David L. Follow 453 Followers \u00b7 4 Following AI practitioner &amp; python coder to record what I learned in python project development Responses (3) What are your thoughts? Respond david libert (\u201cdadoo\u201d) 29 days ago Hello thank you. You forgot Mage.ai, very cool and strong solution \ud83d\ude09 4 Reply Dinu Gherman 27 days ago Missing prefect.io. 2 Reply R. Ganesh 16 days ago these frameworks not libraries Reply More from Gen. David L. Gen. David L. Gen. David L. Advanced Pandas Features: ETL-PIPES: An Efficient Python ETL Enhance Your Data Processing\u2026 Data Processing Library The essence of data analysis lies in ETL-pipes is a powerful and flexible Python uncovering the stories behind the data. In thi\u2026 library specifically designed for ETL (Extract\u2026 Dec 5, 2024 105 Nov 30, 2024 45 6 Gen. David L. Gen. David L. How to Map Column Values in a Must-Know Python Pandas Pandas DataFrame? Functions for Effortless Data\u2026 Mapping column values refers to replacing The key point of data analysis lies in specific values in a column with other values,\u2026 uncovering the stories behind the data. To\u2026 Dec 24, 2024 81 Nov 27, 2024 19 See all from Gen. David L. Recommended from Medium Harold Finch Vijay Gadhave Top 25 Python Scripts To Automate Must-Have Data Engineering Your Daily Tasks Certifications Python is an excellent tool for automating Note: If you\u2019re not a medium member, CLICK daily tasks, thanks to its simplicity and a wid\u2026 HERE Nov 19, 2024 337 5 Dec 3, 2024 300 5 Lists Staff picks Stories to Help You Level-Up at Work 798 stories \u00b7 1568 saves 19 stories \u00b7 916 saves Self-Improvement 101 Productivity 101 20 stories \u00b7 3214 saves 20 stories \u00b7 2716 saves In Wren AI by Howard Chi Varun Singh How Uber is Saving 140,000 Hours Python 3.14 Released \u2014 Top 5 Each Month Using Text-to-SQL \u2014 \u2026 Features You Must Know Discover how Uber\u2019s Text-to-SQL technology Faster Annotations &amp; Mind-Blowing Updates streamlines data queries and learn how to\u2026 You NEED to Know! Jan 2 405 7 Dec 31, 2024 613 4 Sai Parvathaneni Esra Soylu Data Quality Checks (DQCs): A Advanced SQL Techniques Guide for Data Engineers In this article, we will focus on advanced SQL As a data engineer in financial services, I\u2019ve techniques. I will try to explain the methods\u2026 learned one critical lesson: the quality of you\u2026 Dec 8, 2024 62 2 Nov 23, 2024 263 3 See more recommendations</p>"},{"location":"pandas/generate_data.html","title":"Generate Data","text":""},{"location":"pandas/generate_data.html#modules","title":"Modules","text":"<ul> <li>faker In this post we will introduce several must-know Pandas methods for effective data exploration.</li> </ul>"},{"location":"pandas/generate_data.html#pandas-functions","title":"Pandas Functions","text":""},{"location":"pandas/generate_data.html#faker","title":"Faker","text":"<p>TODO: Use stock data to demonstrated this.</p> <p>Although not part of pandas,  we need a dataset to demonstrate various pandas functions.</p> <p>Create a CSV sample dataset using faker.</p> <pre><code>import random\nimport csv\nfrom faker import Faker\n\n# Initialize Faker\n\nfake = Faker()\n\n# List of products and their categories\n\nproducts = [\n    {\"name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 899.99},\n    {\"name\": \"Smartphone\", \"category\": \"Electronics\", \"price\": 699.99},\n    {\"name\": \"Headphones\", \"category\": \"Accessories\", \"price\": 49.99},\n    {\"name\": \"Coffee Maker\", \"category\": \"Home Appliances\", \"price\": 79.99},\n    {\"name\": \"Sneakers\", \"category\": \"Fashion\", \"price\": 59.99},\n    {\"name\": \"Backpack\", \"category\": \"Fashion\", \"price\": 39.99},\n    {\"name\": \"Blender\", \"category\": \"Home Appliances\", \"price\": 99.99},\n    {\"name\": \"Desk Chair\", \"category\": \"Furniture\", \"price\": 129.99},\n    {\"name\": \"Water Bottle\", \"category\": \"Accessories\", \"price\": 19.99},\n    {\"name\": \"Notebook\", \"category\": \"Stationery\", \"price\": 5.99},\n]\n\n# Define a function to generate order data\n\ndef generate_order_data(num_rows):\n    data = []\n    for_ in range(num_rows):\n        product = random.choice(products)\n        quantity = random.randint(1, 10)\n        total = round(product[\"price\"] * quantity, 2)\n        order = {\n            \"Order ID\": fake.uuid4(),\n            \"Customer Name\": fake.name(),\n            \"Customer Email\": fake.email(),\n            \"Product Name\": product[\"name\"],\n            \"Category\": product[\"category\"],\n            \"Quantity\": quantity,\n            \"Price\": product[\"price\"],\n            \"Total\": total,\n            \"Order Date\": fake.date_this_year(),\n            \"Shipping Address\": fake.address(),\n        }\n        data.append(order)\n    return data\n\n# Generate 1000 rows of data\n\nnum_rows = 1000\norder_data = generate_order_data(num_rows)\n\n# Save the data to a CSV file\n\noutput_file = \"sample_orders.csv\"\nwith open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n    writer = csv.DictWriter(file, fieldnames=order_data[0].keys())\n    writer.writeheader()\n    writer.writerows(order_data)\nprint(f\"Sample dataset with {num_rows} rows has been saved to '{output_file}'.\")\n</code></pre> <p>This code ensures that the sample dataset is saved as a structured CSV file (sample_orders.csv) for further data analysis by pandas functions. Head function head() head(): Used to preview the top rows of the sample dataset.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the first 10 rows of the dataset\n\nprint(df.head(10))\n</code></pre>"},{"location":"pandas/generate_data.html#head-tail","title":"Head Tail","text":"<p>head(): Use to preview the bottom rows of the sample dataset. tail(): Use to preview the bottom rows of the sample dataset.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the last 10 rows of the dataset\n\nprint(df.head(10))\nprint(df.tail(10))\n</code></pre>"},{"location":"pandas/generate_data.html#sample","title":"Sample","text":"<p>sample(): This function is highly valuable when working with large datasets. When we need to extract and analyze a smaller subset from a larger DataFrame, <code>sample()</code> helps efficiently retrieve random samples, enabling preliminary data exploration or performance evaluation.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Read and display the random 10 rows from the dataset\n\nprint(df.sample(10))\n</code></pre>"},{"location":"pandas/generate_data.html#info","title":"Info","text":"<p>Information function info() info(): This function provides a summary of the dataset, including the number of entries, column names, data types, and memory usage.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display a summary of the dataset\n\nprint(df.info())\n</code></pre>"},{"location":"pandas/generate_data.html#describe","title":"Describe","text":"<p>describe(): This function provides basic statistical information about the dataset, such as mean, standard deviation, minimum and maximum values, and quartiles.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the basic statistical information about the dataset\n\nprint(df.describe())\n</code></pre>"},{"location":"pandas/generate_data.html#value-counts","title":"Value Counts","text":"<p>Value counts function value_counts() value_counts(): This method returns the count of all unique values in a column or a pandas Series.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the count of all unique values in a column,such as \"Category\"\n\nprint(df[\"Category\"].value_counts())\n</code></pre>"},{"location":"pandas/generate_data.html#shape","title":"Shape","text":"<p>shape: This attribute returns the number of rows and columns in the dataset.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the number of rows and columns in the dataset\n\nprint(df.shape)\n~~~~\n\n#### Dtypes\n\nDataframe dtypes attribute\ndf.dtypes: This attribute returns the data types of all columns.\n\n~~~python\nimport pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the data types of all columns\n\nprint(df.dtypes)\n</code></pre>"},{"location":"pandas/generate_data.html#unique","title":"Unique","text":"<p>Unique function unique() unique(): This method returns all unique values in a column or a pandas Series.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display all unique values in a column\n\nprint(df[\"Category\"].unique())\n</code></pre>"},{"location":"pandas/generate_data.html#non-unique","title":"Non-unique","text":"<p>Nunique function nunique() nunique(): This function returns the number of unique values in a DataFrame.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the count of unique values in the dataset, sorted in descending order\n\ndf.nunique().sort_values(ascending=False)\n</code></pre>"},{"location":"pandas/google_sheets.html","title":"Google Sheets","text":""},{"location":"pandas/google_sheets.html#overview","title":"Overview","text":"<p>Medium article: Automate Google Sheets CRUD Automation</p> <p>This article covers setting up a Google Sheets API that will allow python to perform CRUD operations \u2014 Create, Read, Update, and Delete \u2014 directly from your python script to your google spreadsheet.</p> <p>This process is handy if you need to automate reports, sync live data, or managing spreadsheets programmatically, this tutorial will give you the tools to make your\u00a0Google Sheets dynamically update in real time\u00a0with Python!</p>"},{"location":"pandas/google_sheets.html#use-cases","title":"Use Cases","text":"<ul> <li>Automated reports and dashboards \u2014 Fetch and update live data for business reports.</li> <li>Stock and crypto price tracking \u2014 Automatically log and update real-time financial data.</li> <li>Inventory management \u2014 Keep track of product stock levels dynamically.</li> <li>Task and project management \u2014 Use Google Sheets as a simple task tracker with automatic updates.</li> <li>News and web scraping logs \u2014 Extract and store live news headlines or scraped web data.</li> <li>API data logging \u2014 Pull in weather, sales, or user analytics from APIs and store in Sheets.</li> <li>Student and employee records \u2014 Automate updates to attendance, grades, or work logs.</li> <li>Database alternative \u2014 Use Google Sheets as a lightweight database for simple applications.</li> <li>Form submission processing \u2014 Automate handling of Google Forms data in real-time.</li> </ul>"},{"location":"pandas/google_sheets.html#steps","title":"Steps","text":"<ol> <li>Go to Google Cloud Console</li> <li>gsheets-453804-c772b0c9403a.json</li> </ol>"},{"location":"pandas/google_sheets.html#implemented","title":"Implemented.","text":""},{"location":"pandas/list_function.html","title":"List Function","text":""},{"location":"pandas/numpy.html","title":"NumPy","text":"<p>Explore pandas functions that can also use with numpy</p> <ul> <li>Pandas for data manipulation and analysis or </li> <li>NumPy for numerical computing</li> </ul>"},{"location":"pandas/numpy.html#compare-pandas-and-numpy","title":"Compare Pandas and Numpy","text":""},{"location":"pandas/numpy.html#1-shift","title":"1. Shift","text":"<p>The pandas.Series.shift() function shifts the values of a particular position in a Series up or down by a specified number of periods. </p> <p>Pandas: Shifts the values of a series by a certain number of periods. NumPy Equivalent: You can use np.roll() to achieve a similar effect.</p> <p>Example <pre><code># Pandas\ndf['shifted'] = df['column'].shift(1)\n\n#  NumPy\ndf['shifted'] = np.roll(df['column'].values, 1)\n</code></pre></p>"},{"location":"pandas/numpy.html#2-diff","title":"2. Diff","text":"<p>pandas.Series.diff() is used to calculate the difference between consecutive elements in a given series.</p> <p>Interestingly, you can achieve the same result with NumPy using np.diff().</p> <p>Example <pre><code># Pandas\ndf['difference'] = df['column'].diff()\n# NumPy\ndf['difference'] = np.diff(df['column'], prepend=np.nan)\n</code></pre></p> <p>Output <pre><code>output\n   column  difference  difference_np\n0      30         NaN             NaN\n1      32         2.0             2.0\n2      35         3.0             3.0\n3      31        -4.0            -4.0\n4      29        -2.0            -2.0\n</code></pre></p>"},{"location":"pandas/numpy.html#3-apply","title":"3. Apply","text":"<p>pandas.DataFrame.apply() you can add two rows and form a new one as an answer.</p> <p>Pandas: Apply a function to each row or column. NumPy Equivalent: Use np.apply_along_axis() for similar functionality on NumPy arrays.</p> <p>Example</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}\ndf = pd.DataFrame(data)\n# Using Pandas\ndf['result'] = df.apply(lambda row: row['a'] + row['b'], axis=1)\n# Using NumPy\nresult = np.apply_along_axis(lambda row: row[0] + row[1], 1, df[['a', 'b']].valu\nprint(df)\nprint(\"NumPy Result:\", result)\n</code></pre> <p>Output <pre><code>   a  b  result\n0  1  4       5\n1  2  5       7\n2  3  6       9\nNumPy Result: [5 7 9]\n</code></pre></p>"},{"location":"pandas/numpy.html#4-rank","title":"4. Rank","text":"<p>pandas.Series.rank()</p> <p>Pandas: Computes the rank of each element in a Series. NumPy Equivalent: You can achieve similar functionality using np.argsort().</p> <p>Example: <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'column': [50, 20, 80, 60]}\ndf = pd.DataFrame(data)\n# Using Pandas\ndf['rank'] = df['column'].rank()\n# Using NumPy\nranks = np.argsort(np.argsort(df['column'].values))\ndf['numpy_rank'] = ranks + 1  # Adding 1 because ranks start from 1 in Pandas\nprint(df)\n</code></pre></p> <p>Output <pre><code>   column  rank  numpy_rank\n0      50   2.0           2\n1      20   1.0           1\n2      80   4.0           4\n3      60   3.0           3\n</code></pre></p>"},{"location":"pandas/numpy.html#5-isin","title":"5. IsIn","text":"<p>pandas.Series.isin() Pandas: Check if elements in a Series are in a given list or array. NumPy Equivalent: Use np.in1d() to perform this task.</p> <p>Example: <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {\n    'product': ['TV', 'Sofa', 'Laptop', 'Table', 'Shirt', 'Headphones', 'Shoes']\n    'category': ['Electronics', 'Furniture', 'Electronics', 'Furniture', 'Clothi\n}\ndf = pd.DataFrame(data)\n# Categories to check\ntarget_categories = ['Electronics', 'Furniture', 'Clothing']\n# Using Pandas\ndf['is_in'] = df['category'].isin(target_categories)\n# Using NumPy\ndf['is_in_np'] = np.in1d(df['category'], target_categories)\n# Display the DataFrame\nprint(df)\n</code></pre></p> <p>Output: <pre><code>      product    category  is_in  is_in_np\n0          TV  Electronics   True      True\n1        Sofa   Furniture    True      True\n2      Laptop  Electronics   True      True\n3       Table   Furniture    True      True\n4       Shirt   Clothing     True      True\n5  Headphones  Electronics   True      True\n6       Shoes     Apparel   False     False\n</code></pre></p>"},{"location":"pandas/numpy.html#6-cumsum","title":"6. CumSum","text":"<p>Pandas: Calculates the cumulative sum of the values in a series. NumPy Equivalent: Use np.cumsum() for cumulative summation.</p> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'column': [1, 2, 3, 4, 5]}\ndf = pd.DataFrame(data)\n# Using Pandas to calculate cumulative sum\ndf['cumsum_pandas'] = df['column'].cumsum()\n# Using NumPy to calculate cumulative sum\ndf['cumsum_numpy'] = np.cumsum(df['column'].values)\nprint(df)\n</code></pre> <p>Output: <pre><code>   column  cumsum_pandas  cumsum_numpy\n0       1              1             1\n1       2              3             3\n2       3              6             6\n3       4             10            10\n4       5             15            15\n</code></pre></p>"},{"location":"pandas/numpy.html#7-expanding","title":"7. Expanding","text":"<p>Now, we can achieve this using the pandas.Series.expanding() function, which expands a window over the data to compute cumulative statistics like the running mean.</p> <p>NumPy Equivalent: You can achieve this manually with np.cumsum() and computing the desired statistic over expanding windows.</p> <p>Example: <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'column': [1, 2, 3, 4, 5]}\ndf = pd.DataFrame(data)\n# Using Pandas to calculate the expanding mean\ndf['expanding_mean'] = df['column'].expanding().mean()\n# Using NumPy to calculate the manual expanding mean\nexpanding_mean = np.cumsum(df['column'].values) / np.arange(1, len(df) + 1)\n# Adding the NumPy result to the DataFrame\ndf['expanding_mean_numpy'] = expanding_mean\n# Display the DataFrame\nprint(df)\n</code></pre></p> <p>Output: <pre><code>   column  expanding_mean  expanding_mean_numpy\n0       1             1.0                  1.0\n1       2             1.5                  1.5\n2       3             2.0                  2.0\n3       4             2.5                  2.5\n4       5             3.0                  3.0\n</code></pre></p>"},{"location":"pandas/numpy.html#8-pct_change","title":"8. Pct_Change","text":"<p>Pandas: Computes the percentage change between the current and prior element. NumPy Equivalent: Use a combination of NumPy array operations to calculate the percentage change.</p> <p>Example: <pre><code>import pandas as pd\nimport numpy as np\n# Sample sales data\ndata = {'day': ['Day 1', 'Day 2', 'Day 3', 'Day 4', 'Day 5'],\n        'sales': [100, 110, 150, 120, 130]}\n# Create DataFrame\ndf = pd.DataFrame(data)\n# Using Pandas to compute percentage change\ndf['pct_change_pandas'] = df['sales'].pct_change()\n# Using NumPy to compute percentage change manually\npct_change_numpy = np.diff(df['sales'].values) / df['sales'].values[:-1]\npct_change_numpy = np.insert(pct_change_numpy, 0, np.nan)  # Insert NaN at the s\n# Add the NumPy result to the DataFrame\ndf['pct_change_numpy'] = pct_change_numpy\n# Display the DataFrame\nprint(df)\n</code></pre></p> <p>Output <pre><code>     day  sales  pct_change_pandas  pct_change_numpy\n0  Day 1    100                NaN               NaN\n1  Day 2    110           0.100000          0.100000\n2  Day 3    150           0.363636          0.363636\n3  Day 4    120          -0.200000         -0.200000\n4  Day 5    130           0.083333          0.083333\n</code></pre></p>"},{"location":"pandas/numpy.html#9-fill-na","title":"9. Fill NA","text":"<p>Pandas: Fills NaN values with a specified value or method. NumPy Equivalent: Use np.where() to replace NaN values in a NumPy array.</p> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample data with NaN values\ndata = {'scores': [100, 200, np.nan, 300, np.nan]}\n# Create DataFrame\ndf = pd.DataFrame(data)\n# Using Pandas to fill NaN values\ndf['filled_pandas'] = df['scores'].fillna(0)\n# Using NumPy to fill NaN values manually\ndf['filled_numpy'] = np.where(np.isnan(df['scores'].values), 0, df['scores'].val\n# Display the DataFrame\nprint(df)\n\n\nOutput:\n~~~plain\n   scores  filled_pandas  filled_numpy\n0   100.0          100.0         100.0\n1   200.0          200.0         200.0\n2     NaN            0.0           0.0\n3   300.0          300.0         300.0\n4     NaN            0.0           0.0\n</code></pre>"},{"location":"pandas/numpy.html#10-drop-na","title":"10. Drop NA","text":"<p>The pandas.DataFrame.dropna() function removes rows (or columns) from a</p> <p>Pandas: Drops rows or columns with NaN values. NumPy Equivalent: Use ~np.isnan() to filter out rows containing NaN.</p> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample data with NaN values\ndata = {'scores': [100, 200, np.nan, 300, 150]}\n# Create DataFrame\ndf = pd.DataFrame(data)\n# Using Pandas to drop rows with NaN values\ndf_cleaned_pandas = df.dropna()\n# Using NumPy to manually filter out rows with NaN values\ndf_cleaned_numpy = df[~np.isnan(df['scores'].values)]\n# Display the results\nprint(\"Original DataFrame:\\n\", df)\nprint(\"\\nPandas dropna():\\n\", df_cleaned_pandas)\nprint(\"\\nNumPy equivalent:\\n\", df_cleaned_numpy)\n</code></pre> <p>Output <pre><code>Original DataFrame:\n    scores\n0   100.0\n1   200.0\n2     NaN\n3   300.0\n4   150.0\nPandas dropna():\n    scores\n0   100.0\n1   200.0\n3   300.0\n4   150.0\nNumPy equivalent:\n    scores\n0   100.0\n1   200.0\n3   300.0\n4   150.0\n</code></pre></p>"},{"location":"pandas/numpy.html#11-value-counts","title":"11. Value Counts","text":"<p>Pandas: Counts the unique values in a series. NumPy Equivalent: Use np.unique() with return_counts=True to get unique values and their counts. Example: Responses: ['A', 'B', 'A', 'C', 'B', 'A', 'B'] <pre><code>import pandas as pd\nimport numpy as np\n# Sample data\ndata = {'responses': ['A', 'B', 'A', 'C', 'B', 'A', 'B']}\n# Create DataFrame\ndf = pd.DataFrame(data)\n# Using Pandas to count unique values\nvalue_counts_pandas = df['responses'].value_counts()\n# Using NumPy to count unique values manually\nunique, counts = np.unique(df['responses'].values, return_counts=True)\nvalue_counts_numpy = dict(zip(unique, counts))  # Combine unique values and coun\n# Display the results\nprint(\"Pandas value_counts():\\n\", value_counts_pandas)\nprint(\"\\nNumPy equivalent:\\n\", value_counts_numpy)\n</code></pre></p> <p>Output: <pre><code>Pandas value_counts():\n A    3\n B    3\n C    1\nName: responses, dtype: int64\nNumPy equivalent:\n {'A': 3, 'B': 3, 'C': 1}\n</code></pre></p>"},{"location":"pandas/profiling.html","title":"Profiling","text":"<p>TODO:  Use chatgpt to generate content</p> <ul> <li>Generate Reports Using Pandas Profiling, </li> <li>Deploy Using Streamlit Kaustubh Gupta 7 Last Updated : 23 Oct, 2024 Pandas library offers a wide range of functions, making it an indispensable tool for data manipulation that caters to almost every task. One convenient feature, often employed for gaining quick insights into a dataset, is the pandas describe function. This function gives users a descriptive statistical summary of all the features, helping them understand the data\u2019s overall characteristics. However, for a more comprehensive analysis, the pandas profiling Package is an additional valuable tool in the Pandas ecosystem. Pandas profiling is the solution to this problem. It offers report generation for the dataset with lots of features and customizations for the report generated. In this article, we will explore this library, look at all the features provided, and some advanced use cases and integrations that can be useful to create stunning reports out of the data frames! This article was published as a part of the Data Science Blogathon. Table of cWoen utsee ncotoskies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Accept all cookies Use necessary cookies Add MetaData Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Controlling parameters of the Report</li> <li>Integrations Widget  in Jupyter notebook</li> <li>How to Make it a Part of Streamlit App? Step 1: Install the streamlit_pandas_profiling Step 2: Create a Python file</li> <li>Conclusion Installation Like every other Python package, pandas profiling can be easily installed via the pip package manager: Copy Code pip install pandas-profiling It can also be installed via Conda package manager too: Copy Code conda env create -n pandas-profiling conda activate pandas-profiling conda install -c conda-forge pandas-profiling Dataset and Setup Now it\u2019s time to see how to start the Python pandas profiling library and generate the We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; report out of the data frames. First things first,  let\u2019s import a dataset for which we will Cookies Policy. be generatinSgh powro dfieleta rilesport. I am using the agriculture dataset which contains the State_name, District_name, Crop_year, Season, Crop, Area, and Production. Copy Code import pandPaesrs oansa lipzedd GenAI Learning Path 2025\u2728  Crafted Just for YOU! df = pd.read_csv(\"crops data.csv\") Before I discuss the Python pandas profiling, have a look at the pandas describe function output for the dataframe: Copy Code df.describe(include='all') (Notice that I have used the include parameter of the describe function set to \u201call\u201d which forces pandas to include all the data types of the dataset to be included in the summary. The string type values are accompanied by options such as unique, top, and frequency) Let\u2019s import the Python pandas profiling library: We use cookies essential for this site to function well. Please click to help us improve its Copy Code from pandasu_sperfuolnfeislsi wnigth  iadmdpiotiornta l Pcroookfieisl.e LReeaprno rabtout our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details To start profiling a dataframe, you have two ways:</li> <li>You can call the \u2018.profile_report()\u2019 function on pandas dataframe. This function is Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! not part of the pandas API but as soon as you import the profiling library, it adds this function to dataframe objects. You can pass the dataframe object to the profiling function and then call the function object created to start the generation of the profile. You will get the same output report in either of the methods. I am using the second method to generate the report for the imported agriculture dataset. Copy Code profile = ProfileReport(df) profile Animation Showing report generation Sections of the Report Now that the report is generated, let\u2019s explore all the sections of the report one by one. Overview This section consists of the 3 tabs: Overview, Warnings, and Reproduction. We use cookies essential for this site to function well. Please click to help us improve its The Overview generated by pandas profiling provides a comprehensive dataset usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. summary, encompassing various key statistics. It covers the fundamental Show details characteristics such as the Number of variables (features or columns of the data frame) and the Number of observations (rows of the data frame). Additionally, it sheds Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! light on data quality by revealing insights into Missing cells and their corresponding percentage, offering a quick assessment of the dataset\u2019s completeness. The Duplicate rows section provides information on the presence of identical rows, including the percentage of duplicate rows. As a holistic touch, the overview concludes with the total memory size, encapsulating the overall footprint of the dataset. Integrating pandas profiling seamlessly facilitates a profound understanding of these essential aspects, enhancing the efficiency of exploratory data analysis. The warnings tab contains any warnings related to cardinality, correlation with other variables, missing values, zeroes, skewness of the variables, and many others. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! The reproduction tab displays information related to the report generation. It shows the start and end times of the analysis, the time taken to generate the report, the software version of pandas profiling, and a configuration download option. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! We will discuss the configuration file in this article\u2019s advanced use case section. Variables This section of the report gives a detailed analysis of all the variables/columns/features of the dataset. The information presented varies depending upon the data type of variable. Let\u2019s break it down. Numeric Variables You get information about the distinct values, missing values, min-max, mean, and negative values count for numeric data type features. You also get small representation values in the form of a Histogram. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! The toggle button expands to the Statistics, Histogram, Common Values, and Extreme Values tabs. The statistics tab includes:</li> <li>Quantile statistics: Min-Max, percentiles, median, range, and IQR (Inter Quartile range)</li> <li>Descriptive statistics: Standard Deviation, Coefficient of variance, Kurtosis, mean, skewness, variance, and monotonicity. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! The histogram tab displays the frequency of variables or distribution of numeric data. The common values tab is basically value_counts of the variables presented as both counts and percentage frequency. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! String Variables For string-type variables, you get Distinct (unique) values, distinct percentages, missing missing percentages, memory size, and a horizontal bar presentation of all the unique values with count presentation. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details (It also reports any warnings associated with the variable irrespective of its data type) Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! The toggle button expands to the Overview, Categories, Words, and Characters tab.  The Overview tab displays the max-min median mean length, total characters, distinct characters, distinct categories, unique characters, and sample from the dataset for string type values. The categories tab displays a histogram and sometimes a pie chart of the feature\u2019s value counts. The table contains the value, count, and percentage frequency. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! The words and the characters tab does the same job as the categories tab in terms of presenting the data in tabular and histogram format. Still, it can go much deeper into the lower case, upper case, punctuation, special characters categories count too! Correlations Correlation describes the degree to which two variables move in coordination with one another. The pandas profiling python report provides five types of correlation coefficients: Pearson\u2019s r, Spearman\u2019s \u03c1, Kendall\u2019s \u03c4, Phik (\u03c6k), and Cram\u00e9r\u2019s V (\u03c6c). We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! You can also click the toggle button for details about the correlation coefficients. Missing values The report generated also contains visualizations for the missing values in the dataset. You get three types of plots: count, matrix, and dendrogram. The count plot is a basic bar plot with an x-axis as column names, and the length of the bar represents the We use cookies essential for this site to function well. Please click to help us improve its number of vausluefeulsne spsr weisthe anddt it(iownaitl hcoooukite sn. uLella rvna albuouets o)u.r  uSsiem ofi lcaorolkyie,s t ihn eou mr Parivtaricxy  Paonlicdy  &amp;the Cookies Policy. dendrogram are. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Sample This section displays the first and last 10 rows of the dataset. How to Save the Report? So far, you\u2019ve learned how to generate dataframe reports with a single line of code or function and explored the report\u2019s included features. You may want to export this analysis to an external file for integration with other applications or web publishing We use cookies essential for this site to function well. Please click to help us improve its Guess what?us eYfuolnue scsa wnith s aaddviteio ntahl icso orkeieps.o Lreta! rYn oabuo uct aounr  ussae vofe c otohkiiess  rine opuor rPtr ivinac y\u2013 Policy &amp; Cookies Policy. Show details</li> <li>HTML format</li> <li>JSON format The save function remains the same for any of the formats. Change the file extension Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! while saving. To save the report, call the \u201c.to_file()\u201d function on the profile object: Copy Code profile.to_file(\"Analysis.html\") profile.to_file(\"Analysis.json\") Advanced Usage The report generated by Pandas profiling Python is a complete analysis without any input from the user except the dataframe object. All the report elements are chosen automatically, and default values are preferred. There might be some elements in the report that you don\u2019t want to include, or you need to add your metadata for the final report. There comes the advanced usage of this library. You can control every aspect of your report by changing the default configurations. Let\u2019s see some of how you can customize your reports. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Add MetaData Cookies Policy. Show details You can add information such as \u201ctitle\u201d, \u201cdescription\u201d, \u201ccreator\u201d, \u201cauthor\u201d, \u201cURL\u201d, Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! \u201ccopyright_year\u201d, and  \u201ccopyright_holder\u201d. This information will appear in the dataset overview section. A new tab called \u201cdataset\u201d will be created for this metadata. To add this data to the report, use the dataset parameter in the ProfileReport function and pass this data as a dictionary: Copy Code profile = ProfileReport(df,                         title=\"Agriculture Data\",         dataset={         \"description\": \"This profiling report was generated for Analytics Vidhya Blog\",         \"copyright_holder\": \"Analytics Vidhya\",         \"copyright_year\": \"2021\",         \"url\": \"https://www.analyticsvidhya.com/blog/\",     },) profile You can also add information about the variables used in the dataset using the variables parameter. This takes in the dictionary with descriptions as the key and value as another dictionary with a key-value pair, where the key is the variable name and the value is the description of the variable. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Copy Code variables={ Cookies Policy.         \"descriptions\": { Show details             \"State_Name\": \"Name of the state\",             \"District_Name\": \"Name of district\",             \"Crop_Year\": \"Year when it was seeded\",             \"Season\": \"Crop year\",            P e\"rsCornoapli\"ze:d  \"GWehniAcI hLe acrrnoinpg  Pwaatsh  2s0e25e\u2728de dC?\"ra,fted Just for YOU!             \"Area\": \"How much area was allocated to the crop?\",             \"Production\": \"How much production?\",         }     } When you add this to your ProfileReport function, a separate tab will be created named \u201cVariables\u201d under the overview section: Controlling parameters of the Report Suppose you don\u2019t want to display all types of correlation coefficients. You can disable other coefficients by using the configuration for correlations. This is also a dictionary object and can be passed to the ProfileReport function: We use cookies essential for this site to function well. Please click to help us improve its Copy Code profile = ProfileReport(df, usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp;            C o o k ie s  P o l ic y.   title=\"Agriculture Data\",            S  h o w   d e t a i ls  correlations={                                         \"pearson\": {\"calculate\": True},                                         \"spearman\": {\"calculate\": False},                                         \"kendall\": {\"calculate\": False},                                         \"phi_k\": {\"calculate\": False},     }) Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Similarly, you can customize every report section, the HTML format, plots, and everything. \u201c Check out this page of the documentation for details. Integrations After making your reports stunning by configuring every aspect of it, you might want to publish it anyhow. You can export it to HTML format and upload it to the web. But there are some other methods to make your report stand out. Widget  in Jupyter notebook While running the panda profiling in your Jupyter notebooks, you will get the HTML rendered in the code cell only. This disturbs the experience of the user. You can make it act like a widget that is easily accessible and offers a compact view. To do this, simply call \u201c.to_widgets()\u201d on your profile object: We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! How to Make it a Part of Streamlit App? Yes! You can make this report as a part of a streamlit app, too. Streamlit is a powerful package that enables GUI web app building with minimal code. The applications are interactive and compatible with almost every device. You can make your reports as a part of the streamlit app by following this code: Step 1: Install the streamlit_pandas_profiling  Copy Code pip install streamlit-pandas-profiling Step 2: Create a Python file Create a python file and write code in this format: We use cookies essential for this site to function well. Please click to help us improve its Copy Code import panduasse fualnse spsd with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. import pandSash_opwr odfeitlaiilnsg import streamlit as st from streamlit_pandas_profiling import st_profile_report Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! from pandas_profiling import ProfileReport df = pd.read_csv(\"crops data.csv\", na_values=['=']) profile = ProfileReport(df,                         title=\"Agriculture Data\",         dataset={         \"description\": \"This profiling report was generated for Analytics Vidhya Blog\",         \"copyright_holder\": \"Analytics Vidhya\",         \"copyright_year\": \"2021\",         \"url\": \"https://www.analyticsvidhya.com/blog/\",     },     variables={         \"descriptions\": {             \"State_Name\": \"Name of the state\",             \"District_Name\": \"Name of district\",             \"Crop_Year\": \"Year when it was seeded\",             \"Season\": \"Crop year\",            W \"eC ursoep c\"o:o ki\"eWs heisscehn ticarl foopr  thwias ss ites etoe fduendct?io\"n, well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy.             \"Area\": \"How much area was allocated to the crop?\", Show details             \"Production\": \"How much production?\", Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU!         }     } ) st.title(\"Pandas Profiling in Streamlit!\") st.write(df) st_profile_report(profile) Step 3: Run your streamlit app In the terminal, type: streamlit run .py We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Exploratory Data Analysis Using Pandas Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Exploratory Data Analysis (EDA) is like exploring a new place. You start by looking around to understand what\u2019s there. Similarly, in EDA, you look at a dataset to see what\u2019s in it and what it can tell you. It\u2019s essentially the initial data exploration stage in data science, where you delve into the dataset statistics and examine its intricacies. Here\u2019s what you do during EDA: Look at the Numbers: You start by checking basic things like averages, ranges, and the spread of the numbers. Make Pictures: Instead of just staring at numbers, you make charts and graphs to show the data visually. It\u2019s like drawing a map of your exploration. Clean Up: Sometimes, data can be messy with missing pieces or weird values. So, you clean it up by filling in missing parts or removing the weird stuff. Create New Ideas: You might develop new ideas or ways to look at the data, like combining different parts or changing how you measure things. Find Connections: You try to see if different parts of the data are related. For example, if one thing goes up, does another also go up? Make Things Simple: If the data is too complicated, you might simplify it to see the big picture more clearly. Look at Time: If your data changes over time, you\u2019ll examine how it changes and whether there are any repeating patterns. We use cookies essential for this site to function well. Please click to help us improve its Test Ideuassef:u lFneisnsa wliltyh, a yddoituio ntael scoto ykioesu. rL eiadrne aabso utto o usr eusee  ioff  cthooekyie sm ina okure P rsiveacnys Peo liacyn &amp;d if the Cookies Policy. patterns you see are real or just random. Show details Overall, EDA helps you understand your data better before doing any fancy analysis or Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! making big conclusions. It\u2019s like exploring a map before going on a big adventure! Conclusion In this article, you are introduced to a new tool, \u201cPandas Profiling,\u201d a one-stop solution for generating reports out of the Pandas dataframe. We explore all the features of this tool, different sections, and their content. Then, we move on to saving the report generated. Later, we look at some of the advanced use cases of this library and finally integrate the Streamlit app to make the reports more promising and interactive. The media shown in this article are not owned by Analytics Vidhya and are used at the Author\u2019s discretion. Kaustubh Gupta Kaustubh Gupta is a skilled engineer with a B.Tech in Information Technology from Maharaja Agrasen Institute of Technology. With experience as a CS Analyst and Analyst Intern at Prodigal Technologies, Kaustubh excels in Python, SQL, Libraries, and various engineering tools. He has developed core components of product intent engines, created gold tables in Databricks, and built internal tools and dashboards using Streamlit and Tableau. Recognized as India\u2019s Top 5 Community Contributor 2023 by Analytics Vidhya, Kaustubh is also a prolific writer and mentor, contributing significantly to the tech community through speaking sessions and workshops. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Data Engineering Data Exploration Data Visualization Intermediate Listicle Machine Learning Project Python Python Streamlit Structured Data Technique Free Courses  4.7 Generative AI - A Way of Life Explore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics.  4.5 Getting Started with Large Language Models Master Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple. We use cookies e s4s.e6ntial for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Building LLM Applications using Prompt Engineering Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! This free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data.  4.8 Improving Real World RAG Systems: Key Challenges &amp; Practical Solutions Explore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications.  4.7 Microsoft Excel: Formulas &amp; Functions Master MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Responses From Readers What are your thoughts?... Submit reply Santosh Kesava Hi , This is really a informative post thank you for posting. My scenario is same but the only missing part is how to We uscer ceoaotkeie sa e tsasebn twiailt fho rc thoims spitlee ttoe f udnacttiaon a wnedll.  bPuletatsoen c lticok  dtoo hwenlpl ousa idm pinrove its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; excel or csv ? Please help. Thank you Santosh Kesava Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Niladri Chakraborty Nice article. I'm trying to run pandas profiling in my python 3.8.x version. But I'm getting error message saying PydanticImportError: BaseSettingshas been moved to thepydantic-settings package while I'm running from pandas_profiling import ProfileReport. Can you please guide me to resolve the issue? We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Frequently Asked Questions How to use pandas-profiling? A. To use pandas-profiling, you should first install it using pip. Then, import it into your Python script or Jupyter Notebook. Load your dataset with Pandas, create a ProfileReport object, and call its to_file() or to_widgets() methods to obtain a detailed analysis and visualization of your data. What is Pandas profiling? Why use pandas profiling? Q4. How to pip install pandas-profiling? Write for us  Write, captivate, and earn accolades and rewards for your work We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Reach Cao Goklioesb Paol lAicyu.dience Get ExpSehrot wF edeedtabialsck Build Your Brand &amp; Audience Cash In on Your Knowledge Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Join a Thriving Community Level Up Your Data Science Game Flagship Courses GenAI Pinnacle Program | AI/ML BlackBelt Courses Free Courses Generative AI | Large Language Models | Building LLM Applications using Prompt Engineering | Building Your first RAG System using LlamaIndex | Stability.AI | MidJourney | Building Production Ready RAG systems using LlamaIndex | Building LLMs for Code | Deep Learning | Python | Microsoft ExcWele| u Msea ccohokiniees  eLseseanrtniailn fogr |th Dis esicteis tioo fnun Tctrioene wse|ll .P Palenadsea csl icfok rto D haeltpa u As inmaplryosveis its| Ensemble usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Learning | NLP | NLP using Deep Learning | Neural Networks | Loan Prediction Practice Cookies Policy. Problem | Time Series Forecasting | Tableau | Business Analytics Show details Popular Categories Generative AI | Prompt Engineering | Generative AI Application | News | Technical Guides | AI Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Tools | Interview Preparation | Research Papers | Success Stories | Quiz | Use Cases | Listicles Generative AI Tools and Techniques GANs | VAEs | Transformers | StyleGAN | Pix2Pix | Autoencoders | GPT | BERT | Word2Vec | LSTM | Attention Mechanisms | Diffusion Models | LLMs | SLMs | StyleGAN | Encoder Decoder Models | Prompt Engineering | LangChain | LlamaIndex | RAG | Fine-tuning | LangChain AI Agent | Multimodal Models | RNNs | DCGAN | ProGAN | Text-to-Image Models | DDPM | Document Question Answering | Imagen | T5 (Text-to-Text Transfer Transformer) | Seq2seq Models | WaveNet | Attention Is All You Need (Transformer Architecture) Popular GenAI Models Llama 3.1 | Llama 3 | Llama 2 | GPT 4o Mini | GPT 4o | GPT 3 | Claude 3 Haiku | Claude 3.5 Sonnet | Phi 3.5 | Phi 3 | Mistral Large 2 | Mistral NeMo | Mistral-7b | Gemini 1.5 Pro | Gemini Flash 1.5 | Bedrock | Vertex AI | DALL.E | Midjourney | Stable Diffusion Data Science Tools and Techniques Python | R | SQL | Jupyter Notebooks | TensorFlow | Scikit-learn | PyTorch | Tableau | Apache Spark | Matplotlib | Seaborn | Pandas | Hadoop | Docker | Git | Keras | Apache Kafka | AWS | NLP | Random Forest | Computer Vision | Data Visualization | Data Exploration | Big Data | Common Machine Learning Algorithms | Machine Learning Company Discover About Us Blogs Contact Us Expert Sessions We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Careers Learning Paths Cookies Policy. Show details Comprehensive Guides Learn Engage Free Courses Community Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! AI&amp;ML Program Hackathons GenAI Program Events Agentic AI Program Podcasts Contribute Enterprise Become an Author Our Offerings Become a Speaker Trainings Become a Mentor Data Culture Become an Instructor AI Newsletter Terms &amp; conditions   Refund Policy   Privacy Policy   Cookies Policy  \u00a9 Analytics Vidhya 2025.All rights reserved. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details</li> </ul>"},{"location":"pandas/quick_examples.html","title":"Quick Examples","text":""},{"location":"pandas/quick_examples.html#example-code","title":"Example Code","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n# 1. Load Data with read_csv\ndf = pd.read_csv('data.csv', usecols=['Column1', 'Column2'])\n\n# 2. View the First Few Rows with head()\ndf.head()\n\n# 3. Check missing values\ndf.isna().sum()\n\n# 4. Remove duplicates\ndf.drop_duplicates(inplace=True)\n\n# 5. Rename columns\ndf.rename(columns={'old_name':'new_name'}, inplace=True)\n\n# 6. Convert Data Types with astype()\ndf['Column1'] = df['Column1'].astype(float)\n\n# 7. Handle Missing Data with fillna()\ndf.fillna(method='ffill', inplace=True)\n\n# 8. Set a Column as the Index\ndf.set_index('Column1', inplace=True)\n\n# 9. Use groupby() for Aggregation\ndf.groupby('Category').agg({'Value': 'sum'})\n\n# 10. Sort Values with sort_values()\ndf.sort_values(by='Column1', ascending=False)\n\n# 11. Use apply() for Custom Functions\ndf['new_column'] = df['Column1'].apply(lambda x: x * 2)\n\n# 12. Filter Rows Based on Conditions\ndf_filtered = df[df['Column1'] &gt; 10]\n\n# 13. Use query() for More Complex Filters\ndf.query('Column1 &gt; 10 and Column2 == \"Yes\"')\n\n# 14. Concatenate DataFrames with concat()\ndf_combined = pd.concat([df1, df2], axis=0)\n\n# 15. Merge DataFrames with merge()\ndf_merged = pd.merge(df1, df2, on='ID')\n\n# 16. Reset Index with reset_index()\ndf.reset_index(drop=True, inplace=True)\n\n# 17. Export Data to Excel with to_excel()\ndf.to_excel('output.xlsx', index=False)\n\n# 18. Save dataframe to CSV\ndf.to_csv('output.csv', index=False)\n\n# 19. Quickly Get Descriptive Statistics with describe()\ndf.describe()\n\n# 20. Lambda Functions on Columns\ndf['new_column'] = df.apply(lambda row: row['Column1'] + row['Column2'], axis=1)\n\n# 21. Drop Unwanted Columns with drop()\ndf.drop(columns=['Column1', 'Column2'], inplace=True)\n\n# 22. Random sampling \u2014 use sample()\ndf_sample = df.sample(n=5)\n\n# 23. Extract Year, Month, or Day from DateTime Columns\ndf['year'] = df['DateColumn'].dt.year\ndf['month'] = df['DateColumn'].dt.month\n\n# 24. Convert Column to Categorical Type\ndf['Category'] = df['Category'].astype('category')\n\n# 25. Find Duplicates Across Specific Columns\ndf[df.duplicated(subset=['Column1', 'Column2'])]\n\n# 26. Use shift() for Lagging Data\ndf['lagged_column'] = df['Value'].shift(1)\n\n# 27. Pivot Data with pivot_table()\ndf_pivot = df.pivot_table(values='Value', index='Category', columns='Region', ag\n\n# 28. Plot with Pandas\u2019 Built-In Plotting\nYou can also plot quick plots with pandas integrated with Matplotlib.\ndf['Value'].plot(kind='line')\n\n# 29. Working with Strings on the Columns\ndf['name'] = df['name'].str.lower()\n\n# 30. Make a New Column Using a Condition\nimport numpy as np\ndf['NewColumn'] = np.where(df['Value'] &gt; 10, 'High', 'Low')\n\n# 31. Using nunique() for Unique Values Count\ndf['Category'].nunique()\n\n# 32. Get Column Data Types with dtypes\ndf.dtypes\n\n# 33. Find Correlations with corr()\ndf.corr()\n\n# 34. Change Display Options\npd.set_option('display.max_rows', 100)\n\n35. Efficient Row-wise Operations\nfor idx, row in df.iterrows():\n    print(row['Column1'])\n</code></pre>"},{"location":"pandas/quick_examples.html#conclusion","title":"Conclusion","text":""},{"location":"pandas/select_filter.html","title":"Select and Filter","text":""},{"location":"pandas/select_filter.html#selecting-and-filtering-data","title":"Selecting and Filtering Data","text":"<p>Selecting and filtering data are fundamental operations when working with Pandas DataFrames. Pandas provides multiple methods for selecting specific rows and columns based on labels, positions, and conditions.</p>"},{"location":"pandas/select_filter.html#1-selecting-data","title":"1. Selecting Data","text":""},{"location":"pandas/select_filter.html#11-selecting-columns","title":"1.1 Selecting Columns","text":"<ul> <li>Using column names: <pre><code>import pandas as pd\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    \"Age\": [25, 30, 35, 40],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n})\n\n# Select a single column\nprint(df[\"Name\"])\n\n# Select multiple columns\nprint(df[[\"Name\", \"City\"]])\n</code></pre></li> </ul>"},{"location":"pandas/select_filter.html#12-selecting-rows","title":"1.2 Selecting Rows","text":"<ul> <li> <p>Using <code>.loc[]</code> (label-based indexing): <pre><code># Select a row by index label\nprint(df.loc[0])  # First row\n</code></pre></p> </li> <li> <p>Using <code>.iloc[]</code> (position-based indexing): <pre><code># Select a row by position\nprint(df.iloc[2])  # Third row\n</code></pre></p> </li> <li> <p>Selecting multiple rows: <pre><code>print(df.loc[0:2])  # Select rows 0 to 2\nprint(df.iloc[0:2])  # Select first two rows\n</code></pre></p> </li> </ul>"},{"location":"pandas/select_filter.html#2-filtering-data","title":"2. Filtering Data","text":"<p>Filtering is used to extract subsets of a DataFrame that meet specific conditions.</p>"},{"location":"pandas/select_filter.html#21-filtering-rows-based-on-condition","title":"2.1 Filtering Rows Based on Condition","text":"<pre><code># Select rows where Age &gt; 30\nprint(df[df[\"Age\"] &gt; 30])\n</code></pre>"},{"location":"pandas/select_filter.html#22-filtering-with-multiple-conditions","title":"2.2 Filtering with Multiple Conditions","text":"<p>Using <code>&amp;</code> (AND condition): <pre><code># Select rows where Age &gt; 30 and City is \"Chicago\"\nprint(df[(df[\"Age\"] &gt; 30) &amp; (df[\"City\"] == \"Chicago\")])\n</code></pre></p> <p>Using <code>|</code> (OR condition): <pre><code># Select rows where Age &lt; 30 or City is \"Houston\"\nprint(df[(df[\"Age\"] &lt; 30) | (df[\"City\"] == \"Houston\")])\n</code></pre></p> <p>Using <code>.isin()</code> for filtering specific values: <pre><code># Select rows where City is in [\"New York\", \"Houston\"]\nprint(df[df[\"City\"].isin([\"New York\", \"Houston\"])])\n</code></pre></p> <p>Using <code>.between()</code> for range-based filtering: <pre><code># Select rows where Age is between 25 and 35\nprint(df[df[\"Age\"].between(25, 35)])\n</code></pre></p> <p>Using <code>.query()</code> for SQL-like filtering: <pre><code>print(df.query(\"Age &gt; 30 and City == 'Chicago'\"))\n</code></pre></p>"},{"location":"pandas/select_filter.html#3-selecting-specific-rows-and-columns","title":"3. Selecting Specific Rows and Columns","text":"<pre><code># Select specific rows and columns\nprint(df.loc[df[\"Age\"] &gt; 30, [\"Name\", \"City\"]])\n</code></pre> <pre><code># Select first two rows and first two columns\nprint(df.iloc[0:2, 0:2])\n</code></pre>"},{"location":"pandas/select_filter.html#mini-tutorial-filtering-a-real-dataset","title":"Mini Tutorial: Filtering a Real Dataset","text":"<p>Let's assume we have a dataset <code>data.csv</code> with employee information:</p>"},{"location":"pandas/select_filter.html#step-1-load-the-dataset","title":"Step 1: Load the Dataset","text":"<pre><code>df = pd.read_csv(\"data.csv\")\n</code></pre>"},{"location":"pandas/select_filter.html#step-2-explore-the-data","title":"Step 2: Explore the Data","text":"<pre><code>print(df.head())  # View the first few rows\nprint(df.info())  # Get summary information\n</code></pre>"},{"location":"pandas/select_filter.html#step-3-apply-filtering","title":"Step 3: Apply Filtering","text":"<pre><code># Employees older than 40\nolder_employees = df[df[\"Age\"] &gt; 40]\n\n# Employees in the IT department\nit_employees = df[df[\"Department\"] == \"IT\"]\n\n# Employees with salary between 50K and 100K\nmid_salary_employees = df[df[\"Salary\"].between(50000, 100000)]\n\n# IT employees in New York\nny_it_employees = df[(df[\"Department\"] == \"IT\") &amp; (df[\"City\"] == \"New York\")]\n</code></pre>"},{"location":"pandas/select_filter.html#step-4-save-the-filtered-data","title":"Step 4: Save the Filtered Data","text":"<pre><code>ny_it_employees.to_csv(\"filtered_data.csv\", index=False)\n</code></pre>"},{"location":"pandas/select_filter.html#conclusion","title":"Conclusion","text":"<ul> <li>Use <code>.loc[]</code> and <code>.iloc[]</code> for row and column selection.</li> <li>Use conditional filtering with boolean operators (<code>&amp;</code>, <code>|</code>).</li> <li>Use <code>.query()</code> for SQL-like filtering.</li> <li>Use <code>.isin()</code> and <code>.between()</code> for specialized filtering.</li> </ul> <p>Would you like an example using PandasGUI for interactive filtering? \ud83d\ude80</p>"},{"location":"pandas/skimpy.html","title":"Skimpy","text":"<p>Here\u2019s what Skimpy offers: Data Shape: Shows the number of rows and columns. Column Data Types: Groups your columns by data type for clarity. Summary Statistics: Includes mean, median, and other key stats. Missing Values: Highlights missing data for each column. Visual Insights: Offers distribution charts to spot patterns quickly.</p>"},{"location":"pandas/skimpy.html#implementation","title":"Implementation","text":"<p>Install <pre><code>pip install polars==0.18.4 \npip install summarytools \npip install skimpy\n</code></pre></p> <p>Example</p> <pre><code>import polars as pl\nimport pandas as pd\nimport seaborn as sns\nfrom summarytools import dfSummary\nfrom skimpy import skim\ndf_pd = sns.load_dataset('iris')\ndf_pl = pl.from_pandas(df_pd)\nskim(df_pd)\n\nOpeninapp\nOpen in app\nskim(df_pl) # works with Polars DataFrame\nSearch Write\ndfSummary(df_pd)\n</code></pre> <p>Illustration: </p> <pre><code>from skimpy import skim\nimport pandas as pd\n# Create a sample DataFrame\ndata = {'Age': [25, 30, 35, 40, None], \n        'Salary': [50000, 60000, 70000, 80000, 90000], \n        'Department': ['HR', 'IT', 'Finance', 'IT', 'HR']}\ndf = pd.DataFrame(data)\n\n# Generate a summary\nskim(df)\nWhen you run this code in a Jupyter Notebook, Skimpy creates a beautiful,\nstructured report that\u2019s way better than the plain output of df.describe().\nBonus: Skimpy also works with Polars, which is a fast and efficient\nalternative to Pandas for large datasets.\nHere\u2019s a quick example of how to use Skimpy to summarize a dataset. We\u2019ll\ncreate a sample dataset and use Skimpy to generate a comprehensive\nsummary.\nStep 1: Install Skimpy\nFirst, ensure that you have the Skimpy library installed. You can install it\nusing pip:\npip install skimpy\nStep 2: Import Libraries and Create a Sample Dataset\nWe\u2019ll use Pandas to create a DataFrame, then summarize it with Skimpy.\nfrom skimpy import skim\nimport pandas as pd\n# Create a sample dataset\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, None],\n    'Salary': [50000, 60000, 70000, 80000, 90000],\n    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR'],\n    'Joining Date': ['2020-01-15', '2019-06-20', '2021-03-10', '2018-12-25', '20\n}\n# Convert Joining Date to datetime\ndf = pd.DataFrame(data)\ndf['Joining Date'] = pd.to_datetime(df['Joining Date'])\nStep 3: Generate the Summary\nUsing Skimpy is straightforward. Pass the DataFrame to the skim() function.\n# Generate a summary of the dataset\nskim(df)\n</code></pre> <p>What You\u2019ll Get: <pre><code>The output will include:\nGeneral Overview: Number of rows, columns, and missing values.\nData Types: Organized by type (e.g., numeric, categorical).\nStatistics: Mean, median, min, max, and standard deviation for numeric\ncolumns.\nUnique Values: For categorical columns.\nDistribution Insights: Charts for numeric columns (when supported in\nthe environment).\n</code></pre></p> <pre><code>import polars as pl\nfrom skimpy import skim\n# Create a Polars DataFrame\ndata = pl.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, None],\n    'Salary': [50000, 60000, 70000, 80000, 90000],\n    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR'],\n    'Joining Date': ['2020-01-15', '2019-06-20', '2021-03-10', '2018-12-25', '20\n})\n# Summarize the dataset\nskim(data)\n</code></pre>"},{"location":"pandas/summarizer.html","title":"SummaryTools","text":"<p>SummaryTools is another library that provides a detailed overview of your dataset. It\u2019s similar to Skimpy but adds a few extra features, like: Collapsible Summaries: Perfect for when you have large datasets and want a clean overview. Tabbed Summaries: Makes it easy to switch between different views of your data.</p>"},{"location":"pandas/summarizer.html#implamentation","title":"Implamentation","text":"<p>Here\u2019s how to use SummaryTools:</p> <pre><code>from summarytools import dfSummary\nimport pandas as pd\n# Create a sample DataFrame\ndata = {'Age': [25, 30, 35, 40, None], \n        'Salary': [50000, 60000, 70000, 80000, 90000], \n        'Department': ['HR', 'IT', 'Finance', 'IT', 'HR']}\ndf = pd.DataFrame(data)\n# Generate a collapsible summary\nsummary = dfSummary(df)\nsummary.to_notebook()  # Use this to display the report in Jupyter Notebook\n</code></pre> <pre><code>from summarytools import dfSummary\nimport pandas as pd\n# Create a sample dataset\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, None],\n    'Salary': [50000, 60000, 70000, 80000, 90000],\n    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR'],\n    'Joining Date': ['2020-01-15', '2019-06-20', '2021-03-10', '2018-12-25', '20\n}\ndf = pd.DataFrame(data)\ndf['Joining Date'] = pd.to_datetime(df['Joining Date'])  # Convert Joining Date \n\n# Step 3: Generate a Summary with SummaryTools\nUse the dfSummary function to create a summary and display it in a Jupyter\nNotebook.\n# Generate a summary of the dataset\nsummary = dfSummary(df) \n# Display the summary in a Jupyter Notebook\nsummary.to_notebook()\n</code></pre> <pre><code>What the Summary Includes\nOverview: Number of rows, columns, and missing values.\nColumn-Level Insights:\nData type.\nMean, median, and standard deviation for numeric columns.\nUnique values for categorical columns.\nDistribution charts (where applicable).\nInteractive Features:\nCollapsible sections for large datasets.\nTabbed views for switching between summaries.\n</code></pre> <p>Example</p> <pre><code>Example Output\nGeneral Overview:\nRows: 5\nColumns: 5\nMissing Values: Age (1 missing)\nColumn Details:\nAdvanced Features\nSave the Summary to HTML\nYou can save the summary report as an HTML file for sharing:\nsummary.to_html('dataset_summary.html')\nExport as JSON or CSV\nYou can export the data insights for programmatic use:\nsummary.to_json('dataset_summary.json') \nsummary.to_csv('dataset_summary.csv')\n</code></pre>"},{"location":"python/00_Contents.html","title":"Contents","text":"<p>Welcome to this comprehensive Python guide, specifically tailored for data scientists, machine learning experts, and predictive modeling specialists. This resource aims to equip you with best practices, detailed examples, and practical use cases to streamline your data science projects and enhance your productivity. Whether you are a seasoned professional or a beginner looking to deepen your expertise, you'll find structured, actionable information to elevate your skills in Python programming, data analysis, and predictive modeling.</p> <p>Chapters</p> <ul> <li>01 Overview</li> <li>02 Setup &amp; Installation</li> <li>03 Core Concepts</li> <li>04 Advanced Concepts</li> <li>05 Syntax and Variables</li> <li>06 Control Flow</li> <li>07 Data Structures</li> <li>08 Functions and Modules</li> <li>09 Object-Oriented Programming</li> <li>10 Python Standard Library</li> <li>11 File Handling and I/O</li> <li>12 Exception Handling and Debugging</li> <li>13 Web Development</li> <li>14 Data Science</li> <li>15 Machine Learning and AI</li> <li>16 Predictive Modeling</li> <li>17 Databases</li> </ul> <p>Future - 15 \ud83c\udfae Game Development (Pygame, Godot) - 16 \ud83d\udd12 Cybersecurity &amp; Ethical Hacking - 17 \ud83e\udd16 Automation &amp; Scripting - 18 \ud83d\udd78\ufe0f Web Scraping (BeautifulSoup, Scrapy) - 19 \ud83c\udfa8 GUI Development (Tkinter, PyQt) - 20 \ud83d\ude80 Performance Optimization &amp; Best Practices - 21 \ud83c\udfed Working with Databases (SQLite, PostgreSQL, MySQL) - 22 \ud83c\udfd7\ufe0f API Development &amp; RESTful Services - 23 \ud83c\udf0d Deployment &amp; Cloud (AWS, Docker, Heroku) - 24 \ud83d\udce6 Virtual Environments &amp; Package Management - 25 \ud83d\udcdd Testing &amp; Debugging (unittest, pytest) - 26 \u23f3 Concurrency &amp; Parallel Processing - 27 \u2699\ufe0f DevOps &amp; CI/CD for Python Projects - 28 \ud83d\udd04 Functional Programming in Python - 29 \ud83e\uddea Scientific Computing (SciPy, SymPy) - 30 \ud83d\udcdc Writing Pythonic Code &amp; Best Practices</p>"},{"location":"python/00_Contents_Jazzy.html","title":"Python for Data Science &amp; Predictive Modeling","text":"docs/python/images/python_logo.jpg    Welcome! <p>This comprehensive Python guide is specifically tailored for data scientists, machine learning experts, and predictive modeling specialists. Learn best practices, detailed examples, and practical use cases to streamline your projects and boost your productivity.</p> Get Started"},{"location":"python/00_Contents_Jazzy.html#chapters","title":"\ud83d\udcd6 Chapters","text":"01 Overview 02 Setup &amp; Installation 03 Core Concepts 04 Advanced Concepts 05 Syntax and Variables 06 Control Flow 07 Data Structures 08 Functions and Modules 09 Object-Oriented Programming 10 Python Standard Library 11 File Handling and I/O 12 Exception Handling &amp; Debugging 13 Web Development 14 Data Science 15 Machine Learning and AI 16 Predictive Modeling 17 Databases    Ready to dive in? Start your Python journey now!"},{"location":"python/00_landing.html","title":"Welcome","text":"<p>This comprehensive Python guide is specifically tailored for data scientists, machine learning experts, and predictive modeling specialists. Learn best practices, detailed examples, and practical use cases to streamline your projects and boost your productivity.</p> Get Started"},{"location":"python/01_Overview.html","title":"01 \ud83d\udcd8 Overview","text":"<p>Python is a high-level, interpreted programming language known for its simplicity and readability. Created by   Guido van Rossum   and first released in 1991, Python has become one of the most popular languages due to its ease of use, vast ecosystem, and broad applicability.</p> <p>Python supports multiple programming paradigms, including   procedural, object-oriented, and functional programming  , making it a versatile choice for a variety of applications, such as   web development, data science, artificial intelligence, automation, and scientific computing  .</p>"},{"location":"python/01_Overview.html#features-of-python","title":"Features of Python","text":""},{"location":"python/01_Overview.html#1-simple-readable-syntax","title":"1. Simple &amp; Readable Syntax","text":"<ul> <li>Python\u2019s syntax is designed to be   easy to read and write  , reducing the learning curve for beginners.</li> <li>Code readability is emphasized with   indentation-based structuring   (instead of braces <code>{}</code> like in C/C++ or Java).</li> </ul>"},{"location":"python/01_Overview.html#2-interpreted-language","title":"2. Interpreted Language","text":"<ul> <li>Python is an   interpreted language  , meaning code is executed   line by line   without requiring compilation.</li> <li>This makes debugging easier but may affect execution speed compared to compiled languages.</li> </ul>"},{"location":"python/01_Overview.html#3-dynamically-typed","title":"3. Dynamically Typed","text":"<ul> <li>You   don\u2019t need to declare data types   explicitly; Python automatically detects them at runtime.</li> </ul> <pre><code>x = 10  # Integer\ny = \"Hello\"  # String\nz = 3.14  # Float\n</code></pre>"},{"location":"python/01_Overview.html#4-object-oriented-functional-programming","title":"4. Object-Oriented &amp; Functional Programming","text":"<ul> <li>Supports   object-oriented programming (OOP)  , allowing for encapsulation, inheritance, and polymorphism.</li> <li>Also supports   functional programming  , allowing the use of   map, filter, lambda functions, and higher-order functions  .</li> </ul>"},{"location":"python/01_Overview.html#5-cross-platform-compatibility","title":"5. Cross-Platform Compatibility","text":"<ul> <li>Python is   portable   and can run on   Windows, macOS, Linux, and even embedded systems   without modification.</li> </ul>"},{"location":"python/01_Overview.html#6-large-standard-library","title":"6. Large Standard Library","text":"<ul> <li>Python comes with a   rich standard library   that provides modules for file handling, networking, regular expressions, data structures, and more.</li> </ul> <pre><code>import math  # Using the built-in math module\nprint(math.sqrt(16))  # Output: 4.0\n</code></pre>"},{"location":"python/01_Overview.html#7-extensive-third-party-libraries","title":"7. Extensive Third-Party Libraries","text":"<ul> <li>Python has a vast ecosystem of libraries, such as:</li> <li>NumPy, Pandas, Matplotlib   (for Data Science)</li> <li>TensorFlow, PyTorch, Scikit-learn   (for Machine Learning &amp; AI)</li> <li>Flask, Django, FastAPI   (for Web Development)</li> <li>Requests, BeautifulSoup, Scrapy   (for Web Scraping)</li> <li>PyQt, Tkinter   (for GUI Development)</li> </ul>"},{"location":"python/01_Overview.html#8-automatic-memory-management","title":"8. Automatic Memory Management","text":"<ul> <li>Python handles memory allocation and deallocation   automatically   using   Garbage Collection (GC)  .</li> </ul>"},{"location":"python/01_Overview.html#9-multi-purpose-language","title":"9. Multi-Purpose Language","text":"<ul> <li>Used for:</li> <li>Web Development  </li> <li>Data Science &amp; Analytics  </li> <li>Machine Learning &amp; AI  </li> <li>Automation &amp; Scripting  </li> <li>Cybersecurity  </li> <li>Game Development  </li> <li>Embedded Systems (MicroPython, Raspberry Pi)  </li> </ul>"},{"location":"python/01_Overview.html#operating-systems","title":"Operating Systems","text":"<p>Python is a highly portable language that runs on a wide variety of operating systems including:</p> <ul> <li>Windows \u2013 Supports Windows 10, 11, and older versions (7, 8, Server editions).</li> <li>macOS \u2013 Available on Intel and Apple Silicon (M1, M2, M3 chips).</li> <li>Linux \u2013 Supports major distributions:</li> <li>Ubuntu</li> <li>Debian</li> <li>Fedora</li> <li>CentOS</li> <li>Red Hat Enterprise Linux (RHEL)</li> <li>Arch Linux</li> <li>openSUSE</li> <li>Manjaro, etc.</li> <li>Unix-based OS:</li> <li>FreeBSD</li> <li>OpenBSD</li> <li>NetBSD</li> <li>Solaris</li> <li>AIX (IBM Unix)</li> <li>Android \u2013 Python can run via Termux or custom builds.</li> <li>iOS/iPadOS \u2013 Python can be used via apps like Pythonista or Pyto.</li> </ul>"},{"location":"python/01_Overview.html#platforms","title":"Platforms","text":"<p>Platforms include:</p> <ul> <li>x86 (32-bit and 64-bit) \u2013 Common on Windows, Linux, and older macOS systems.</li> <li>ARM (32-bit and 64-bit) \u2013 Used in Raspberry Pi, Android devices, and Apple Silicon (via native builds).</li> <li>RISC-V \u2013 Growing support for open-source hardware.</li> <li>Web (Browser-based execution via Pyodide or Brython).</li> <li>Embedded Systems (Microcontrollers like Raspberry Pi Pico, ESP32 using MicroPython or CircuitPython).</li> <li>Mainframes (IBM z/OS supports Python for enterprise applications).</li> <li>Cloud Platforms \u2013 Runs on AWS, Azure, Google Cloud, and other cloud environments.</li> <li>Docker &amp; Containers \u2013 Python is widely used in containerized environments.</li> <li>Virtual Machines \u2013 Can run inside VMs like VirtualBox, VMware, and Hyper-V.</li> </ul> <p>Python\u2019s versatility ensures it can run on almost any modern computing environment.</p>"},{"location":"python/01_Overview.html#conclusion","title":"Conclusion","text":"<p>Python is a powerful, easy-to-learn language with a vast ecosystem, making it suitable for beginners and professionals alike. Its   simplicity, flexibility, and extensive libraries   make it a top choice for   AI, web development, data science, and automation  . \ud83d\ude80</p>"},{"location":"python/02_Setup_Installation.html","title":"02 \ud83d\udee0\ufe0f Setup and Installation","text":""},{"location":"python/02_Setup_Installation.html#environment-setup","title":"Environment Setup","text":""},{"location":"python/02_Setup_Installation.html#python-installation","title":"Python Installation","text":"<p>Setting up Python correctly is essential. We recommend downloading the latest stable Python version from the official Python website. Ensure you select the appropriate version compatible with your operating system (Windows, macOS, Linux). Follow the installer instructions, selecting the option to add Python to your PATH environment variable.</p> <p>Verify your installation by running:</p> <pre><code>python --version\n</code></pre>"},{"location":"python/02_Setup_Installation.html#virtual-environment","title":"Virtual Environment","text":"<p>Using virtual environments helps isolate your project dependencies and avoids conflicts.</p>"},{"location":"python/02_Setup_Installation.html#pip","title":"pip","text":"<p><code>pip</code> is Python's package installer, allowing easy management of packages. It comes bundled with Python 3 by default. Verify pip installation by running:</p> <pre><code>pip --version\n</code></pre>"},{"location":"python/02_Setup_Installation.html#virtual-environment-venv","title":"Virtual Environment (venv)","text":"<p>Virtual environments isolate project dependencies. Set up a virtual environment with Python's built-in <code>venv</code>:</p>"},{"location":"python/02_Setup_Installation.html#creating-and-activating-a-virtual-environment","title":"Creating and Activating a Virtual Environment","text":"<pre><code>python -m venv myenv\n\n# Activate the environment\n# On Windows:\nmyenv\\Scripts\\activate\n\n# On macOS/Linux\nsource myenv/bin/activate\n</code></pre> <p>VSCode</p> <p>In VSCode the default library when Creating Environment is .venv instead of venv</p>"},{"location":"python/02_Setup_Installation.html#installing-packages","title":"Installing Packages","text":"<p>Once activated, install packages using:</p> <pre><code>pip install numpy pandas matplotlib\n</code></pre>"},{"location":"python/02_Setup_Installation.html#integrated-development-environment","title":"Integrated Development Environment","text":"<p>Choosing the right Integrated Development Environment (IDE) is crucial for efficiency.</p>"},{"location":"python/02_Setup_Installation.html#idle","title":"IDLE","text":"<p>IDLE comes bundled with Python and is suitable for basic scripting and quick experiments.</p> <ul> <li>Launch IDLE from your Python installation.</li> <li>Provides a straightforward interactive shell and simple editor.</li> <li>Ideal for beginners or small tasks.</li> </ul>"},{"location":"python/02_Setup_Installation.html#vscode","title":"VSCode","text":"<ul> <li>Highly recommended for professional development.</li> <li>Feature-rich editor with extensive Python support through extensions.</li> <li>Easy integration with Jupyter Notebooks, Git, and debugging tools.</li> <li>Installation: VSCode.</li> </ul>"},{"location":"python/02_Setup_Installation.html#pycharm","title":"PyCharm","text":"<p>A powerful, dedicated Python IDE ideal for larger, complex projects.</p> <ul> <li>Excellent code completion, debugging, and version control integration.</li> <li>Offers Community (free) and Professional (paid) editions.</li> </ul>"},{"location":"python/02_Setup_Installation.html#jupyterlab","title":"JupyterLab","text":"<p>Web-based IDE highly popular among data scientists.</p> <ul> <li>Combines notebooks, terminal, text editors, and visualization in one interface.</li> <li>Perfect for interactive data exploration, visualization, and documentation.</li> <li>Install via pip:</li> </ul> <pre><code>pip install jupyterlab\n</code></pre> <p>Launch by running:</p> <pre><code>jupyter lab\n</code></pre> <p>This setup ensures a solid foundation to maximize productivity in your Python-based projects.</p> <p>Bug</p> <p>Add topic Python CLI</p>"},{"location":"python/03_Core_Concepts.html","title":"03 \u2699\ufe0f Core Concepts","text":""},{"location":"python/03_Core_Concepts.html#basic","title":"Basic","text":""},{"location":"python/03_Core_Concepts.html#1-variables-data-types","title":"1. Variables &amp; Data Types","text":"<p>Python supports multiple data types:</p> <pre><code>x = 10         # Integer\ny = 3.14       # Float\nz = \"Python\"   # String\na = True       # Boolean\nb = [1, 2, 3]  # List\nc = (4, 5, 6)  # Tuple\nd = {\"key\": \"value\"}  # Dictionary\n</code></pre>"},{"location":"python/03_Core_Concepts.html#2-conditional-statements","title":"2. Conditional Statements","text":"<pre><code>x = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelif x == 5:\n    print(\"x is 5\")\nelse:\n    print(\"x is less than 5\")\n</code></pre>"},{"location":"python/03_Core_Concepts.html#3-loops-for-while","title":"3. Loops (For &amp; While)","text":"<pre><code># For loop\nfor i in range(5):\n    print(i)\n\n# While loop\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1\n</code></pre>"},{"location":"python/03_Core_Concepts.html#4-functions","title":"4. Functions","text":"<pre><code>def greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"Ted\"))  # Output: Hello, Ted!\n</code></pre>"},{"location":"python/03_Core_Concepts.html#5-object-oriented-programming-oop","title":"5. Object-Oriented Programming (OOP)","text":"<pre><code>class Car:\n    def __init__(self, brand, model):\n        self.brand = brand\n        self.model = model\n\n    def display(self):\n        return f\"Car: {self.brand} {self.model}\"\n\ncar1 = Car(\"Toyota\", \"Corolla\")\nprint(car1.display())  # Output: Car: Toyota Corolla\n</code></pre>"},{"location":"python/03_Core_Concepts.html#6-exception-handling","title":"6. Exception Handling","text":"<pre><code>try:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\nfinally:\n    print(\"Execution completed.\")\n</code></pre>"},{"location":"python/03_Core_Concepts.html#7-file-handling","title":"7. File Handling","text":"<pre><code>with open(\"example.txt\", \"w\") as file:\n    file.write(\"Hello, Python!\")\n\nwith open(\"example.txt\", \"r\") as file:\n    content = file.read()\n    print(content)\n</code></pre>"},{"location":"python/03_Core_Concepts.html#8-modules-libraries","title":"8. Modules &amp; Libraries","text":"<pre><code>import math\nprint(math.factorial(5))  # Output: 120\n</code></pre> <p>Mastering these core Python concepts enables Data Scientists, Machine Learning Engineers, and AI developers to build efficient, scalable, and high-performance solutions. \ud83d\ude80</p> <p>Later,  code challenges or exercises to reinforce these topics will be covered. \ud83e\udd14</p>"},{"location":"python/04_Advanced_Concepts.html","title":"04 \u26a1 Advanced Concepts","text":"<p>Python's versatility makes it the go-to language for Data Science, Machine Learning, and AI. Mastering core concepts such as Object-Oriented Programming (OOP), Decorators, Generators, Iterators, Comprehensions, Multithreading, and Asynchronous Programming is crucial for writing efficient, scalable, and maintainable code.</p> <p>This chapter covers essential Python concepts that empower professionals in AI and data science to build optimized pipelines, parallel computations, and reusable components.</p>"},{"location":"python/04_Advanced_Concepts.html#object-oriented-programming-oop","title":"\ud83c\udfd7\ufe0f Object-Oriented Programming (OOP)","text":"<p>Object-Oriented Programming (OOP) is a programming paradigm that enables modularity, code reusability, and encapsulation. It is widely used in Machine Learning for building models, creating reusable components, and managing data pipelines.</p>"},{"location":"python/04_Advanced_Concepts.html#key-oop-concepts","title":"\ud83d\udd39 Key OOP Concepts","text":"<ul> <li>Class &amp; Object \u2013 A class is a blueprint, and an object is an instance of a class.</li> <li>Encapsulation \u2013 Restrict direct access to variables and protect data integrity.</li> <li>Inheritance \u2013 Reuse attributes and methods from a parent class.</li> <li>Polymorphism \u2013 Different classes can implement the same method.</li> </ul>"},{"location":"python/04_Advanced_Concepts.html#example-oop-in-machine-learning","title":"\ud83d\udd39 Example: OOP in Machine Learning","text":"<pre><code>class Model:\n    def __init__(self, name):\n        self.name = name\n\n    def train(self):\n        print(f\"{self.name} model is training...\")\n\nclass NeuralNetwork(Model):\n    def train(self):\n        print(f\"Training deep learning model: {self.name}\")\n\n# Usage\nmodel1 = Model(\"Linear Regression\")\nmodel2 = NeuralNetwork(\"CNN\")\n\nmodel1.train()  # Output: Linear Regression model is training...\nmodel2.train()  # Output: Training deep learning model: CNN\n</code></pre> <p>\u2705 Use Case: OOP allows structured design in ML model pipelines, hyperparameter tuning, and deployment frameworks.</p>"},{"location":"python/04_Advanced_Concepts.html#decorators-generators","title":"\ud83c\udfad Decorators &amp; Generators","text":"<p>Python decorators and generators help optimize code efficiency, making them essential in data pipelines and AI model training.</p>"},{"location":"python/04_Advanced_Concepts.html#decorators-function-wrappers","title":"\ud83d\udd39 Decorators (Function Wrappers)","text":"<p>Decorators modify the behavior of functions without changing their code.</p> <p>Example: Timing an ML Function</p> <pre><code>import time\n\ndef timer(func):\n    def wrapper(*args, kwargs):\n        start = time.time()\n        result = func(*args, kwargs)\n        end = time.time()\n        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n        return result\n    return wrapper\n\n@timer\ndef train_model():\n    time.sleep(2)  # Simulating model training time\n    print(\"Model training complete!\")\n\ntrain_model()\n</code></pre> <p>\u2705 Use Case: Logging, debugging, measuring execution time, monitoring ML pipelines.</p>"},{"location":"python/04_Advanced_Concepts.html#generators-memory-efficient-iteration","title":"\ud83d\udd39 Generators (Memory-Efficient Iteration)","text":"<p>Generators are functions that return iterators lazily, saving memory when handling large datasets.</p> <p>Example: Processing Large Data Efficiently</p> <pre><code>def read_large_file(file_path):\n    with open(file_path, \"r\") as file:\n        for line in file:\n            yield line  # Yields one line at a time\n\n# Usage\nfor line in read_large_file(\"data.csv\"):\n    process(line)  # Process each line lazily\n</code></pre> <p>\u2705 Use Case: Streaming large datasets, real-time data processing in AI applications.</p>"},{"location":"python/04_Advanced_Concepts.html#iterators-comprehensions","title":"\ud83d\udd17 Iterators &amp; Comprehensions","text":"<p>Efficient data handling is critical in Machine Learning when working with large datasets and feature engineering.</p>"},{"location":"python/04_Advanced_Concepts.html#iterators","title":"\ud83d\udd39 Iterators","text":"<p>An iterator is an object that allows traversal of elements one at a time.</p> <p>Example: Custom Data Iterator</p> <pre><code>class DataLoader:\n    def __init__(self, data):\n        self.data = data\n        self.index = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.index &gt;= len(self.data):\n            raise StopIteration\n        result = self.data[self.index]\n        self.index += 1\n        return result\n\n# Usage\ndata = DataLoader([\"image1\", \"image2\", \"image3\"])\nfor d in data:\n    print(d)\n</code></pre> <p>\u2705 Use Case: Data loaders, streaming large datasets for ML models.</p>"},{"location":"python/04_Advanced_Concepts.html#list-dict-and-set-comprehensions","title":"\ud83d\udd39 List, Dict, and Set Comprehensions","text":"<p>Comprehensions make data transformation concise and are widely used in feature engineering and preprocessing.</p> <pre><code># Convert temperature from Celsius to Fahrenheit using list comprehension\ncelsius = [0, 10, 20, 30]\nfahrenheit = [((temp * 9/5) + 32) for temp in celsius]\nprint(fahrenheit)  # Output: [32.0, 50.0, 68.0, 86.0]\n</code></pre> <p>\u2705 Use Case: Feature scaling, data transformation, filtering large datasets.</p>"},{"location":"python/04_Advanced_Concepts.html#multithreading-multiprocessing","title":"\ud83d\udd04 Multithreading &amp; Multiprocessing","text":"<p>Parallel execution is essential for speeding up computations in AI and ML.</p>"},{"location":"python/04_Advanced_Concepts.html#multithreading-efficient-for-io-bound-tasks","title":"\ud83d\udd39 Multithreading (Efficient for I/O-bound tasks)","text":"<p>Example: Fetching multiple datasets in parallel</p> <pre><code>import threading\n\ndef fetch_data(source):\n    print(f\"Fetching from {source}\")\n\nsources = [\"dataset1.csv\", \"dataset2.csv\", \"dataset3.csv\"]\nthreads = [threading.Thread(target=fetch_data, args=(src,)) for src in sources]\n\nfor thread in threads:\n    thread.start()\nfor thread in threads:\n    thread.join()\n</code></pre> <p>\u2705 Use Case: Downloading datasets, web scraping, handling I/O-heavy tasks.</p>"},{"location":"python/04_Advanced_Concepts.html#multiprocessing-efficient-for-cpu-bound-tasks","title":"\ud83d\udd39 Multiprocessing (Efficient for CPU-bound tasks)","text":"<p>Multiprocessing utilizes multiple CPU cores, making it ideal for heavy computations.</p> <p>Example: Parallel Model Training</p> <pre><code>from multiprocessing import Pool\n\ndef train_model(model_id):\n    return f\"Training model {model_id}\"\n\nmodels = [1, 2, 3, 4]\nwith Pool(4) as p:\n    results = p.map(train_model, models)\nprint(results)\n</code></pre> <p>\u2705 Use Case: Parallel model training, large dataset processing, hyperparameter tuning.</p>"},{"location":"python/04_Advanced_Concepts.html#async-await","title":"\ud83e\uddf5 Async &amp; Await","text":"<p>Asynchronous programming is critical for handling large-scale web-based AI applications, real-time data processing, and API calls.</p>"},{"location":"python/04_Advanced_Concepts.html#async-for-efficient-io-operations","title":"\ud83d\udd39 Async for Efficient I/O Operations","text":"<pre><code>import asyncio\n\nasync def fetch_data():\n    print(\"Fetching data...\")\n    await asyncio.sleep(2)  # Simulating delay\n    print(\"Data fetched!\")\n\nasync def main():\n    await asyncio.gather(fetch_data(), fetch_data(), fetch_data())\n\nasyncio.run(main())\n</code></pre> <p>\u2705 Use Case: Handling multiple API requests, web scraping for AI datasets, real-time ML monitoring.</p>"},{"location":"python/04_Advanced_Concepts.html#summary","title":"\ud83d\ude80 Summary","text":"Concept Use Case \ud83c\udfd7\ufe0f OOP ML model architecture, data pipeline design \ud83c\udfad Decorators Logging, debugging, function optimization \ud83c\udfad Generators Handling large datasets efficiently \ud83d\udd17 Iterators Streaming datasets, loading ML batches \ud83d\udd17 Comprehensions Feature engineering, data transformation \ud83d\udd04 Multithreading I/O-bound tasks (API calls, web scraping) \ud83d\udd04 Multiprocessing CPU-bound tasks (ML training, parallel computations) \ud83e\uddf5 Async/Await Real-time AI applications, non-blocking API calls"},{"location":"python/04_Advanced_Concepts.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Mastering these core Python concepts enables Data Scientists, Machine Learning Engineers, and AI developers to build efficient, scalable, and high-performance solutions. \ud83d\ude80</p> <p>Would you like to add code challenges or exercises to reinforce these topics? \ud83e\udd14</p>"},{"location":"python/05_Syntax_and_Variables.html","title":"05 \ud83d\udcdd Syntax and Variables","text":"<p>Python's simple and readable syntax makes it a favorite among beginners and experts alike. Understanding its basic syntax and variable handling is essential for writing clean and efficient code.</p>"},{"location":"python/05_Syntax_and_Variables.html#31-python-syntax-basics","title":"\ud83d\udd39 3.1 Python Syntax Basics","text":"<p>Python follows an indentation-based syntax rather than using <code>{}</code> like C or Java. This makes the code cleaner and more readable.</p>"},{"location":"python/05_Syntax_and_Variables.html#example-python-syntax","title":"\u2705 Example: Python Syntax","text":"<pre><code># Correct indentation\nif True:\n    print(\"Hello, Python!\")  # Indented block\n</code></pre> <pre><code># \u274c Incorrect indentation (will raise an error)\nif True:\nprint(\"Hello, Python!\")  # IndentationError\n</code></pre>"},{"location":"python/05_Syntax_and_Variables.html#key-features-of-python-syntax","title":"\ud83d\udee0 Key Features of Python Syntax","text":"<ul> <li>No curly braces <code>{}</code> for blocks\u2014indentation matters!</li> <li>No need for semicolons <code>;</code> at the end of statements.</li> <li>Uses <code>#</code> for single-line comments and <code>\"\"\" \"\"\"</code> for multi-line comments.</li> </ul>"},{"location":"python/05_Syntax_and_Variables.html#32-variables-in-python","title":"\ud83d\udd39 3.2 Variables in Python","text":"<p>Variables in Python store data and do not require explicit type declaration. Python is dynamically typed, meaning the data type is determined at runtime.</p>"},{"location":"python/05_Syntax_and_Variables.html#declaring-variables","title":"\u2705 Declaring Variables","text":"<pre><code>name = \"Alice\"       # String\nage = 25            # Integer\nheight = 5.9        # Float\nis_student = True   # Boolean\n</code></pre>"},{"location":"python/05_Syntax_and_Variables.html#rules-for-variable-naming","title":"\ud83d\udd39 Rules for Variable Naming","text":"<p>\u2705 Allowed:</p> <ul> <li>Can start with a letter or underscore <code>_</code></li> <li>Can contain letters, numbers, and underscores</li> <li>Case-sensitive (<code>Age</code> and <code>age</code> are different)</li> </ul> <p>\u274c Not Allowed:</p> <ul> <li>Cannot start with a number (<code>2name \u274c</code>)</li> <li>Cannot use special characters (<code>@name \u274c</code>)</li> </ul>"},{"location":"python/05_Syntax_and_Variables.html#multiple-variable-assignment","title":"\ud83d\udd39 Multiple Variable Assignment","text":"<pre><code>a, b, c = 1, 2, \"Python\"\nprint(a, b, c)  # Output: 1 2 Python\n</code></pre> <p>\u2705 Use Case: Quick assignment of multiple values.</p>"},{"location":"python/05_Syntax_and_Variables.html#33-data-types-in-python","title":"\ud83d\udd39 3.3 Data Types in Python","text":"<p>Python provides built-in data types for handling different kinds of values.</p> Type Example Description <code>int</code> <code>x = 10</code> Whole numbers <code>float</code> <code>y = 3.14</code> Decimal numbers <code>str</code> <code>s = \"Python\"</code> Text/String <code>bool</code> <code>b = True</code> Boolean (True/False) <code>list</code> <code>l = [1,2,3]</code> Ordered, mutable collection <code>tuple</code> <code>t = (1,2,3)</code> Ordered, immutable collection <code>dict</code> <code>d = {\"key\": \"value\"}</code> Key-value pairs <code>set</code> <code>s = {1,2,3}</code> Unordered unique elements <p>\u2705 Use Case: Storing structured data, lists, and key-value mappings.</p>"},{"location":"python/05_Syntax_and_Variables.html#34-type-conversion","title":"\ud83d\udd39 3.4 Type Conversion","text":"<p>Python allows explicit type conversion (casting) when needed.</p> <pre><code>x = 5          # Integer\ny = str(x)     # Convert to string\nz = float(x)   # Convert to float\nprint(y, z)    # Output: '5' 5.0\n</code></pre> <p>\u2705 Use Case: Ensuring correct data formats in ML/DL models and databases.</p>"},{"location":"python/05_Syntax_and_Variables.html#35-string-manipulation","title":"\ud83d\udd39 3.5 String Manipulation","text":"<p>Python strings (<code>str</code>) support multiple operations.</p> <pre><code>name = \"Python\"\nprint(name.upper())   # PYTHON\nprint(name.lower())   # python\nprint(name[0:3])      # Pyt (Slicing)\n</code></pre> <p>\u2705 Use Case: Data cleaning in text processing and NLP.</p>"},{"location":"python/05_Syntax_and_Variables.html#36-user-input","title":"\ud83d\udd39 3.6 User Input","text":"<p>Python allows reading user input using <code>input()</code>.</p> <pre><code>name = input(\"Enter your name: \")\nprint(\"Hello, \" + name + \"!\")\n</code></pre> <p>\u2705 Use Case: Interactive Python applications and CLI tools.</p>"},{"location":"python/05_Syntax_and_Variables.html#37-constants-in-python","title":"\ud83d\udd39 3.7 Constants in Python","text":"<p>Python doesn\u2019t have built-in constants, but by convention, uppercase names are used.</p> <pre><code>PI = 3.1416  # Treated as a constant\n</code></pre> <p>\u2705 Use Case: Defining scientific constants.</p>"},{"location":"python/05_Syntax_and_Variables.html#38-f-strings-for-string-formatting","title":"\ud83d\udd39 3.8 f-Strings for String Formatting","text":"<pre><code>name = \"Alice\"\nage = 25\nprint(f\"My name is {name} and I am {age} years old.\")\n</code></pre> <p>\u2705 Use Case: Readable string interpolation.</p>"},{"location":"python/05_Syntax_and_Variables.html#summary","title":"\ud83d\ude80 Summary","text":"Concept Key Takeaway Python Syntax Uses indentation instead of <code>{}</code> Variables Dynamically typed, no explicit declaration needed Data Types Includes <code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>list</code>, <code>dict</code>, etc. Type Conversion Use <code>str()</code>, <code>int()</code>, <code>float()</code> for casting String Manipulation Supports <code>.upper()</code>, <code>.lower()</code>, slicing, and f-strings User Input <code>input()</code> for user interaction Constants Uppercase variable names conventionally used"},{"location":"python/05_Syntax_and_Variables.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Understanding basic syntax and variables is the foundation for mastering Python. Once comfortable with these, you can move on to data structures, control flow, and advanced programming concepts.</p> <p>Would you like exercises or quizzes to reinforce learning? \ud83d\ude80</p>"},{"location":"python/06_Control_Flow.html","title":"06 \ud83d\udd04 Control Flow","text":"<p>Control flow determines the execution order of statements in a Python program. It includes conditional statements, loops, and exception handling, allowing programs to make decisions and repeat actions efficiently.</p>"},{"location":"python/06_Control_Flow.html#41-conditional-statements-if-elif-else","title":"\ud83d\udd39 4.1 Conditional Statements (if, elif, else)","text":"<p>Conditional statements allow Python to execute different blocks of code based on conditions.</p>"},{"location":"python/06_Control_Flow.html#basic-if-else-statement","title":"\u2705 Basic if-else Statement","text":"<pre><code>x = 10\n\nif x &gt; 0:\n    print(\"Positive number\")\nelif x &lt; 0:\n    print(\"Negative number\")\nelse:\n    print(\"Zero\")\n</code></pre> <p>Output: <code>Positive number</code></p>"},{"location":"python/06_Control_Flow.html#nested-if-statements","title":"\ud83d\udd39 Nested if Statements","text":"<pre><code>age = 20\n\nif age &gt; 18:\n    if age &gt;= 21:\n        print(\"Eligible for full privileges\")\n    else:\n        print(\"Limited privileges\")\nelse:\n    print(\"Not eligible\")\n</code></pre> <p>\u2705 Use Case: Decision trees in ML models, data validation, user authentication.</p>"},{"location":"python/06_Control_Flow.html#42-looping-in-python","title":"\ud83d\udd39 4.2 Looping in Python","text":"<p>Loops allow repeating actions based on conditions.</p>"},{"location":"python/06_Control_Flow.html#for-loop-iterating-over-sequences","title":"\ud83d\udd39 for Loop (Iterating over Sequences)","text":"<pre><code>numbers = [1, 2, 3, 4, 5]\n\nfor num in numbers:\n    print(num)\n</code></pre> <p>Output:</p> <pre><code>1\n2\n3\n4\n5\n</code></pre> <p>\u2705 Use Case: Iterating over lists, tuples, dictionaries, and strings in data processing, ML datasets.</p>"},{"location":"python/06_Control_Flow.html#for-loop-with-range","title":"\ud83d\udd39 for Loop with range()","text":"<pre><code>for i in range(1, 6):\n    print(i)\n</code></pre> <p>Output:  </p> <pre><code>1\n2\n3\n4\n5\n</code></pre> <p>\u2705 Use Case: Creating training epochs in machine learning models.</p>"},{"location":"python/06_Control_Flow.html#while-loop-repeat-until-condition-fails","title":"\ud83d\udd39 while Loop (Repeat Until Condition Fails)","text":"<pre><code>count = 0\n\nwhile count &lt; 5:\n    print(\"Count:\", count)\n    count += 1\n</code></pre> <p>Output:  </p> <pre><code>Count: 0\nCount: 1\nCount: 2\nCount: 3\nCount: 4\n</code></pre> <p>\u2705 Use Case: Keeping a server running, waiting for a user input, or training an ML model until convergence.</p>"},{"location":"python/06_Control_Flow.html#43-loop-control-statements","title":"\ud83d\udd39 4.3 Loop Control Statements","text":"<p>Python provides ways to modify loop behavior using <code>break</code>, <code>continue</code>, and <code>pass</code>.</p>"},{"location":"python/06_Control_Flow.html#break-exit-loop-early","title":"\ud83d\udd39 break (Exit Loop Early)","text":"<pre><code>for num in range(10):\n    if num == 5:\n        break  # Stops at 5\n    print(num)\n</code></pre> <p>Output:  </p> <pre><code>0\n1\n2\n3\n4\n</code></pre> <p>\u2705 Use Case: Stopping an AI model early if a condition is met.</p>"},{"location":"python/06_Control_Flow.html#continue-skip-iteration","title":"\ud83d\udd39 continue (Skip Iteration)","text":"<pre><code>for num in range(5):\n    if num == 2:\n        continue  # Skips 2\n    print(num)\n</code></pre> <p>Output:  </p> <pre><code>0\n1\n3\n4\n</code></pre> <p>\u2705 Use Case: Skipping invalid data points in datasets.</p>"},{"location":"python/06_Control_Flow.html#pass-do-nothing","title":"\ud83d\udd39 pass (Do Nothing)","text":"<pre><code>for i in range(5):\n    if i == 3:\n        pass  # Placeholder\n    print(i)\n</code></pre> <p>\u2705 Use Case: Placeholder for functions, classes, loops.</p>"},{"location":"python/06_Control_Flow.html#44-list-comprehensions-for-loops","title":"\ud83d\udd39 4.4 List Comprehensions for Loops","text":"<p>Python supports one-liner loops with list comprehensions, improving efficiency.</p> <pre><code>numbers = [x * 2 for x in range(5)]\nprint(numbers)\n</code></pre> <p>Output: <code>[0, 2, 4, 6, 8]</code></p> <p>\u2705 Use Case: Feature engineering, transforming datasets, list filtering.</p>"},{"location":"python/06_Control_Flow.html#45-exception-handling-try-except-finally","title":"\ud83d\udd39 4.5 Exception Handling (try-except-finally)","text":"<p>Handling exceptions prevents crashes in programs.</p> <pre><code>try:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\nfinally:\n    print(\"Execution complete.\")\n</code></pre> <p>Output:  </p> <pre><code>Cannot divide by zero!\nExecution complete.\n</code></pre> <p>\u2705 Use Case: Preventing failures in data pipelines, ML model training.</p>"},{"location":"python/06_Control_Flow.html#summary","title":"\ud83d\ude80 Summary","text":"Concept Key Takeaway if-elif-else Executes different blocks based on conditions for loop Iterates over sequences (lists, tuples, etc.) while loop Runs while condition is <code>True</code> break Exits loop early continue Skips current iteration pass Placeholder statement List Comprehensions Shorter syntax for loops Exception Handling Prevents program crashes"},{"location":"python/06_Control_Flow.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Control flow is crucial for decision-making and iteration in Python. Mastering it allows writing efficient, error-free code.</p> <p>Would you like exercises or real-world examples to reinforce these topics? \ud83d\ude80</p>"},{"location":"python/07_Data_Structures.html","title":"07 \ud83c\udf92 Data Structures","text":"<p>Data structures are essential for organizing, storing, and managing data efficiently in Python. Python provides built-in data structures such as Lists, Tuples, Dictionaries, and Sets, each suited for different tasks.  </p> <p>This chapter covers their properties, operations, and use cases in Data Science, Machine Learning, and AI applications.</p>"},{"location":"python/07_Data_Structures.html#071-lists-ordered-mutable-indexed","title":"07.1 \ud83d\udccb Lists (Ordered, Mutable, Indexed)","text":"<p>A list is an ordered, mutable (changeable) collection that allows duplicate values. Lists are widely used for storing and manipulating datasets.</p>"},{"location":"python/07_Data_Structures.html#creating-a-list","title":"\u2705 Creating a List","text":"<pre><code>fruits = [\"apple\", \"banana\", \"cherry\"]\nnumbers = [10, 20, 30, 40]\nmixed = [1, \"Python\", 3.14, True]\n</code></pre>"},{"location":"python/07_Data_Structures.html#list-operations","title":"\ud83d\udd39 List Operations","text":"<pre><code>fruits.append(\"mango\")  # Add element\nfruits.remove(\"banana\") # Remove element\nfruits.insert(1, \"grape\")  # Insert at index\nfruits.pop()  # Remove last item\nprint(fruits)\n</code></pre> <p>\u2705 Use Case: Managing data records, feature lists, training data batches in ML.</p>"},{"location":"python/07_Data_Structures.html#list-slicing","title":"\ud83d\udd39 List Slicing","text":"<pre><code>numbers = [1, 2, 3, 4, 5, 6]\nprint(numbers[1:4])  # Output: [2, 3, 4]\n</code></pre> <p>\u2705 Use Case: Extracting subsets of data in AI and ML models.</p>"},{"location":"python/07_Data_Structures.html#072-tuples-ordered-immutable-indexed","title":"07.2 \ud83d\udccc Tuples (Ordered, Immutable, Indexed)","text":"<p>A tuple is like a list but immutable (cannot be modified). Tuples are used where data should not change.</p>"},{"location":"python/07_Data_Structures.html#creating-a-tuple","title":"\u2705 Creating a Tuple","text":"<pre><code>coordinates = (10.5, 20.3)\ncolors = (\"red\", \"green\", \"blue\")\n</code></pre>"},{"location":"python/07_Data_Structures.html#tuple-operations","title":"\ud83d\udd39 Tuple Operations","text":"<pre><code>print(coordinates[0])  # Access elements\nprint(len(colors))     # Tuple length\n</code></pre> <p>\u2705 Use Case: Storing constant data like color codes, geographic coordinates.</p>"},{"location":"python/07_Data_Structures.html#tuple-packing-unpacking","title":"\ud83d\udd39 Tuple Packing &amp; Unpacking","text":"<pre><code>point = (3, 4)\nx, y = point  # Unpacking\nprint(x, y)   # Output: 3 4\n</code></pre> <p>\u2705 Use Case: Assigning multiple values in one step in AI and data transformations.</p>"},{"location":"python/07_Data_Structures.html#073-dictionaries-key-value-pairs-unordered-mutable","title":"07.3 \ud83d\uddc2\ufe0f Dictionaries (Key-Value Pairs, Unordered, Mutable)","text":"<p>A dictionary (<code>dict</code>) stores data in key-value pairs, making it ideal for fast lookups.</p>"},{"location":"python/07_Data_Structures.html#creating-a-dictionary","title":"\u2705 Creating a Dictionary","text":"<pre><code>student = {\"name\": \"Alice\", \"age\": 21, \"grade\": \"A\"}\n</code></pre>"},{"location":"python/07_Data_Structures.html#dictionary-operations","title":"\ud83d\udd39 Dictionary Operations","text":"<pre><code>print(student[\"name\"])   # Access value\nstudent[\"age\"] = 22      # Modify value\nstudent[\"city\"] = \"NY\"   # Add new key-value pair\ndel student[\"grade\"]     # Remove key-value pair\n</code></pre> <p>\u2705 Use Case: Storing JSON-like data, AI model parameters, ML hyperparameters.</p>"},{"location":"python/07_Data_Structures.html#iterating-over-a-dictionary","title":"\ud83d\udd39 Iterating Over a Dictionary","text":"<pre><code>for key, value in student.items():\n    print(f\"{key}: {value}\")\n</code></pre> <p>\u2705 Use Case: Extracting metadata from datasets, handling API responses.</p>"},{"location":"python/07_Data_Structures.html#074-sets-unordered-unique-elements-fast-lookups","title":"07.4 \ud83d\udd25 Sets (Unordered, Unique Elements, Fast Lookups)","text":"<p>A set is an unordered collection of unique elements, useful for removing duplicates and fast lookups.</p>"},{"location":"python/07_Data_Structures.html#creating-a-set","title":"\u2705 Creating a Set","text":"<pre><code>numbers = {1, 2, 3, 4, 4, 2}  # Duplicates removed automatically\n</code></pre>"},{"location":"python/07_Data_Structures.html#set-operations","title":"\ud83d\udd39 Set Operations","text":"<pre><code>numbers.add(5)  # Add element\nnumbers.remove(3)  # Remove element\n</code></pre>"},{"location":"python/07_Data_Structures.html#set-operations-for-ai-ml","title":"\ud83d\udd39 Set Operations for AI &amp; ML","text":"<pre><code>A = {1, 2, 3, 4}\nB = {3, 4, 5, 6}\n\nprint(A.union(B))    # {1, 2, 3, 4, 5, 6}\nprint(A.intersection(B))  # {3, 4}\nprint(A.difference(B))    # {1, 2}\n</code></pre> <p>\u2705 Use Case: Removing duplicate values in datasets, comparing feature sets.</p>"},{"location":"python/07_Data_Structures.html#summary","title":"\ud83d\ude80 Summary","text":"Data Structure Properties Use Case List  \ud83d\udccb Ordered, Mutable Storing datasets, feature lists Tuple \ud83d\udccc Ordered, Immutable Constants, fixed ML configurations Dictionary \ud83d\uddc2\ufe0f Key-Value Pairs, Mutable Fast lookups, JSON data, ML parameters Set \ud83d\udd25 Unordered, Unique Elements Removing duplicates, comparing data"},{"location":"python/07_Data_Structures.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Understanding Lists, Tuples, Dictionaries, and Sets is crucial for data processing, feature engineering, and AI applications.</p> <p>Would you like real-world coding exercises on these topics? \ud83d\ude80</p>"},{"location":"python/08_Functions_and_Modules.html","title":"08 \ud83c\udfad Functions and Modules","text":"<p>Functions and modules are essential building blocks in Python, promoting code reuse, modularity, and maintainability. Understanding them is crucial for data science, machine learning, and AI, where reusable code improves efficiency and readability.  </p> <p>This chapter covers defining functions, argument handling, lambda functions, recursion, and working with modules to write efficient, modular, and scalable Python code.</p>"},{"location":"python/08_Functions_and_Modules.html#081-functions-the-building-blocks-of-python","title":"08.1 \ud83c\udfaf Functions: The Building Blocks of Python","text":"<p>A function is a reusable block of code that performs a specific task.</p>"},{"location":"python/08_Functions_and_Modules.html#defining-a-function","title":"\u2705 Defining a Function","text":"<pre><code>def greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"Alice\"))  # Output: Hello, Alice!\n</code></pre> <p>\u2705 Use Case: Encapsulating repetitive code, making programs more readable.</p>"},{"location":"python/08_Functions_and_Modules.html#082-function-arguments-and-parameters","title":"08.2 \ud83c\udfad Function Arguments and Parameters","text":"<p>Functions in Python support different types of arguments:</p>"},{"location":"python/08_Functions_and_Modules.html#positional-arguments","title":"\ud83d\udd39 Positional Arguments","text":"<pre><code>def add(a, b):\n    return a + b\n\nprint(add(5, 3))  # Output: 8\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#default-arguments","title":"\ud83d\udd39 Default Arguments","text":"<pre><code>def power(base, exponent=2):\n    return base  exponent\n\nprint(power(3))     # Output: 9 (3\u00b2)\nprint(power(3, 3))  # Output: 27 (3\u00b3)\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#keyword-arguments","title":"\ud83d\udd39 Keyword Arguments","text":"<pre><code>print(power(exponent=3, base=2))  # Output: 8\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#variable-length-arguments-args-kwargs","title":"\ud83d\udd39 Variable-Length Arguments (*args, **kwargs)","text":"<pre><code>def sum_all(*numbers):\n    return sum(numbers)\n\nprint(sum_all(1, 2, 3, 4))  # Output: 10\n\ndef display_info(info):\n    print(info)\n\ndisplay_info(name=\"Alice\", age=25)\n</code></pre> <p>\u2705 Use Case: Handling dynamic data inputs in ML models, APIs, and automation scripts.</p>"},{"location":"python/08_Functions_and_Modules.html#083-lambda-anonymous-functions","title":"08.3 \u26a1 Lambda (Anonymous) Functions","text":"<p>Lambda functions are short, one-line functions often used in data processing.</p>"},{"location":"python/08_Functions_and_Modules.html#lambda-syntax","title":"\u2705 Lambda Syntax","text":"<pre><code>square = lambda x: x  2\nprint(square(5))  # Output: 25\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#lambda-with-map-filter-reduce","title":"\ud83d\udd39 Lambda with map(), filter(), reduce()","text":"<pre><code>numbers = [1, 2, 3, 4, 5]\nsquared = list(map(lambda x: x2, numbers))\nprint(squared)  # Output: [1, 4, 9, 16, 25]\n</code></pre> <p>\u2705 Use Case: Data transformations in pandas, NumPy, and machine learning preprocessing.</p>"},{"location":"python/08_Functions_and_Modules.html#084-recursion-functions-calling-themselves","title":"08.4 \ud83d\udd01 Recursion: Functions Calling Themselves","text":"<p>Recursion is used when a problem can be broken down into smaller subproblems.</p>"},{"location":"python/08_Functions_and_Modules.html#factorial-calculation","title":"\u2705 Factorial Calculation","text":"<pre><code>def factorial(n):\n    if n == 1:\n        return 1\n    return n * factorial(n - 1)\n\nprint(factorial(5))  # Output: 120\n</code></pre> <p>\u2705 Use Case: Tree-based algorithms, graph traversal (DFS), Fibonacci sequence.</p>"},{"location":"python/08_Functions_and_Modules.html#085-python-modules-importing-and-organizing-code","title":"08.5 \ud83d\udce6 Python Modules (Importing and Organizing Code)","text":"<p>Modules allow code organization by grouping related functions and variables.</p>"},{"location":"python/08_Functions_and_Modules.html#importing-a-module","title":"\u2705 Importing a Module","text":"<pre><code>import math\nprint(math.sqrt(25))  # Output: 5.0\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#importing-specific-functions","title":"\ud83d\udd39 Importing Specific Functions","text":"<pre><code>from math import sqrt\nprint(sqrt(16))  # Output: 4.0\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#creating-a-custom-module","title":"\ud83d\udd39 Creating a Custom Module","text":"<p>\ud83d\udccc Create a file <code>mymodule.py</code> </p> <pre><code>def greet(name):\n    return f\"Hello, {name}!\"\n</code></pre> <p>\ud83d\udccc Import and Use the Module</p> <pre><code>import mymodule\nprint(mymodule.greet(\"Alice\"))\n</code></pre> <p>\u2705 Use Case: Reusing functions in large AI projects, ML models, and APIs.</p>"},{"location":"python/08_Functions_and_Modules.html#086-working-with-built-in-and-third-party-modules","title":"08.6 \ud83d\udcc2 Working with Built-in and Third-Party Modules","text":""},{"location":"python/08_Functions_and_Modules.html#useful-built-in-modules","title":"\ud83d\udd39 Useful Built-in Modules","text":"Module Purpose <code>math</code> Mathematical functions <code>random</code> Random number generation <code>datetime</code> Date and time operations <code>os</code> File and system operations <code>sys</code> System-related functions <code>re</code> Regular expressions"},{"location":"python/08_Functions_and_Modules.html#installing-using-third-party-modules","title":"\ud83d\udd39 Installing &amp; Using Third-Party Modules","text":"<pre><code>pip install numpy pandas\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\n</code></pre> <p>\u2705 Use Case: Data science, AI model training, automation.</p>"},{"location":"python/09_Object-Oriented_Programming.html","title":"09 \ud83c\udfd7\ufe0f Object-Oriented","text":"<p>Object-Oriented Programming (OOP) is a programming paradigm that organizes code into reusable objects. It is widely used in data science, machine learning, and AI to manage models, datasets, and complex systems efficiently.</p> <p>This chapter covers classes, objects, inheritance, polymorphism, encapsulation, and abstraction to help structure Python programs for scalability and maintainability.</p>"},{"location":"python/09_Object-Oriented_Programming.html#091-what-is-oop","title":"09.1 \ud83c\udfd7\ufe0f What is OOP?","text":"<p>OOP is based on the concept of objects that contain data (attributes) and functions (methods). This approach makes programs modular, reusable, and easy to maintain.</p>"},{"location":"python/09_Object-Oriented_Programming.html#key-oop-concepts","title":"\ud83d\udd39 Key OOP Concepts","text":"Concept Description Class A blueprint for creating objects Object An instance of a class Encapsulation Hiding internal details of an object Inheritance A child class inherits attributes and methods from a parent class Polymorphism Different classes can have methods with the same name but different behaviors Abstraction Hiding unnecessary implementation details"},{"location":"python/09_Object-Oriented_Programming.html#092-creating-classes-and-objects","title":"09.2 \ud83d\udce6 Creating Classes and Objects","text":"<p>A class is a blueprint for creating objects.</p>"},{"location":"python/09_Object-Oriented_Programming.html#defining-a-class-and-creating-an-object","title":"\u2705 Defining a Class and Creating an Object","text":"<pre><code>class Car:\n    def __init__(self, brand, model):\n        self.brand = brand  # Attribute\n        self.model = model  # Attribute\n\n    def display_info(self):  # Method\n        return f\"{self.brand} {self.model}\"\n\n# Creating an object\ncar1 = Car(\"Toyota\", \"Corolla\")\nprint(car1.display_info())  # Output: Toyota Corolla\n</code></pre> <p>\u2705 Use Case: Creating machine learning models, database records, or simulation objects.</p>"},{"location":"python/09_Object-Oriented_Programming.html#093-encapsulation-data-protection","title":"09.3 \ud83d\udd10 Encapsulation (Data Protection)","text":"<p>Encapsulation restricts direct access to object attributes, ensuring data integrity.</p>"},{"location":"python/09_Object-Oriented_Programming.html#private-variables-in-a-class","title":"\u2705 Private Variables in a Class","text":"<pre><code>class BankAccount:\n    def __init__(self, balance):\n        self.__balance = balance  # Private attribute\n\n    def deposit(self, amount):\n        self.__balance += amount\n\n    def get_balance(self):\n        return self.__balance\n\n# Usage\naccount = BankAccount(1000)\naccount.deposit(500)\nprint(account.get_balance())  # Output: 1500\n</code></pre> <p>\u2705 Use Case: Protecting sensitive data like user credentials, financial data.</p>"},{"location":"python/09_Object-Oriented_Programming.html#094-inheritance-code-reusability","title":"09.4 \ud83d\udd04 Inheritance (Code Reusability)","text":"<p>Inheritance allows a child class to use the properties and methods of a parent class, reducing redundant code.</p>"},{"location":"python/09_Object-Oriented_Programming.html#single-inheritance","title":"\u2705 Single Inheritance","text":"<pre><code>class Animal:\n    def make_sound(self):\n        return \"Some sound\"\n\nclass Dog(Animal):\n    def make_sound(self):\n        return \"Bark\"\n\n# Usage\ndog = Dog()\nprint(dog.make_sound())  # Output: Bark\n</code></pre> <p>\u2705 Use Case: Extending functionality of ML models, custom layers in deep learning.</p>"},{"location":"python/09_Object-Oriented_Programming.html#multiple-inheritance","title":"\ud83d\udd39 Multiple Inheritance","text":"<pre><code>class A:\n    def method_a(self):\n        return \"Method A\"\n\nclass B:\n    def method_b(self):\n        return \"Method B\"\n\nclass C(A, B):  # Inheriting from A and B\n    pass\n\nobj = C()\nprint(obj.method_a())  # Output: Method A\nprint(obj.method_b())  # Output: Method B\n</code></pre> <p>\u2705 Use Case: Combining functionalities from different modules (e.g., ML models + preprocessing steps).</p>"},{"location":"python/09_Object-Oriented_Programming.html#095-polymorphism-multiple-forms","title":"09.5 \ud83d\udd04 Polymorphism (Multiple Forms)","text":"<p>Polymorphism allows different classes to use the same method name but behave differently.</p>"},{"location":"python/09_Object-Oriented_Programming.html#method-overriding","title":"\u2705 Method Overriding","text":"<pre><code>class Shape:\n    def area(self):\n        return \"Area method not implemented\"\n\nclass Circle(Shape):\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return 3.14 * self.radius  2  # Overriding method\n\n# Usage\ncircle = Circle(5)\nprint(circle.area())  # Output: 78.5\n</code></pre> <p>\u2705 Use Case: Implementing AI models with different training methods.</p>"},{"location":"python/09_Object-Oriented_Programming.html#096-abstraction-hiding-implementation-details","title":"09.6 \ud83c\udfad Abstraction (Hiding Implementation Details)","text":"<p>Abstraction hides complex logic and exposes only relevant details.</p>"},{"location":"python/09_Object-Oriented_Programming.html#using-the-abc-module-for-abstraction","title":"\u2705 Using the <code>ABC</code> Module for Abstraction","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass Payment(ABC):  # Abstract Class\n    @abstractmethod\n    def process_payment(self, amount):\n        pass\n\nclass CreditCardPayment(Payment):\n    def process_payment(self, amount):\n        return f\"Processing credit card payment of ${amount}\"\n\n# Usage\npayment = CreditCardPayment()\nprint(payment.process_payment(100))  # Output: Processing credit card payment of $100\n</code></pre> <p>\u2705 Use Case: Defining AI model architecture, creating frameworks for ML algorithms.</p>"},{"location":"python/09_Object-Oriented_Programming.html#097-oop-in-real-world-ai-and-ml","title":"09.7 \ud83d\udee0\ufe0f OOP in Real-World AI and ML","text":""},{"location":"python/09_Object-Oriented_Programming.html#oop-for-machine-learning-models","title":"\u2705 OOP for Machine Learning Models","text":"<pre><code>class MLModel:\n    def train(self, data):\n        return \"Training the model on data\"\n\nclass NeuralNetwork(MLModel):\n    def train(self, data):\n        return \"Training deep learning model\"\n\nmodel1 = MLModel()\nmodel2 = NeuralNetwork()\n\nprint(model1.train(\"Dataset\"))  # Output: Training the model on data\nprint(model2.train(\"Dataset\"))  # Output: Training deep learning model\n</code></pre> <p>\u2705 Use Case: Modularizing ML models and creating reusable AI components.</p>"},{"location":"python/09_Object-Oriented_Programming.html#summary","title":"\ud83d\ude80 Summary","text":"OOP Concept Description Use Case Class &amp; Object Blueprint and instance of an object AI models, Data structures Encapsulation Restricting direct access to attributes Secure financial transactions Inheritance Child class inherits from parent class Model pipelines, feature engineering Polymorphism Same method, different behavior Different AI models processing inputs Abstraction Hiding unnecessary details AI frameworks, APIs"},{"location":"python/09_Object-Oriented_Programming.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Object-Oriented Programming enhances modularity and reusability, making it ideal for building scalable ML models, AI systems, and large applications.</p> <p>Would you like real-world coding challenges for hands-on practice? \ud83d\ude80</p>"},{"location":"python/10_Python_Standard_Library.html","title":"10 \ud83e\uddf0 Python Standard Library","text":"<p>The Python Standard Library is a collection of built-in modules and functions that provide powerful functionality without needing external dependencies. It includes modules for mathematical operations, file handling, date/time manipulation, data structures, web services, OS interaction, and more.</p> <p>This chapter explores some of the most useful standard library modules, their features, and how they can simplify Python development.</p>"},{"location":"python/10_Python_Standard_Library.html#101-math-mathematical-operations","title":"10.1 \ud83d\udd22 <code>math</code> \u2013 Mathematical Operations","text":"<p>The <code>math</code> module provides mathematical functions such as trigonometry, logarithms, and factorials.</p>"},{"location":"python/10_Python_Standard_Library.html#common-functions-in-math","title":"\u2705 Common Functions in <code>math</code>","text":"<pre><code>import math\n\nprint(math.sqrt(25))    # Output: 5.0\nprint(math.factorial(5))  # Output: 120\nprint(math.log(100, 10))  # Output: 2.0\nprint(math.pi)  # Output: 3.141592653589793\nprint(math.e)   # Output: 2.718281828459045\n</code></pre> <p>\u2705 Use Case: Calculations in machine learning, data science, and AI models.</p>"},{"location":"python/10_Python_Standard_Library.html#102-random-generating-random-numbers","title":"10.2 \ud83c\udfb2 <code>random</code> \u2013 Generating Random Numbers","text":"<p>The <code>random</code> module helps in random sampling, shuffling, and generating random numbers.</p>"},{"location":"python/10_Python_Standard_Library.html#generating-random-numbers","title":"\u2705 Generating Random Numbers","text":"<pre><code>import random\n\nprint(random.randint(1, 10))  # Random integer between 1 and 10\nprint(random.choice([\"apple\", \"banana\", \"cherry\"]))  # Random choice\nprint(random.sample(range(100), 5))  # 5 random numbers from 0-99\n</code></pre> <p>\u2705 Use Case: Random sampling, AI model parameter initialization, data augmentation.</p>"},{"location":"python/10_Python_Standard_Library.html#103-datetime-working-with-dates-and-time","title":"10.3 \ud83d\udcc6 <code>datetime</code> \u2013 Working with Dates and Time","text":"<p>The <code>datetime</code> module provides tools for date and time manipulation.</p>"},{"location":"python/10_Python_Standard_Library.html#getting-current-date-and-time","title":"\u2705 Getting Current Date and Time","text":"<pre><code>from datetime import datetime\n\nnow = datetime.now()\nprint(now.strftime(\"%Y-%m-%d %H:%M:%S\"))  # Output: 2025-03-06 14:30:15\n</code></pre>"},{"location":"python/10_Python_Standard_Library.html#calculating-time-differences","title":"\ud83d\udd39 Calculating Time Differences","text":"<pre><code>from datetime import timedelta\n\nfuture_date = now + timedelta(days=7)\nprint(future_date.strftime(\"%Y-%m-%d\"))  # Output: (7 days ahead)\n</code></pre> <p>\u2705 Use Case: Timestamps in logs, scheduling tasks, time-series analysis.</p>"},{"location":"python/10_Python_Standard_Library.html#104-os-interacting-with-the-operating-system","title":"10.4 \ud83d\udcc2 <code>os</code> \u2013 Interacting with the Operating System","text":"<p>The <code>os</code> module helps manage files, directories, and system operations.</p>"},{"location":"python/10_Python_Standard_Library.html#common-os-operations","title":"\u2705 Common OS Operations","text":"<pre><code>import os\n\nprint(os.getcwd())  # Get current working directory\nos.mkdir(\"new_folder\")  # Create a new folder\nos.rename(\"old_file.txt\", \"new_file.txt\")  # Rename a file\nos.remove(\"new_file.txt\")  # Delete a file\n</code></pre> <p>\u2705 Use Case: File automation, script execution, managing system resources.</p>"},{"location":"python/10_Python_Standard_Library.html#105-sys-system-specific-functions","title":"10.5 \ud83d\udcdc <code>sys</code> \u2013 System-Specific Functions","text":"<p>The <code>sys</code> module provides functions related to the Python interpreter and command-line arguments.</p>"},{"location":"python/10_Python_Standard_Library.html#getting-command-line-arguments","title":"\u2705 Getting Command-Line Arguments","text":"<pre><code>import sys\nprint(sys.argv)  # List of command-line arguments\n</code></pre>"},{"location":"python/10_Python_Standard_Library.html#exiting-the-program","title":"\ud83d\udd39 Exiting the Program","text":"<pre><code>sys.exit(\"Terminating program\")\n</code></pre> <p>\u2705 Use Case: Handling script execution arguments, system interaction.</p>"},{"location":"python/10_Python_Standard_Library.html#106-re-regular-expressions-pattern-matching","title":"10.6 \ud83d\udd0d <code>re</code> \u2013 Regular Expressions (Pattern Matching)","text":"<p>The <code>re</code> module provides powerful text searching and pattern matching.</p>"},{"location":"python/10_Python_Standard_Library.html#searching-for-patterns","title":"\u2705 Searching for Patterns","text":"<pre><code>import re\n\ntext = \"My email is example@gmail.com\"\nmatch = re.search(r\"\\w+@\\w+\\.\\w+\", text)\nif match:\n    print(match.group())  # Output: example@gmail.com\n</code></pre> <p>\u2705 Use Case: Data cleaning, log processing, text extraction in NLP.</p>"},{"location":"python/10_Python_Standard_Library.html#107-urllib-fetching-web-data","title":"10.7 \ud83d\udce1 <code>urllib</code> \u2013 Fetching Web Data","text":"<p>The <code>urllib</code> module allows sending HTTP requests and fetching web content.</p>"},{"location":"python/10_Python_Standard_Library.html#downloading-a-webpage","title":"\u2705 Downloading a Webpage","text":"<pre><code>import urllib.request\n\nresponse = urllib.request.urlopen(\"https://www.python.org\")\nhtml = response.read().decode(\"utf-8\")\nprint(html[:200])  # Prints first 200 characters of HTML\n</code></pre> <p>\u2705 Use Case: Web scraping, downloading datasets, API calls.</p>"},{"location":"python/10_Python_Standard_Library.html#108-json-handling-json-data","title":"10.8 \ud83d\uddc3\ufe0f <code>json</code> \u2013 Handling JSON Data","text":"<p>The <code>json</code> module allows converting Python objects to JSON format and vice versa.</p>"},{"location":"python/10_Python_Standard_Library.html#converting-python-dictionary-to-json","title":"\u2705 Converting Python Dictionary to JSON","text":"<pre><code>import json\n\ndata = {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"}\njson_data = json.dumps(data)\nprint(json_data)  # Output: JSON formatted string\n</code></pre>"},{"location":"python/10_Python_Standard_Library.html#parsing-json-data","title":"\ud83d\udd39 Parsing JSON Data","text":"<pre><code>parsed_data = json.loads(json_data)\nprint(parsed_data[\"name\"])  # Output: Alice\n</code></pre> <p>\u2705 Use Case: Handling API responses, saving structured data.</p>"},{"location":"python/10_Python_Standard_Library.html#109-csv-reading-and-writing-csv-files","title":"10.9 \ud83d\udd04 <code>csv</code> \u2013 Reading and Writing CSV Files","text":"<p>The <code>csv</code> module allows handling comma-separated values (CSV) files, commonly used in data science.</p>"},{"location":"python/10_Python_Standard_Library.html#reading-a-csv-file","title":"\u2705 Reading a CSV File","text":"<pre><code>import csv\n\nwith open(\"data.csv\", newline=\"\") as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)\n</code></pre> <p>\u2705 Use Case: Reading structured datasets for ML training.</p>"},{"location":"python/10_Python_Standard_Library.html#1010-collections-advanced-data-structures","title":"10.10 \ud83d\uddc2\ufe0f <code>collections</code> \u2013 Advanced Data Structures","text":"<p>The <code>collections</code> module provides specialized data structures like deque, Counter, defaultdict.</p>"},{"location":"python/10_Python_Standard_Library.html#counting-elements-with-counter","title":"\u2705 Counting Elements with Counter","text":"<pre><code>from collections import Counter\n\ndata = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\"]\ncounter = Counter(data)\nprint(counter)  # Output: {'apple': 3, 'banana': 2, 'orange': 1}\n</code></pre> <p>\u2705 Use Case: Counting words in NLP, tracking data occurrences.</p>"},{"location":"python/10_Python_Standard_Library.html#summary-of-useful-python-standard-library-modules","title":"\ud83d\ude80 Summary of Useful Python Standard Library Modules","text":"Module Purpose math Mathematical operations (sqrt, factorial, log, pi) random Random number generation, shuffling datetime Date and time handling os OS file management (create, rename, delete) sys System operations, command-line arguments re Regular expressions for pattern matching urllib Web requests and data fetching json JSON data encoding/decoding csv Reading and writing CSV files collections Advanced data structures (Counter, deque)"},{"location":"python/10_Python_Standard_Library.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>The Python Standard Library provides powerful tools for data processing, automation, system operations, and web interactions. Mastering these modules helps in building efficient Python programs.</p> <p>Would you like hands-on exercises on these modules? \ud83d\ude80</p>"},{"location":"python/11_File_Handling_and_IO.html","title":"11 \ud83c\udf10 File Handling","text":"<p>File handling is an essential part of programming, allowing reading, writing, and manipulating files. In data science, machine learning, and AI, working with text files, CSV files, and logs is common for storing and processing large datasets.  </p> <p>This chapter covers file handling operations, including reading, writing, appending, working with different file formats, and error handling.</p>"},{"location":"python/11_File_Handling_and_IO.html#111-working-with-files-in-python","title":"11.1 \ud83d\udcc2 Working with Files in Python","text":"<p>Python provides built-in functions for opening, reading, writing, and closing files.</p>"},{"location":"python/11_File_Handling_and_IO.html#opening-a-file","title":"\u2705 Opening a File","text":"<pre><code>file = open(\"example.txt\", \"r\")  # Open a file in read mode\nprint(file.read())  # Read the entire file content\nfile.close()  # Always close the file after use\n</code></pre> <p>\u2705 Use Case: Reading log files, dataset files, configuration files.</p>"},{"location":"python/11_File_Handling_and_IO.html#112-writing-to-a-file","title":"11.2 \ud83d\udcdd Writing to a File","text":"<p>To write to a file, use write ('w') mode. If the file doesn\u2019t exist, Python creates a new file.</p>"},{"location":"python/11_File_Handling_and_IO.html#writing-data","title":"\u2705 Writing Data","text":"<pre><code>file = open(\"example.txt\", \"w\")  # Open in write mode\nfile.write(\"Hello, Python!\\nWelcome to File Handling.\")\nfile.close()\n</code></pre> <p>\ud83d\udd39 Note: <code>'w'</code> mode overwrites existing content. To append, use <code>'a'</code>.</p>"},{"location":"python/11_File_Handling_and_IO.html#appending-data-to-an-existing-file","title":"\u2705 Appending Data to an Existing File","text":"<pre><code>file = open(\"example.txt\", \"a\")\nfile.write(\"\\nThis is an additional line.\")\nfile.close()\n</code></pre> <p>\u2705 Use Case: Logging data, saving AI/ML model outputs.</p>"},{"location":"python/11_File_Handling_and_IO.html#113-reading-files-efficiently","title":"11.3 \ud83d\udcd6 Reading Files Efficiently","text":"<p>Python provides different ways to read files efficiently.</p>"},{"location":"python/11_File_Handling_and_IO.html#reading-a-file-line-by-line","title":"\u2705 Reading a File Line by Line","text":"<pre><code>file = open(\"example.txt\", \"r\")\n\nfor line in file:\n    print(line.strip())  # Removing extra newlines\nfile.close()\n</code></pre> <p>\u2705 Use Case: Processing large datasets without memory overflow.</p>"},{"location":"python/11_File_Handling_and_IO.html#using-readlines-to-read-all-lines-as-a-list","title":"\ud83d\udd39 Using <code>readlines()</code> to Read All Lines as a List","text":"<pre><code>file = open(\"example.txt\", \"r\")\nlines = file.readlines()\nprint(lines)  # Output: List of lines in the file\nfile.close()\n</code></pre> <p>\u2705 Use Case: Reading structured text data (e.g., logs, reports).</p>"},{"location":"python/11_File_Handling_and_IO.html#114-using-with-statement-for-file-handling","title":"11.4 \ud83d\ude80 Using <code>with</code> Statement for File Handling","text":"<p>Using <code>with open()</code> ensures that the file automatically closes after execution.</p>"},{"location":"python/11_File_Handling_and_IO.html#safe-file-handling-with-with","title":"\u2705 Safe File Handling with <code>with</code>","text":"<pre><code>with open(\"example.txt\", \"r\") as file:\n    content = file.read()\n    print(content)\n</code></pre> <p>\u2705 Use Case: Better memory management, avoiding file lock issues.</p>"},{"location":"python/11_File_Handling_and_IO.html#115-handling-different-file-formats","title":"11.5 \ud83d\udcca Handling Different File Formats","text":""},{"location":"python/11_File_Handling_and_IO.html#working-with-csv-files-csv-module","title":"\ud83d\udd39 Working with CSV Files (<code>csv</code> Module)","text":"<pre><code>import csv\n\nwith open(\"data.csv\", \"r\") as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)  # Output: List of values in each row\n</code></pre> <p>\u2705 Use Case: Reading structured datasets in ML models.</p>"},{"location":"python/11_File_Handling_and_IO.html#writing-to-a-csv-file","title":"\ud83d\udd39 Writing to a CSV File","text":"<pre><code>with open(\"output.csv\", \"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Name\", \"Age\"])\n    writer.writerow([\"Alice\", 25])\n</code></pre> <p>\u2705 Use Case: Saving processed data from AI pipelines.</p>"},{"location":"python/11_File_Handling_and_IO.html#working-with-json-files-json-module","title":"\ud83d\udd39 Working with JSON Files (<code>json</code> Module)","text":"<pre><code>import json\n\ndata = {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"}\n\n# Writing JSON data\nwith open(\"data.json\", \"w\") as file:\n    json.dump(data, file)\n\n# Reading JSON data\nwith open(\"data.json\", \"r\") as file:\n    loaded_data = json.load(file)\nprint(loaded_data)  # Output: {'name': 'Alice', 'age': 25, 'city': 'New York'}\n</code></pre> <p>\u2705 Use Case: Handling API responses, configuration files, AI model metadata.</p>"},{"location":"python/11_File_Handling_and_IO.html#116-error-handling-in-file-operations","title":"11.6 \ud83d\udee0\ufe0f Error Handling in File Operations","text":"<p>It\u2019s important to handle file-related errors to prevent crashes.</p>"},{"location":"python/11_File_Handling_and_IO.html#handling-file-not-found-error","title":"\u2705 Handling File Not Found Error","text":"<pre><code>try:\n    with open(\"non_existent.txt\", \"r\") as file:\n        content = file.read()\nexcept FileNotFoundError:\n    print(\"Error: File not found!\")\n</code></pre> <p>\u2705 Use Case: Ensuring robust scripts in production AI pipelines.</p>"},{"location":"python/11_File_Handling_and_IO.html#117-working-with-directories-os-module","title":"11.7 \ud83d\udcc2 Working with Directories (<code>os</code> Module)","text":"<p>The <code>os</code> module allows working with directories and file management.</p>"},{"location":"python/11_File_Handling_and_IO.html#listing-files-in-a-directory","title":"\u2705 Listing Files in a Directory","text":"<pre><code>import os\n\nprint(os.listdir(\".\"))  # Lists all files in the current directory\n</code></pre>"},{"location":"python/11_File_Handling_and_IO.html#creating-and-deleting-folders","title":"\ud83d\udd39 Creating and Deleting Folders","text":"<pre><code>os.mkdir(\"new_folder\")  # Create a folder\nos.rmdir(\"new_folder\")  # Remove a folder\n</code></pre> <p>\u2705 Use Case: Managing dataset directories in ML projects.</p>"},{"location":"python/11_File_Handling_and_IO.html#summary","title":"Summary","text":"Concept Description Use Case Reading Files <code>open(\"file.txt\", \"r\")</code> Loading datasets, logs Writing Files <code>open(\"file.txt\", \"w\")</code> Saving AI model results Appending to Files <code>open(\"file.txt\", \"a\")</code> Logging incremental data Using <code>with</code> Statement Ensures safe file handling Avoids resource leaks CSV Handling <code>csv.reader()</code>, <code>csv.writer()</code> Working with structured data JSON Handling <code>json.load()</code>, <code>json.dump()</code> API responses, metadata storage Error Handling <code>try-except</code> for missing files Prevents crashes in AI pipelines Directory Management <code>os.listdir()</code>, <code>os.mkdir()</code> Managing datasets and logs"},{"location":"python/11_File_Handling_and_IO.html#final-thoughts","title":"Final Thoughts","text":"<p>Mastering file handling and I/O is essential for data processing, logging, and storage in AI, ML, and automation projects.</p> <p>Would you like real-world coding exercises for practice? \ud83d\ude80</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html","title":"12 \u26a1 Errors and Debugging","text":"<p>Errors are inevitable in programming. Python provides exception handling and debugging tools to ensure programs fail gracefully and help identify issues efficiently. Exception handling improves code reliability, while debugging helps track and fix errors.</p> <p>This chapter covers handling exceptions with <code>try-except-finally</code>, built-in exceptions, raising custom exceptions, logging errors, and debugging techniques.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#121-understanding-errors-in-python","title":"12.1 \u274c Understanding Errors in Python","text":"<p>Python has two main types of errors:  </p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#syntax-errors-parsing-errors","title":"\ud83d\udd39 Syntax Errors (Parsing Errors)","text":"<p>Occurs when Python encounters an incorrectly written statement.</p> <pre><code>print(\"Hello\"  # Missing closing parenthesis -&gt; SyntaxError\n</code></pre> <p>\u2705 Fix: Ensure correct syntax.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#runtime-errors-exceptions","title":"\ud83d\udd39 Runtime Errors (Exceptions)","text":"<p>Occur during execution, such as dividing by zero or accessing an undefined variable.</p> <pre><code>x = 5 / 0  # ZeroDivisionError\n</code></pre> <p>Python provides built-in exception types, such as:  </p> Exception Type Description <code>ZeroDivisionError</code> Division by zero error <code>TypeError</code> Invalid operation on incompatible data types <code>ValueError</code> Function receives incorrect data type <code>IndexError</code> Accessing an invalid index in a list <code>KeyError</code> Accessing a missing key in a dictionary <code>FileNotFoundError</code> Attempting to open a non-existent file"},{"location":"python/12_Exception_Handling_and_Debugging.html#122-handling-exceptions-with-try-except","title":"12.2 \ud83d\udee0\ufe0f Handling Exceptions with <code>try-except</code>","text":"<p>The <code>try-except</code> block catches runtime errors and prevents program crashes.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#basic-exception-handling","title":"\u2705 Basic Exception Handling","text":"<pre><code>try:\n    x = 5 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\n</code></pre> <p>Output:  </p> <pre><code>Cannot divide by zero!\n</code></pre> <p>\u2705 Use Case: Handling invalid user inputs, avoiding crashes in AI applications.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#123-handling-multiple-exceptions","title":"12.3 \ud83d\udd04 Handling Multiple Exceptions","text":"<p>Multiple exceptions can be handled in a single <code>try-except</code> block.</p> <pre><code>try:\n    num = int(\"Python\")  # Causes ValueError\nexcept (ValueError, TypeError):\n    print(\"Invalid input!\")\n</code></pre> <p>\u2705 Use Case: Handling diverse input errors in data processing pipelines.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#124-using-finally-for-cleanup","title":"12.4 \ud83d\udccc Using <code>finally</code> for Cleanup","text":"<p>The <code>finally</code> block executes whether an exception occurs or not.</p> <pre><code>try:\n    file = open(\"example.txt\", \"r\")\n    print(file.read())\nexcept FileNotFoundError:\n    print(\"File not found!\")\nfinally:\n    print(\"Closing the file.\")  # Always runs\n</code></pre> <p>\u2705 Use Case: Ensuring file/database connections are properly closed.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#125-raising-custom-exceptions-raise","title":"12.5 \ud83d\ude80 Raising Custom Exceptions (<code>raise</code>)","text":"<p>Use <code>raise</code> to define custom exceptions for better debugging.</p> <pre><code>def check_age(age):\n    if age &lt; 18:\n        raise ValueError(\"Age must be 18 or above\")\n    return \"Access granted\"\n\nprint(check_age(15))  # Raises ValueError\n</code></pre> <p>\u2705 Use Case: Enforcing validation rules in APIs and ML models.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#126-logging-errors-logging-module","title":"12.6 \ud83d\udcdd Logging Errors (<code>logging</code> Module)","text":"<p>Instead of printing errors, use logging to track them efficiently.</p> <pre><code>import logging\n\nlogging.basicConfig(filename=\"app.log\", level=logging.ERROR)\n\ntry:\n    result = 5 / 0\nexcept ZeroDivisionError as e:\n    logging.error(f\"Error occurred: {e}\")\n</code></pre> <p>\u2705 Use Case: Tracking issues in AI applications, logging errors in production.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#127-debugging-techniques","title":"12.7 \ud83d\udc1b Debugging Techniques","text":"<p>Debugging helps find and fix issues before deployment.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#using-print-for-debugging","title":"\ud83d\udd39 Using <code>print()</code> for Debugging","text":"<pre><code>def add(a, b):\n    print(f\"Adding {a} and {b}\")  # Debugging output\n    return a + b\n\nprint(add(5, 3))\n</code></pre> <p>\u2705 Use Case: Checking function execution and variable values.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#using-assert-for-testing","title":"\ud83d\udd39 Using <code>assert</code> for Testing","text":"<p><code>assert</code> helps validate assumptions in the code.</p> <pre><code>def divide(a, b):\n    assert b != 0, \"Denominator cannot be zero\"\n    return a / b\n\nprint(divide(10, 2))  # Runs fine\nprint(divide(10, 0))  # Raises AssertionError\n</code></pre> <p>\u2705 Use Case: Preventing invalid inputs in ML models.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#debugging-with-pdb-python-debugger","title":"\ud83d\udd39 Debugging with <code>pdb</code> (Python Debugger)","text":"<p><code>pdb</code> allows step-by-step execution.</p> <pre><code>import pdb\n\ndef multiply(x, y):\n    pdb.set_trace()  # Debugging breakpoint\n    return x * y\n\nprint(multiply(3, 4))\n</code></pre> <p>\u2705 Use Case: Investigating code behavior interactively.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#128-best-practices-for-exception-handling-debugging","title":"12.8 \ud83d\udee0\ufe0f Best Practices for Exception Handling &amp; Debugging","text":"<p>\u2705 DO:</p> <ul> <li>Use specific exceptions (<code>ZeroDivisionError</code>, <code>FileNotFoundError</code>)</li> <li>Use logging instead of <code>print()</code></li> <li>Use <code>finally</code> for resource cleanup</li> <li>Use assertions for quick debugging</li> </ul> <p>\u274c AVOID:</p> <ul> <li>Using <code>except:</code> without specifying the exception</li> <li>Silencing exceptions (<code>pass</code> inside <code>except</code>)</li> <li>Overusing <code>try-except</code> in simple operations</li> </ul>"},{"location":"python/12_Exception_Handling_and_Debugging.html#summary","title":"\ud83d\ude80 Summary","text":"Concept Description Use Case try-except Catches runtime errors Handling invalid inputs Multiple Exceptions Catches different errors Complex workflows finally Executes cleanup code Closing files/databases raise Throws custom exceptions Enforcing validation logging Saves errors to a log file Debugging production apps assert Ensures correct values Quick testing pdb Interactive debugging Step-by-step execution"},{"location":"python/12_Exception_Handling_and_Debugging.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Mastering exception handling and debugging is essential for writing robust, error-free Python applications.</p> <p>Would you like hands-on debugging exercises? \ud83d\ude80</p>"},{"location":"python/13_Web_Development.html","title":"13 \ud83d\udce1 Web Development","text":"<p>Python is a powerful language for web development, offering a variety of frameworks for building web applications, APIs, and documentation sites. This chapter covers Flask, FastAPI, MkDocs, and Streamlit, widely used for web applications, REST APIs, documentation, and data visualization.</p>"},{"location":"python/13_Web_Development.html#131-overview-of-python-web-development","title":"13.1 \ud83c\udf0d Overview of Python Web Development","text":"<p>Python provides lightweight and scalable web frameworks for different needs:</p> Framework Purpose Flask Lightweight, flexible web framework FastAPI High-performance API framework for modern web applications MkDocs Static site generator for documentation Streamlit Framework for building data-driven web apps"},{"location":"python/13_Web_Development.html#132-flask-lightweight-web-framework","title":"13.2 \ud83c\udfd7\ufe0f Flask \u2013 Lightweight Web Framework","text":"<p>Flask is a minimalistic and easy-to-use web framework, widely used for small to medium-sized web apps.</p>"},{"location":"python/13_Web_Development.html#installing-flask","title":"\u2705 Installing Flask","text":"<pre><code>pip install flask\n</code></pre>"},{"location":"python/13_Web_Development.html#creating-a-simple-flask-app","title":"\ud83d\udd39 Creating a Simple Flask App","text":"<pre><code>from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef home():\n    return \"Hello, Flask!\"\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre> <p>\ud83d\udd39 Run the app:</p> <pre><code>python app.py\n</code></pre> <p>\ud83d\udd39 Visit <code>http://137.0.0.1:5000/</code> in your browser.</p>"},{"location":"python/13_Web_Development.html#handling-routes-url-parameters","title":"\ud83d\udd39 Handling Routes &amp; URL Parameters","text":"<pre><code>@app.route(\"/user/&lt;name&gt;\")\ndef greet_user(name):\n    return f\"Hello, {name}!\"\n</code></pre> <p>\u2705 Use Case: Building web dashboards, small applications, and APIs.</p>"},{"location":"python/13_Web_Development.html#133-fastapi-high-performance-apis","title":"13.3 \ud83d\ude80 FastAPI \u2013 High-Performance APIs","text":"<p>FastAPI is a modern, fast framework for building APIs with automatic documentation and async support.</p>"},{"location":"python/13_Web_Development.html#installing-fastapi","title":"\u2705 Installing FastAPI","text":"<pre><code>pip install fastapi uvicorn\n</code></pre>"},{"location":"python/13_Web_Development.html#creating-a-simple-fastapi-app","title":"\ud83d\udd39 Creating a Simple FastAPI App","text":"<pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef home():\n    return {\"message\": \"Hello, FastAPI!\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"137.0.0.1\", port=8000)\n</code></pre> <p>\ud83d\udd39 Run the app:</p> <pre><code>uvicorn app:app --reload\n</code></pre> <p>\ud83d\udd39 Visit the automatic API docs:  </p> <ul> <li>Swagger UI: <code>http://137.0.0.1:8000/docs</code></li> <li>ReDoc: <code>http://137.0.0.1:8000/redoc</code></li> </ul>"},{"location":"python/13_Web_Development.html#handling-api-requests","title":"\ud83d\udd39 Handling API Requests","text":"<pre><code>@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int):\n    return {\"item_id\": item_id}\n</code></pre> <p>\u2705 Use Case: Building REST APIs, microservices, AI model deployment.</p>"},{"location":"python/13_Web_Development.html#134-mkdocs-documentation-for-python-projects","title":"13.4 \ud83d\udcd6 MkDocs \u2013 Documentation for Python Projects","text":"<p>MkDocs is a static site generator used for creating project documentation.</p>"},{"location":"python/13_Web_Development.html#installing-mkdocs","title":"\u2705 Installing MkDocs","text":"<pre><code>pip install mkdocs\n</code></pre>"},{"location":"python/13_Web_Development.html#creating-a-new-documentation-site","title":"\ud83d\udd39 Creating a New Documentation Site","text":"<pre><code>mkdocs new my_docs\ncd my_docs\nmkdocs serve\n</code></pre> <p>\ud83d\udd39 View your documentation: Visit <code>http://137.0.0.1:8000/</code> in your browser.</p>"},{"location":"python/13_Web_Development.html#adding-markdown-content","title":"\ud83d\udd39 Adding Markdown Content","text":"<p>Edit <code>docs/index.md</code>:</p> <pre><code># Welcome to My Docs\nThis is a test documentation page.\n</code></pre>"},{"location":"python/13_Web_Development.html#building-the-site","title":"\ud83d\udd39 Building the Site","text":"<pre><code>mkdocs build\n</code></pre> <p>\u2705 Use Case: Project documentation, API documentation, internal guides.</p>"},{"location":"python/13_Web_Development.html#135-streamlit-building-data-driven-web-apps","title":"13.5 \ud83d\udcca Streamlit \u2013 Building Data-Driven Web Apps","text":"<p>Streamlit is a simple framework for building interactive web applications for data visualization.</p>"},{"location":"python/13_Web_Development.html#installing-streamlit","title":"\u2705 Installing Streamlit","text":"<pre><code>pip install streamlit\n</code></pre>"},{"location":"python/13_Web_Development.html#creating-a-simple-streamlit-app","title":"\ud83d\udd39 Creating a Simple Streamlit App","text":"<p>Create <code>app.py</code>:</p> <pre><code>import streamlit as st\n\nst.title(\"Hello, Streamlit!\")\nst.write(\"This is a simple web app using Streamlit.\")\n</code></pre> <p>\ud83d\udd39 Run the app:</p> <pre><code>streamlit run app.py\n</code></pre> <p>\ud83d\udd39 View the app in the browser at <code>http://localhost:8501/</code>.</p>"},{"location":"python/13_Web_Development.html#adding-user-input-and-charts","title":"\ud83d\udd39 Adding User Input and Charts","text":"<pre><code>import pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame(\n    np.random.randn(10, 2),\n    columns=['A', 'B']\n)\n\nst.line_chart(data)\n</code></pre> <p>\u2705 Use Case: Building dashboards, AI model visualizations, interactive reports.</p>"},{"location":"python/13_Web_Development.html#summary","title":"\ud83d\ude80 Summary","text":"Framework Purpose Best For Flask Simple web applications Small apps, dashboards FastAPI High-speed APIs REST APIs, ML model deployment MkDocs Documentation generator API and project documentation Streamlit Data-driven web apps ML dashboards, data visualization"},{"location":"python/13_Web_Development.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Python offers powerful tools for web development, APIs, and data visualization. Would you like step-by-step projects on Flask, FastAPI, MkDocs, or Streamlit? \ud83d\ude80</p>"},{"location":"python/14_Data_Science.html","title":"14 \ud83d\udcca Data Science","text":"<p>Data Science involves analyzing, processing, and visualizing data to derive insights. Python is widely used in Data Science due to its rich ecosystem of libraries. This chapter covers NumPy, Pandas, and Matplotlib, the three core libraries for data manipulation, analysis, and visualization.</p>"},{"location":"python/14_Data_Science.html#141-numpy-numerical-computing","title":"14.1 \ud83d\udd22 NumPy \u2013 Numerical Computing","text":"<p>NumPy (Numerical Python) provides fast, efficient array operations and is the foundation for scientific computing in Python.</p>"},{"location":"python/14_Data_Science.html#installing-numpy","title":"\u2705 Installing NumPy","text":"<pre><code>pip install numpy\n</code></pre>"},{"location":"python/14_Data_Science.html#creating-numpy-arrays","title":"\ud83d\udd39 Creating NumPy Arrays","text":"<pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)  # Output: [1 2 3 4 5]\n</code></pre>"},{"location":"python/14_Data_Science.html#numpy-array-operations","title":"\ud83d\udd39 NumPy Array Operations","text":"<pre><code>a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\nprint(a + b)  # Output: [5 7 9]\nprint(a * b)  # Output: [4 10 18]\nprint(np.dot(a, b))  # Dot product: 1*4 + 2*5 + 3*6 = 32\n</code></pre>"},{"location":"python/14_Data_Science.html#generating-random-data","title":"\ud83d\udd39 Generating Random Data","text":"<pre><code>random_numbers = np.random.rand(5)  # 5 random numbers\nprint(random_numbers)\n</code></pre> <p>\u2705 Use Case: Data preprocessing, handling large numerical datasets efficiently.</p>"},{"location":"python/14_Data_Science.html#142-pandas-data-manipulation","title":"14.2 \ud83d\uddc3\ufe0f Pandas \u2013 Data Manipulation","text":"<p>Pandas simplifies working with structured data (tables, CSVs, JSONs, databases).</p>"},{"location":"python/14_Data_Science.html#installing-pandas","title":"\u2705 Installing Pandas","text":"<pre><code>pip install pandas\n</code></pre>"},{"location":"python/14_Data_Science.html#creating-a-dataframe","title":"\ud83d\udd39 Creating a DataFrame","text":"<pre><code>import pandas as pd\n\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"Salary\": [50000, 60000, 70000]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre>"},{"location":"python/14_Data_Science.html#reading-data-from-csv","title":"\ud83d\udd39 Reading Data from CSV","text":"<pre><code>df = pd.read_csv(\"data.csv\")\nprint(df.head())  # Display first 5 rows\n</code></pre>"},{"location":"python/14_Data_Science.html#filtering-data","title":"\ud83d\udd39 Filtering Data","text":"<pre><code>young_employees = df[df[\"Age\"] &lt; 30]\nprint(young_employees)\n</code></pre>"},{"location":"python/14_Data_Science.html#adding-a-new-column","title":"\ud83d\udd39 Adding a New Column","text":"<pre><code>df[\"Bonus\"] = df[\"Salary\"] * 0.10  # 10% bonus\nprint(df)\n</code></pre> <p>\u2705 Use Case: Handling datasets, cleaning and preparing data for ML models.</p>"},{"location":"python/14_Data_Science.html#143-matplotlib-data-visualization","title":"14.3 \ud83d\udcca Matplotlib \u2013 Data Visualization","text":"<p>Matplotlib is the primary library for plotting graphs and visualizing data.</p>"},{"location":"python/14_Data_Science.html#installing-matplotlib","title":"\u2705 Installing Matplotlib","text":"<pre><code>pip install matplotlib\n</code></pre>"},{"location":"python/14_Data_Science.html#plotting-a-line-graph","title":"\ud83d\udd39 Plotting a Line Graph","text":"<pre><code>import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 20, 25, 30]\n\nplt.plot(x, y, marker=\"o\", linestyle=\"-\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.title(\"Simple Line Plot\")\nplt.show()\n</code></pre>"},{"location":"python/14_Data_Science.html#creating-a-bar-chart","title":"\ud83d\udd39 Creating a Bar Chart","text":"<pre><code>categories = [\"A\", \"B\", \"C\", \"D\"]\nvalues = [10, 20, 15, 25]\n\nplt.bar(categories, values, color=\"blue\")\nplt.title(\"Bar Chart Example\")\nplt.show()\n</code></pre>"},{"location":"python/14_Data_Science.html#creating-a-histogram","title":"\ud83d\udd39 Creating a Histogram","text":"<pre><code>data = np.random.randn(1000)\n\nplt.hist(data, bins=30, color=\"green\")\nplt.title(\"Histogram of Random Data\")\nplt.show()\n</code></pre> <p>\u2705 Use Case: Exploratory Data Analysis (EDA), understanding dataset distributions.</p>"},{"location":"python/14_Data_Science.html#summary","title":"\ud83d\ude80 Summary","text":"Library Purpose Best For NumPy Fast numerical computing Arrays, linear algebra, statistics Pandas Data manipulation CSVs, databases, data wrangling Matplotlib Data visualization Graphs, charts, and plots"},{"location":"python/14_Data_Science.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Mastering NumPy, Pandas, and Matplotlib is essential for data science, machine learning, and AI applications. Would you like practical projects to apply these concepts? \ud83d\ude80</p>"},{"location":"python/15_Machine_Learning_and_AI.html","title":"15 \ud83e\udde0 Machine Learning and AI","text":"<p>Machine Learning (ML) and Artificial Intelligence (AI) enable computers to learn from data and make predictions. Python is the leading language for ML &amp; AI due to its rich ecosystem of libraries.  </p> <p>This chapter covers scikit-learn, TensorFlow, and PyTorch, which are widely used for data preprocessing, building ML models, and deep learning applications.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#151-what-is-machine-learning","title":"15.1 \ud83d\udd0d What is Machine Learning?","text":"<p>Machine Learning is a subset of AI that focuses on teaching computers to learn from data. It is classified into:  </p> Type Description Example Supervised Learning Uses labeled data Spam detection, house price prediction Unsupervised Learning Finds hidden patterns Clustering customers, anomaly detection Reinforcement Learning Learns by trial and error Game AI, robotics"},{"location":"python/15_Machine_Learning_and_AI.html#152-scikit-learn-machine-learning-for-beginners","title":"15.2 \ud83d\udee0\ufe0f Scikit-Learn \u2013 Machine Learning for Beginners","text":"<p>Scikit-learn is a simple and powerful ML library for classification, regression, clustering, and preprocessing.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#installing-scikit-learn","title":"\u2705 Installing Scikit-Learn","text":"<pre><code>pip install scikit-learn\n</code></pre>"},{"location":"python/15_Machine_Learning_and_AI.html#loading-a-dataset","title":"\ud83d\udd39 Loading a Dataset","text":"<pre><code>from sklearn.datasets import load_iris\nimport pandas as pd\n\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\nprint(df.head())\n</code></pre> <p>\u2705 Use Case: Loading datasets for training ML models.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#training-a-simple-classifier","title":"\ud83d\udd39 Training a Simple Classifier","text":"<pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre> <p>\u2705 Use Case: Building predictive models for classification problems.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#153-tensorflow-deep-learning-framework","title":"15.3 \ud83e\udd16 TensorFlow \u2013 Deep Learning Framework","text":"<p>TensorFlow is a powerful framework for building deep learning models.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#installing-tensorflow","title":"\u2705 Installing TensorFlow","text":"<pre><code>pip install tensorflow\n</code></pre>"},{"location":"python/15_Machine_Learning_and_AI.html#creating-a-simple-neural-network","title":"\ud83d\udd39 Creating a Simple Neural Network","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\n# Define a simple model\nmodel = keras.Sequential([\n    keras.layers.Dense(16, activation='relu', input_shape=(4,)),\n    keras.layers.Dense(3, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=8)\n</code></pre> <p>\u2705 Use Case: Training neural networks for AI applications.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#154-pytorch-deep-learning-for-research-ai","title":"15.4 \ud83d\udd25 PyTorch \u2013 Deep Learning for Research &amp; AI","text":"<p>PyTorch is widely used in AI research and deep learning experiments.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#installing-pytorch","title":"\u2705 Installing PyTorch","text":"<pre><code>pip install torch torchvision\n</code></pre>"},{"location":"python/15_Machine_Learning_and_AI.html#creating-a-simple-neural-network_1","title":"\ud83d\udd39 Creating a Simple Neural Network","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the model\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(4, 16)\n        self.fc2 = nn.Linear(16, 3)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return torch.softmax(self.fc2(x), dim=1)\n\nmodel = SimpleNN()\n</code></pre> <p>\u2705 Use Case: Developing custom AI models for image recognition, NLP, and robotics.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#155-model-evaluation-and-metrics","title":"15.5 \ud83d\udcca Model Evaluation and Metrics","text":"<p>Measuring model performance is critical in ML &amp; AI.</p> Metric Use Case Accuracy Classification problems Precision &amp; Recall Imbalanced datasets (e.g., fraud detection) Mean Squared Error (MSE) Regression models ROC Curve Evaluating classification models"},{"location":"python/15_Machine_Learning_and_AI.html#evaluating-a-model","title":"\ud83d\udd39 Evaluating a Model","text":"<pre><code>from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))\n</code></pre> <p>\u2705 Use Case: Assessing model performance before deployment.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#summary","title":"\ud83d\ude80 Summary","text":"Library Purpose Best For Scikit-learn Traditional ML models Classification, regression, clustering TensorFlow Deep learning Neural networks, AI applications PyTorch AI research Custom AI models, NLP, computer vision"},{"location":"python/15_Machine_Learning_and_AI.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Machine Learning and AI enable automation, predictions, and decision-making in various industries.  </p> <p>Would you like hands-on projects on ML &amp; AI? \ud83d\ude80</p>"},{"location":"python/16_Predictive_Modeling.html","title":"16 \ud83d\udd2e Predictive Modeling","text":"<p>Predictive modeling is the core of machine learning, where we use historical data to make future predictions. This chapter covers data preprocessing, feature engineering, model selection, evaluation, and deployment for predictive analytics.  </p>"},{"location":"python/16_Predictive_Modeling.html#161-understanding-predictive-modeling","title":"16.1 \ud83d\udcca Understanding Predictive Modeling","text":"<p>Predictive modeling involves training a machine learning model to make predictions based on input data. It follows these steps:</p> <p>1\ufe0f\u20e3 Data Collection \u2013 Gathering relevant data 2\ufe0f\u20e3 Data Preprocessing \u2013 Cleaning and preparing data 3\ufe0f\u20e3 Feature Engineering \u2013 Selecting important attributes 4\ufe0f\u20e3 Model Selection \u2013 Choosing the best algorithm 5\ufe0f\u20e3 Training &amp; Evaluation \u2013 Assessing model accuracy 6\ufe0f\u20e3 Prediction &amp; Deployment \u2013 Using the model for real-world applications  </p>"},{"location":"python/16_Predictive_Modeling.html#162-data-preprocessing","title":"16.2 \ud83d\udee0\ufe0f Data Preprocessing","text":"<p>Before training a model, we clean and transform data for better accuracy.</p>"},{"location":"python/16_Predictive_Modeling.html#handling-missing-values","title":"\u2705 Handling Missing Values","text":"<pre><code>import pandas as pd\n\ndf = pd.read_csv(\"data.csv\")\ndf.fillna(df.mean(), inplace=True)  # Replace missing values with mean\n</code></pre> <p>\u2705 Use Case: Cleaning noisy datasets before modeling.</p>"},{"location":"python/16_Predictive_Modeling.html#encoding-categorical-variables","title":"\ud83d\udd39 Encoding Categorical Variables","text":"<pre><code>df = pd.get_dummies(df, columns=[\"Category\"])\n</code></pre> <p>\u2705 Use Case: Converting text labels into numerical values for ML models.</p>"},{"location":"python/16_Predictive_Modeling.html#163-feature-engineering","title":"16.3 \ud83c\udfd7\ufe0f Feature Engineering","text":"<p>Feature engineering improves model accuracy by creating meaningful input variables.</p>"},{"location":"python/16_Predictive_Modeling.html#scaling-features","title":"\u2705 Scaling Features","text":"<pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df)\n</code></pre> <p>\u2705 Use Case: Normalizing features for models like Logistic Regression &amp; SVM.</p>"},{"location":"python/16_Predictive_Modeling.html#selecting-important-features","title":"\ud83d\udd39 Selecting Important Features","text":"<pre><code>from sklearn.feature_selection import SelectKBest, f_classif\n\nX_new = SelectKBest(score_func=f_classif, k=5).fit_transform(X, y)\n</code></pre> <p>\u2705 Use Case: Choosing the most relevant features for better predictions.</p>"},{"location":"python/16_Predictive_Modeling.html#164-choosing-the-right-model","title":"16.4 \ud83e\udd16 Choosing the Right Model","text":"<p>Different algorithms are suited for different predictive tasks.</p> Model Type Algorithm Use Case Classification Logistic Regression, Random Forest Fraud detection, spam filtering Regression Linear Regression, XGBoost Stock price prediction, sales forecasting Time Series ARIMA, LSTM Weather forecasting, demand prediction"},{"location":"python/16_Predictive_Modeling.html#training-a-predictive-model","title":"\u2705 Training a Predictive Model","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n</code></pre> <p>\u2705 Use Case: Training an AI model to predict future outcomes.</p>"},{"location":"python/16_Predictive_Modeling.html#165-model-evaluation-performance-metrics","title":"16.5 \ud83d\udcca Model Evaluation &amp; Performance Metrics","text":"<p>Evaluating model accuracy ensures reliable predictions.</p>"},{"location":"python/16_Predictive_Modeling.html#checking-accuracy","title":"\u2705 Checking Accuracy","text":"<pre><code>from sklearn.metrics import accuracy_score\n\ny_pred = model.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"python/16_Predictive_Modeling.html#confusion-matrix","title":"\ud83d\udd39 Confusion Matrix","text":"<pre><code>from sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\n</code></pre> <p>\u2705 Use Case: Measuring classification model performance.</p>"},{"location":"python/16_Predictive_Modeling.html#166-making-predictions","title":"16.6 \ud83d\udd2e Making Predictions","text":"<p>After training, we use the model to predict real-world data.</p> <pre><code>new_data = [[5.1, 3.5, 1.4, 0.2]]\nprediction = model.predict(new_data)\nprint(\"Predicted Class:\", prediction)\n</code></pre> <p>\u2705 Use Case: Predicting customer behavior, stock prices, or disease diagnosis.</p>"},{"location":"python/16_Predictive_Modeling.html#167-deploying-the-model","title":"16.7 \ud83d\ude80 Deploying the Model","text":"<p>A trained model can be deployed using Flask, FastAPI, or Streamlit.</p>"},{"location":"python/16_Predictive_Modeling.html#saving-and-loading-the-model","title":"\u2705 Saving and Loading the Model","text":"<pre><code>import joblib\n\njoblib.dump(model, \"model.pkl\")  # Save model\nloaded_model = joblib.load(\"model.pkl\")  # Load model\n</code></pre> <p>\u2705 Use Case: Deploying AI models into production systems.</p>"},{"location":"python/16_Predictive_Modeling.html#summary","title":"\ud83d\ude80 Summary","text":"Step Description Data Preprocessing Cleaning and transforming data Feature Engineering Selecting the most important variables Model Selection Choosing the best ML algorithm Training &amp; Evaluation Assessing model performance Prediction &amp; Deployment Using the model for real-world applications"},{"location":"python/16_Predictive_Modeling.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Predictive modeling is widely used in finance, healthcare, and business intelligence.  </p> <p>Would you like a hands-on project to build a real-world predictive model? \ud83d\ude80</p>"},{"location":"python/17_Databases.html","title":"17 \ud83c\udfdb\ufe0f Databases","text":"<p>Databases are essential for storing, managing, and retrieving structured data. Python provides built-in and third-party libraries for working with SQL and NoSQL databases.</p> <p>This chapter covers SQLite, PostgreSQL, MySQL, and MongoDB, along with how to perform CRUD operations (Create, Read, Update, Delete).</p>"},{"location":"python/17_Databases.html#171-types-of-databases","title":"17.1 \ud83d\uddc2\ufe0f Types of Databases","text":"Database Type Example Best For Relational (SQL) SQLite, MySQL, PostgreSQL Structured data with relationships NoSQL (Document-based) MongoDB, Firebase Unstructured or semi-structured data"},{"location":"python/17_Databases.html#172-working-with-sqlite-lightweight-sql-database","title":"17.2 \ud83c\udfd7\ufe0f Working with SQLite (Lightweight SQL Database)","text":"<p>SQLite is a lightweight, file-based SQL database that comes pre-installed with Python.</p>"},{"location":"python/17_Databases.html#connecting-to-sqlite","title":"\u2705 Connecting to SQLite","text":"<pre><code>import sqlite3\n\nconn = sqlite3.connect(\"database.db\")  # Creates/opens a database file\ncursor = conn.cursor()\n</code></pre>"},{"location":"python/17_Databases.html#creating-a-table","title":"\ud83d\udd39 Creating a Table","text":"<pre><code>cursor.execute('''\n    CREATE TABLE IF NOT EXISTS users (\n        id INTEGER PRIMARY KEY,\n        name TEXT,\n        age INTEGER\n    )\n''')\nconn.commit()\n</code></pre>"},{"location":"python/17_Databases.html#inserting-data","title":"\ud83d\udd39 Inserting Data","text":"<pre><code>cursor.execute(\"INSERT INTO users (name, age) VALUES (?, ?)\", (\"Alice\", 25))\nconn.commit()\n</code></pre>"},{"location":"python/17_Databases.html#reading-data","title":"\ud83d\udd39 Reading Data","text":"<pre><code>cursor.execute(\"SELECT * FROM users\")\nprint(cursor.fetchall())  # Output: [(1, 'Alice', 25)]\n</code></pre> <p>\u2705 Use Case: Small applications, local storage, prototyping.</p>"},{"location":"python/17_Databases.html#173-working-with-postgresql-mysql-sql-databases-for-large-applications","title":"17.3 \ud83c\udfe6 Working with PostgreSQL &amp; MySQL (SQL Databases for Large Applications)","text":""},{"location":"python/17_Databases.html#installing-postgresqlmysql-connector","title":"\u2705 Installing PostgreSQL/MySQL Connector","text":"<pre><code>pip install psycopg2  # PostgreSQL\npip install mysql-connector-python  # MySQL\n</code></pre>"},{"location":"python/17_Databases.html#connecting-to-postgresql","title":"\ud83d\udd39 Connecting to PostgreSQL","text":"<pre><code>import psycopg2\n\nconn = psycopg2.connect(\n    dbname=\"mydb\",\n    user=\"postgres\",\n    password=\"mypassword\",\n    host=\"localhost\"\n)\ncursor = conn.cursor()\n</code></pre>"},{"location":"python/17_Databases.html#querying-a-postgresql-table","title":"\ud83d\udd39 Querying a PostgreSQL Table","text":"<pre><code>cursor.execute(\"SELECT * FROM users\")\nrows = cursor.fetchall()\nfor row in rows:\n    print(row)\n</code></pre> <p>\u2705 Use Case: Enterprise applications, web applications, analytics.</p>"},{"location":"python/17_Databases.html#174-working-with-mongodb-nosql-database","title":"17.4 \ud83c\udf43 Working with MongoDB (NoSQL Database)","text":"<p>MongoDB stores data in JSON-like documents.</p>"},{"location":"python/17_Databases.html#installing-mongodb-driver","title":"\u2705 Installing MongoDB Driver","text":"<pre><code>pip install pymongo\n</code></pre>"},{"location":"python/17_Databases.html#connecting-to-mongodb","title":"\ud83d\udd39 Connecting to MongoDB","text":"<pre><code>import pymongo\n\nclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\ndb = client[\"mydatabase\"]\ncollection = db[\"users\"]\n</code></pre>"},{"location":"python/17_Databases.html#inserting-a-document","title":"\ud83d\udd39 Inserting a Document","text":"<pre><code>user = {\"name\": \"Alice\", \"age\": 25}\ncollection.insert_one(user)\n</code></pre>"},{"location":"python/17_Databases.html#retrieving-data","title":"\ud83d\udd39 Retrieving Data","text":"<pre><code>for user in collection.find():\n    print(user)\n</code></pre> <p>\u2705 Use Case: Big data, real-time analytics, IoT applications.</p>"},{"location":"python/17_Databases.html#175-crud-operations-in-databases","title":"17.5 \ud83d\udd04 CRUD Operations in Databases","text":"Operation SQL Example MongoDB Example Create <code>INSERT INTO users VALUES (1, 'Alice', 25);</code> <code>collection.insert_one({\"name\": \"Alice\", \"age\": 25})</code> Read <code>SELECT * FROM users;</code> <code>collection.find({})</code> Update <code>UPDATE users SET age=30 WHERE name='Alice';</code> <code>collection.update_one({\"name\": \"Alice\"}, {\"$set\": {\"age\": 30}})</code> Delete <code>DELETE FROM users WHERE name='Alice';</code> <code>collection.delete_one({\"name\": \"Alice\"})</code> <p>\u2705 Use Case: Building full-stack web applications, managing structured/unstructured data.</p>"},{"location":"python/17_Databases.html#summary","title":"\ud83d\ude80 Summary","text":"Database Best For SQLite Small applications, local storage PostgreSQL/MySQL Large-scale, structured data MongoDB Flexible, unstructured data"},{"location":"python/17_Databases.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Databases are essential for storing and managing data in Python applications. Would you like a real-world project on integrating databases with Python? \ud83d\ude80</p>"},{"location":"python/17x_Examples.html","title":"Real-World Examples","text":""},{"location":"python/best_practice.html","title":"Best Practice","text":""},{"location":"python/best_practice.html#production-grade-python-code-styles-every-python-developer-uses","title":"Production-Grade Python Code Styles Every Python Developer Uses","text":"<p>Python has grown into one of the most popular and versatile programming languages in the world. Its clean and readable syntax has made it a favorite for a wide range of applications, from web development to data science. However, while the language itself is simple, writing production-grade Python code requires attention to detail, style, and best practices to ensure maintainability, scalability, and readability. In this article, we\u2019ll explore 20 Python code styles that every developer should follow to ensure their code meets the high standards required for production environments.</p>"},{"location":"python/best_practice.html#1-follow-pep-8-guidelines","title":"1. Follow PEP 8 Guidelines","text":"<p>The Python Enhancement Proposal 8 (PEP 8) is the official style guide for Python code. It covers conventions such as indentation, line length, blank lines, imports, and naming conventions. Following PEP 8 ensures that your code is consistent and readable for other developers who may work on the same project. Some key points to remember include:</p> <p>Use 4 spaces per indentation level (avoid tabs). Limit lines to 79 characters for code and 72 characters for docstrings. Keep a single blank line between functions and methods. By adhering to PEP 8, you ensure that your Python code follows a universally accepted format, making collaboration and code reviews smoother.</p>"},{"location":"python/best_practice.html#2-use-descriptive-variable-and-function-names","title":"2. Use Descriptive Variable and Function Names","text":"<p>Choosing meaningful and descriptive names for variables and functions is crucial for code readability and maintainability. Avoid vague names like temp, data, or foo. Instead, opt for names that clearly convey their purpose. For example:</p> <p>Use num_items instead of x. Use get_user_info() instead of do_stuff(). A good rule of thumb is to think about how someone unfamiliar with your code would interpret the name.</p>"},{"location":"python/best_practice.html#3-write-clear-and-concise-docstrings","title":"3. Write Clear and Concise Docstrings","text":"<p>Documentation is essential for any code, and Python encourages using docstrings to document functions, classes, and modules. Always write clear, concise docstrings for your functions and methods to explain what they do, what parameters they accept, and what they return.</p> <p>For example:</p> <p>def add(a, b):     \"\"\"     Add two numbers.</p> <pre><code>Args:\n    a (int): The first number.\n    b (int): The second number.\n\nReturns:\n    int: The sum of a and b.\n\"\"\"\nreturn a + b\n</code></pre> <p>Following the standard format and providing comprehensive docstrings can make your code much easier to understand and maintain.</p>"},{"location":"python/best_practice.html#4-use-type-hinting","title":"4. Use Type Hinting","text":"<p>Type hinting allows you to explicitly declare the types of variables, function parameters, and return values. This feature improves code clarity, helps catch errors early, and assists IDEs with better code suggestions. For example:</p> <p>def greet(name: str) -&gt; str:     return f\"Hello, {name}!\" Type hinting improves code readability and reduces the chances of runtime errors.</p>"},{"location":"python/best_practice.html#5-use-list-comprehensions-and-generator-expressions","title":"5. Use List Comprehensions and Generator Expressions","text":"<p>Python\u2019s list comprehensions and generator expressions allow you to write more concise and readable code for generating lists or iterating over items. For example, instead of writing:</p> <p>squared_numbers = [] for num in numbers:     squared_numbers.append(num ** 2) You can use a list comprehension:</p> <p>squared_numbers = [num ** 2 for num in numbers] This approach is more compact, readable, and Pythonic. It can be extended to generator expressions when you want to save memory by not storing the entire list in memory.</p>"},{"location":"python/best_practice.html#6-leverage-pythons-built-in-functions","title":"6. Leverage Python\u2019s Built-in Functions","text":"<p>Python comes with a rich set of built-in functions, such as map(), filter(), reduce(), and zip(), which can make your code more efficient and readable. For example, instead of writing:</p> <p>squared_numbers = [] for num in numbers:     squared_numbers.append(num ** 2) You can use the map() function:</p> <p>squared_numbers = list(map(lambda num: num ** 2, numbers)) These functions often lead to more concise, readable, and functional code.</p>"},{"location":"python/best_practice.html#7-keep-functions-small-and-focused","title":"7. Keep Functions Small and Focused","text":"<p>A good practice in writing production-grade code is to keep functions small and focused on a single task. A function should do one thing and do it well. Avoid making functions too large or performing multiple unrelated tasks, as this can make your code harder to understand, test, and maintain.</p>"},{"location":"python/best_practice.html#8-handle-exceptions-properly","title":"8. Handle Exceptions Properly","text":"<p>Robust error handling is essential in production environments. Use try and except blocks to catch potential exceptions, but make sure to catch specific exceptions rather than using a generic except clause. For example:</p> <p>try:     result = 10 / divisor except ZeroDivisionError:     print(\"Divisor cannot be zero\") Additionally, make sure to log errors so they can be traced later.</p>"},{"location":"python/best_practice.html#9-use-logging-for-debugging-and-monitoring","title":"9. Use Logging for Debugging and Monitoring","text":"<p>Instead of relying on print() statements for debugging, use Python\u2019s built-in logging module. It provides a flexible framework for emitting logs at different severity levels, such as DEBUG, INFO, WARNING, ERROR, and CRITICAL. For example:</p> <p>import logging logging.basicConfig(level=logging.DEBUG) def calculate_sum(a, b):     logging.debug(f\"Adding {a} and {b}\")     return a + b Logging is essential for production environments because it provides better traceability and control over what\u2019s happening in your application.</p>"},{"location":"python/best_practice.html#10-use-virtual-environments","title":"10. Use Virtual Environments","text":"<p>A virtual environment ensures that your project\u2019s dependencies are isolated from other projects. This is crucial in production environments, as it prevents version conflicts between different Python packages. Use venv to create a virtual environment for your project:</p> <p>python3 -m venv venv Activate it with:</p> <p>source venv/bin/activate  # On Mac/Linux venv\\Scripts\\activate     # On Windows</p>"},{"location":"python/best_practice.html#11-use-dependency-management-tools","title":"11. Use Dependency Management Tools","text":"<p>Managing project dependencies is crucial for ensuring consistency and avoiding conflicts. Use tools like pip, pipenv, or poetry to manage your Python dependencies. The requirements.txt file or pyproject.toml helps maintain a record of all dependencies for reproducibility.</p> <p>For example, to freeze dependencies in a requirements.txt file, you can use:</p> <p>pip freeze &gt; requirements.txt</p>"},{"location":"python/best_practice.html#12-write-unit-tests","title":"12. Write Unit Tests","text":"<p>Unit testing is a crucial part of writing production-grade code. Writing automated tests helps ensure your code works as expected and prevents bugs from slipping into production. Python\u2019s built-in unittest module makes it easy to write unit tests for your functions.</p> <p>Example:</p> <p>import unittest class TestMathFunctions(unittest.TestCase):     def test_addition(self):         self.assertEqual(add(1, 2), 3) if name == 'main':     unittest.main() Test-driven development (TDD) can greatly improve the stability and quality of your code.</p>"},{"location":"python/best_practice.html#13-ensure-code-is-modular","title":"13. Ensure Code is Modular","text":"<p>Modular code is easier to maintain and scale. Split your code into smaller, reusable modules with a single responsibility. Each module should have one task and should be able to interact with other modules through well-defined interfaces. This improves readability and helps with testing.</p>"},{"location":"python/best_practice.html#14-use-list-and-dict-unpacking","title":"14. Use List and Dict Unpacking","text":"<p>Python allows you to unpack lists and dictionaries in a concise and readable way. For example, when working with tuples:</p> <p>x, y = (1, 2) For dictionaries:</p> <p>data = {\"name\": \"Alice\", \"age\": 30} name, age = data.values() Unpacking can make your code more compact and expressive.</p>"},{"location":"python/best_practice.html#15-avoid-using-global-variables","title":"15. Avoid Using Global Variables","text":"<p>Global variables can make your code difficult to debug and maintain. Instead, pass data explicitly between functions or use classes to encapsulate related functionality. When global state is absolutely necessary, ensure it\u2019s well-documented and controlled.</p>"},{"location":"python/best_practice.html#16-use-decorators-to-enhance-functionality","title":"16. Use Decorators to Enhance Functionality","text":"<p>Python decorators allow you to add functionality to functions or methods in a clean and reusable way. For example, a logging decorator can be used to log function calls:</p> <p>def log_function_call(func):     def wrapper(args, kwargs):         print(f\"Calling {func.name} with arguments {args} and {kwargs}\")         return func(args,**kwargs)     return wrapper Decorators enhance the readability and reusability of your code by abstracting common logic.</p>"},{"location":"python/best_practice.html#17-use-f-strings-for-string-formatting","title":"17. Use f-strings for String Formatting","text":"<p>f-strings, introduced in Python 3.6, are the recommended way to format strings due to their readability and performance. Instead of using the older format() method or % operator, use:</p> <p>name = \"Alice\" greeting = f\"Hello, {name}!\" This is not only more concise but also faster than the alternatives.</p>"},{"location":"python/best_practice.html#18-avoid-premature-optimization","title":"18. Avoid Premature Optimization","text":"<p>While it\u2019s tempting to optimize your code for performance, premature optimization can lead to complex, unreadable code. Focus on writing clean, correct code first, and optimizing only when necessary, using profiling tools to identify bottlenecks.</p>"},{"location":"python/best_practice.html#19-write-code-with-scalability-in-mind","title":"19. Write Code with Scalability in Mind","text":"<p>When writing production code, always think about scalability. This includes considering database query efficiency, memory usage, concurrency, and network performance. Always aim for code that can scale as traffic, data, or users increase.</p>"},{"location":"python/best_practice.html#20-adopt-continuous-integration-ci-and-continuous-delivery-cd","title":"20. Adopt Continuous Integration (CI) and Continuous Delivery (CD)","text":"<p>Continuous Integration (CI) and Continuous Delivery (CD) pipelines automate the process of integrating code changes, testing them, and deploying them. Use tools like GitHub Actions, Jenkins, or Travis CI to automate the testing and deployment of your code. This ensures that bugs are caught early and your code is always ready for production.</p>"},{"location":"python/best_practice.html#conclusion","title":"Conclusion","text":"<p>Writing production-grade Python code requires adherence to best practices that prioritize readability, maintainability, and performance. By following these 20 coding styles, you can ensure that your Python code is high-quality, scalable, and easy to work with in a professional setting. Whether you\u2019re working solo or in a team, these practices will make your development process smoother and your codebase more reliable.</p>"},{"location":"python/f_strings.html","title":"F Strings","text":"<p>Here\u2019s a summary of the 5 Powerful F-String Tricks Every Python Developer Should Know from the article:</p>"},{"location":"python/f_strings.html#key-takeaways","title":"\ud83d\udd11 Key Takeaways:","text":"<ol> <li>Inline Expressions    F-strings allow evaluating expressions directly:</li> </ol> <pre><code>name = \"John\"\nage = 25\nprint(f\"{name} will be {age + 1} next year.\")\n# Output: John will be 26 next year.\n</code></pre> <ol> <li>Formatting Numbers    Easily control decimal places, commas, and percentages:</li> </ol> <pre><code>price = 49.98765\nprint(f\"Price: {price:.2f}\")  # Output: Price: 49.99\n\nbig_number = 1000000\nprint(f\"Formatted: {big_number:,}\")  # Output: 1,000,000\n\nscore = 0.8745\nprint(f\"Success rate: {score:.2%}\")  # Output: 87.45%\n</code></pre> <ol> <li>Debugging with <code>=</code>    Python 3.8+ supports variable name introspection inside f-strings:</li> </ol> <pre><code>x = 10\ny = 5\nprint(f\"{x=}, {y=}, {x + y=}\")\n# Output: x=10, y=5, x + y=15\n</code></pre> <ol> <li>Dynamic Formatting (Nesting F-Strings)    Use variables inside formatting specifiers:</li> </ol> <pre><code>width = 10\nnumber = 42.56789\nprint(f\"{number:.{width}f}\")  # Output: 42.5678900000\n</code></pre> <ol> <li>Multi-line F-Strings    Use f-strings with triple quotes for readable formatted blocks:</li> </ol> <pre><code>name = \"John\"\nage = 25\ncity = \"New York\"\n\ninfo = f\"\"\"\nName: {name}\nAge: {age}\nCity: {city}\n\"\"\"\nprint(info)\n</code></pre>"},{"location":"python/pandas.html","title":"Pandas","text":"<p>A beginner-friendly guide to Pandas for data analysis.</p>"},{"location":"python/yfinance.html","title":"Yfinance","text":"<p>How to use <code>yfinance</code> to fetch and analyze stock market data.</p>"},{"location":"services/data_science.html","title":"Data Science","text":"<p>An overview of data science techniques and methodologies used in Dbnostix.</p>"},{"location":"services/database_infrastructure.html","title":"Database Infrastructure","text":"<p>Details on database infrastructure solutions and best practices.</p>"},{"location":"services/large_language_model.html","title":"Large Language Model","text":"<p>An explanation of large language models and services related to them.</p>"},{"location":"services/predictive_modeling.html","title":"Predictive Modeling","text":"<p>Insights into predictive modeling and its applications.</p>"}]}