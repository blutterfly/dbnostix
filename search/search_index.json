{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome","text":"<p>Precision Analytics, Powerful Results</p> <p>DBNOSTIX specializes in state-of-the-art analytics and diagnostics, empowering businesses with actionable insights to enhance performance and fuel growth.</p> Expertise: Data Analytics | AI Integration | Diagnostics | Real-time Monitoring"},{"location":"index.html#why-choose-us","title":"Why Choose Us?","text":"<ul> <li>Cutting-Edge Technology: Advanced AI-driven solutions for precise analytics.</li> <li>Proven Track Record: Trusted partner delivering measurable business impact.</li> <li>Customized Solutions: Tailored services addressing your unique challenges.</li> </ul>"},{"location":"index.html#our-services","title":"Our Services","text":"Data Analytics <p>Convert complex data into clear, actionable insights tailored for your business needs.</p> AI Integration <p>Seamlessly integrate AI to streamline operations and enhance your competitive edge.</p> Real-Time Monitoring <p>Proactive diagnostics and monitoring, ensuring optimal performance and early issue detection.</p>   ## Trusted By Professionals   <p>\"DBNOSTIX analytics elevated our decision-making to unprecedented clarity and speed.\"</p> Jordan Hayes, CTO at DataMatrix Corp.   ## About Us  DBNOSTIX was founded with the mission to simplify complex data and empower organizations through insightful, real-time diagnostics. Our experienced team leverages state-of-the-art analytics to deliver impactful results, driving efficiency, clarity, and growth.    ## Contact Us  Ready to leverage powerful analytics for your business? Reach out today:   Contact DBNOSTIX \u00a9 2025 DBNOSTIX | All Rights Reserved"},{"location":"algotrade/algotrade.html","title":"Algotrade","text":"<p>A deep dive into algorithmic trading concepts.</p>"},{"location":"algotrade/extract.html","title":"Extract","text":"<p>Techniques and tools for extracting financial data.</p>"},{"location":"algotrade/load.html","title":"Load","text":"<p>Loading financial data into storage systems for further processing.</p>"},{"location":"algotrade/transform.html","title":"Transform","text":"<p>Transforming and cleaning financial data for analysis.</p>"},{"location":"database/database.html","title":"Database","text":"<p>General overview of databases and their importance.</p>"},{"location":"database/db2.html","title":"Db2","text":"<p>Introduction to IBM Db2 and its features.</p>"},{"location":"database/duckdb.html","title":"Duckdb","text":"<p>Introduction to DuckDB, an in-memory analytical database.</p>"},{"location":"database/postgresql.html","title":"Postgresql","text":"<p>A guide to PostgreSQL and its use cases.</p>"},{"location":"dbnostix/about.html","title":"About","text":"<p>Information about the Dbnostix platform, its history, and goals.</p>"},{"location":"dbnostix/contact.html","title":"Contact","text":"<p>Contact details and ways to reach out for inquiries and support.</p>"},{"location":"dbnostix/contents.html","title":"Contents","text":"<p>An overview of Dbnostix, its purpose, and key features.</p> <p>See top navigation bar above for a list of topics covered.</p>"},{"location":"dbnostix/help.html","title":"Help","text":""},{"location":"dbnostix/services.html","title":"Services","text":"<p>A summary of services offered by Dbnostix.</p>"},{"location":"mkdocs/admonitions.html","title":"Admonitions","text":"<p>In MkDocs Material, callouts are called Admonitions. You can implement them easily by following these steps:</p>"},{"location":"mkdocs/admonitions.html#topics","title":"Topics","text":""},{"location":"mkdocs/admonitions.html#modify-mkdocsyml","title":"Modify mkdocs.yml","text":"<p>To enable adminitions, ensure that your Material theme supports admonitions by adding these lines to your configuration file:</p> <pre><code>markdown_extensions:\n  - admonition\n  - pymdownx.details\n  - pymdownx.superfences\n</code></pre>"},{"location":"mkdocs/admonitions.html#syntax-for-callouts","title":"Syntax for Callouts","text":"<p>Here's a basic example:</p> <pre><code>!!! note \"Optional title\"\n    This is a simple note callout.\n</code></pre>"},{"location":"mkdocs/admonitions.html#types-of-admonitions","title":"Types of Admonitions","text":"<p>You can use various predefined types:</p> <p>Info</p> <ul> <li>note</li> <li>info</li> <li>tip</li> <li>warning</li> <li>danger</li> <li>success</li> <li>question</li> <li>example</li> <li>quote</li> <li>bug</li> </ul>"},{"location":"mkdocs/admonitions.html#examples","title":"Examples","text":"<pre><code>!!! info \"Important Information\"\n    Please read carefully before proceeding.\n</code></pre> <p>Important Information</p> <p>Please read carefully before proceeding.</p> <pre><code>!!! warning \"Be Careful!\"\n    You might lose your changes if you proceed without saving.\n</code></pre> <p>Be Careful!</p> <p>You might lose your changes if you proceed without saving.</p> <pre><code>??? tip \"Collapsible tip\"\n    This content is hidden until expanded by the user.\n</code></pre> Collapsible tip <p>This content is hidden until expanded by the user.</p>"},{"location":"mkdocs/collapsible_sections.html","title":"Collapsible Sections","text":"<p>How to create collapsible sections in MkDocs.</p>"},{"location":"mkdocs/mkdocs.html","title":"Overview","text":"<p>Getting started with MkDocs for documentation.</p>"},{"location":"options/contents.html","title":"Options","text":"<p>Collection of strategy, how-to and python script.</p>"},{"location":"options/strangle_strategy.html","title":"Strangle Strategy","text":""},{"location":"options/strangle_strategy.html#what-is","title":"What is","text":"<p>A Strangle is an options trading strategy that involves buying or selling both a call option and a put option with the same expiration date but different strike prices. It is used to take advantage of expected volatility in the underlying asset.</p>"},{"location":"options/strangle_strategy.html#option-strangle-strategy","title":"Option Strangle Strategy","text":""},{"location":"options/strangle_strategy.html#types-of-strangle-strategies","title":"Types of Strangle Strategies","text":"<ol> <li>Long Strangle (Buying a Strangle)</li> <li>Buy an out-of-the-money (OTM) call option.</li> <li>Buy an out-of-the-money (OTM) put option.</li> <li> <p>Used when a trader expects high volatility but is unsure of the direction.</p> </li> <li> <p>Short Strangle (Selling a Strangle)</p> </li> <li>Sell an out-of-the-money (OTM) call option.</li> <li>Sell an out-of-the-money (OTM) put option.</li> <li>Used when a trader expects low volatility and wants to collect premium.</li> </ol>"},{"location":"options/strangle_strategy.html#long-strangle-strategy","title":"Long Strangle Strategy","text":"<ul> <li>Objective: Profit from a significant price move in either direction.</li> <li>Max Loss: Limited to the total premium paid.</li> <li>Max Profit: Unlimited (if the stock moves significantly beyond the strike prices).</li> <li>Breakeven Points:</li> <li>Upper BEP = Call strike price + Premium paid.</li> <li>Lower BEP = Put strike price - Premium paid.</li> </ul> <p>\u2705 Best for: Volatile markets, earnings reports, major news events.</p> <p>\ud83d\udd34 Risk: If the stock remains within the strike prices, the options expire worthless.</p>"},{"location":"options/strangle_strategy.html#short-strangle-strategy","title":"Short Strangle Strategy","text":"<ul> <li>Objective: Profit from low volatility and time decay.</li> <li>Max Profit: Limited to the premium collected.</li> <li>Max Loss: Potentially unlimited if the stock moves sharply.</li> <li>Breakeven Points:</li> <li>Upper BEP = Call strike price + Premium received.</li> <li>Lower BEP = Put strike price - Premium received.</li> </ul> <p>\u2705 Best for: Stable or low-volatility markets.</p> <p>\ud83d\udd34 Risk: Large losses if the stock price moves sharply in either direction.</p>"},{"location":"options/strangle_strategy.html#example-of-a-long-strangle","title":"Example of a Long Strangle","text":"<p>Stock: XYZ trading at $100  </p> <ul> <li>Buy 110 Call at $2.00</li> <li>Buy 90 Put at $2.00  </li> <li>Total Cost: $4.00</li> </ul> <p>Breakeven Points:</p> <ul> <li>Upper BEP = $110 + $4 = $114</li> <li>Lower BEP = $90 - $4 = $86</li> </ul> <p>Profit Scenarios:</p> <ul> <li>If stock moves above $114, the call option gains value.</li> <li>If stock moves below $86, the put option gains value.</li> <li>If stock stays between $90 and $110, both options expire worthless, and you lose the $4 premium.</li> </ul>"},{"location":"options/strangle_strategy.html#example-of-a-short-strangle","title":"Example of a Short Strangle","text":"<p>Stock: XYZ trading at $100  </p> <ul> <li>Sell 110 Call for $2.00</li> <li>Sell 90 Put for $2.00  </li> <li>Total Premium Collected: $4.00</li> </ul> <p>Breakeven Points:</p> <ul> <li>Upper BEP = $110 + $4 = $114</li> <li>Lower BEP = $90 - $4 = $86</li> </ul> <p>Profit Scenarios:</p> <ul> <li>If stock stays between $90 and $110, both options expire worthless, and you keep the premium.</li> <li>If stock moves above $114 or below $86, you face unlimited risk.</li> </ul>"},{"location":"options/strangle_strategy.html#key-takeaways","title":"Key Takeaways","text":"<p>\u2714\ufe0f Long Strangle: Limited risk, unlimited profit potential, best for high volatility. \u2714\ufe0f Short Strangle: Limited profit, unlimited risk, best for low volatility. \u2714\ufe0f Breakeven Points: Critical to manage risk and set exit points. \u2714\ufe0f Implied Volatility (IV): Higher IV makes long strangles more expensive, while lower IV makes short strangles more profitable.  </p>"},{"location":"options/strangle_strategy.html#script-to-backtest-this-strategy","title":"Script to backtest this strategy \ud83d\ude80","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport yfinance as yf\n\n# Define parameters for the strangle strategy\nticker = \"AAPL\"\nstart_date = \"2023-01-01\"\nend_date = \"2024-01-01\"\nstrike_diff = 5  # Difference from ATM for Call and Put\npremium_paid = 4  # Estimated cost of buying both options\npremium_received = 4  # Estimated premium collected for short strangle\n\n# Fetch historical stock data\ndata = yf.download(ticker, start=start_date, end=end_date)\ndata[\"MidPrice\"] = (data[\"High\"] + data[\"Low\"]) / 2\n\n# Define breakeven points\ndata[\"Upper_BEP_Long\"] = data[\"MidPrice\"] + strike_diff + premium_paid\ndata[\"Lower_BEP_Long\"] = data[\"MidPrice\"] - strike_diff - premium_paid\ndata[\"Upper_BEP_Short\"] = data[\"MidPrice\"] + strike_diff + premium_received\ndata[\"Lower_BEP_Short\"] = data[\"MidPrice\"] - strike_diff - premium_received\n\n# Calculate long strangle profit/loss\ndata[\"Long_Strangle_PL\"] = np.where(\n    (data[\"MidPrice\"] &gt; data[\"Upper_BEP_Long\"]) | (data[\"MidPrice\"] &lt; data[\"Lower_BEP_Long\"]),\n    abs(data[\"MidPrice\"] - data[\"Upper_BEP_Long\"]) - premium_paid,\n    -premium_paid\n)\n\n# Calculate short strangle profit/loss\ndata[\"Short_Strangle_PL\"] = np.where(\n    (data[\"MidPrice\"] &gt; data[\"Upper_BEP_Short\"]) | (data[\"MidPrice\"] &lt; data[\"Lower_BEP_Short\"]),\n    -abs(data[\"MidPrice\"] - data[\"Upper_BEP_Short\"]) + premium_received,\n    premium_received\n)\n\n\n# Display first few rows using pprint\npprint(data[[\"MidPrice\", \"Long_Strangle_PL\", \"Short_Strangle_PL\"]].head())\n</code></pre>"},{"location":"pandas/05_PandasGUI.html","title":"PandasGUI","text":""},{"location":"pandas/05_PandasGUI.html#what-is-pandasgui","title":"What is PandasGUI?","text":"<p>PandasGUI is a Python-based graphical user interface (GUI) tool that allows users to interactively explore and manipulate Pandas DataFrames without writing extensive code. It provides a user-friendly way to perform data analysis, generate visualizations, and automatically generate Python scripts for operations performed through the interface.</p>"},{"location":"pandas/05_PandasGUI.html#installation","title":"Installation","text":"<p>To install PandasGUI, run: <pre><code>pip install pandasgui\n</code></pre> For better compatibility, it is recommended to use Python 3.8 or above in a virtual environment.</p> <p>To create a virtual environment and install PandasGUI: <pre><code>conda create -n pandasgui_env python=3.8\nconda activate pandasgui_env\npip install pandasgui\n</code></pre></p>"},{"location":"pandas/05_PandasGUI.html#using-pandasgui","title":"Using PandasGUI","text":""},{"location":"pandas/05_PandasGUI.html#1-launching-pandasgui","title":"1. Launching PandasGUI","text":"<p>To open PandasGUI without any dataset: <pre><code>from pandasgui import show\nshow()\n</code></pre> This will launch an interactive GUI where you can manually load datasets.</p>"},{"location":"pandas/05_PandasGUI.html#2-loading-a-dataframe","title":"2. Loading a DataFrame","text":"<p>To open PandasGUI with a DataFrame: <pre><code>import pandas as pd\nfrom pandasgui import show\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n})\n\n# Launch GUI\nshow(df)\n</code></pre> This will open the GUI with the dataset loaded, allowing you to explore it visually.</p>"},{"location":"pandas/05_PandasGUI.html#features-of-pandasgui","title":"Features of PandasGUI","text":"<ol> <li>Interactive Data Viewing  </li> <li>View datasets in an Excel-like format.</li> <li> <p>Sort and filter rows using a graphical interface.</p> </li> <li> <p>Data Editing</p> </li> <li> <p>Directly edit cells within the GUI.</p> </li> <li> <p>Visualization Tools</p> </li> <li> <p>Create charts such as bar plots, histograms, and scatter plots without coding.</p> </li> <li> <p>Automatic Code Generation</p> </li> <li> <p>Any operation performed in the GUI generates equivalent Python code for reproducibility.</p> </li> <li> <p>Multiple Dataset Handling</p> </li> <li>Load and compare multiple datasets in a single session:    <pre><code>show(df1, df2, df3)\n</code></pre></li> </ol>"},{"location":"pandas/05_PandasGUI.html#mini-tutorial-analyzing-data-with-pandasgui","title":"Mini-Tutorial: Analyzing Data with PandasGUI","text":""},{"location":"pandas/05_PandasGUI.html#step-1-load-the-dataset","title":"Step 1: Load the Dataset","text":"<p>Assuming you have a CSV file <code>data.csv</code>, you can load it as follows: <pre><code>df = pd.read_csv(\"data.csv\")\nshow(df)\n</code></pre></p>"},{"location":"pandas/05_PandasGUI.html#step-2-visualize-the-data","title":"Step 2: Visualize the Data","text":"<p>Once the GUI opens: - Click on the Plots tab. - Select X-axis and Y-axis values. - Choose a plot type (e.g., scatter plot, histogram). - Click Generate Plot.</p>"},{"location":"pandas/05_PandasGUI.html#step-3-edit-data","title":"Step 3: Edit Data","text":"<ul> <li>Click on any cell to modify its value.</li> <li>Use the right-click menu to delete or duplicate rows.</li> </ul>"},{"location":"pandas/05_PandasGUI.html#step-4-export-the-changes","title":"Step 4: Export the Changes","text":"<p>Once you've modified the data, save it: <pre><code>df.to_csv(\"updated_data.csv\", index=False)\n</code></pre></p>"},{"location":"pandas/05_PandasGUI.html#conclusion","title":"Conclusion","text":"<p>PandasGUI is a powerful and intuitive tool for exploring and analyzing Pandas DataFrames with minimal coding effort. It is especially useful for data analysts who prefer a GUI-based approach to handling data.</p> <p>Would you like to see an example of how PandasGUI generates Python scripts based on GUI interactions? \ud83d\ude80</p>"},{"location":"pandas/09_DuckDB_and_MotherDuck.html","title":"09 DuckDB and MotherDuck","text":"<p>Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage Jesus L. Monroy \u00b7 Follow Published in T3CH \u00b7 4 min read \u00b7 2 days ago 61 Photo by Growtika on Unsplash Overview This project explores how to leverage the strengths of DuckDB and MotherDuck to build a robust data processing and storage solution. DuckDB excels at fast in-memory analytics, while MotherDuck provides a scalable and cost-effective cloud data warehouse. By combining these technologies, you can achieve optimal performance for both local and cloud-based data operations. Environment settings import polars as pl import duckdb as db import glob Extraction from data sources - csv files csv_files = glob.glob('./datasets/.csv') list(enumerate(csv_files)) - json files json_files = glob.glob('./datasets/.json') list(enumerate(json_files)) - database tables db_files = glob.glob('datasets/*.db') list(enumerate(db_files)) Data warehouse creation conn = db.connect('my_database.db') Data warehouse load conn.sql(f\"create or replace table water_collection as          select * from '{csv_files[0]}' \") conn.sql(f\"create or replace table contains_null as         select * from '{csv_files[1]}' \") conn.sql(f\"create or replace table sales_info as         select * from '{csv_files[2]}' \") conn.sql(f\"create or replace table cdmx_subway as         select * from '{csv_files[3]}' \") conn.sql(f\"create or replace table airports as         select * from '{csv_files[4]}' \") conn.sql(f\"create or replace table colors as         select * from '{csv_files[5]}' \") conn.sql(f\"create or replace table sets as         select * from '{csv_files[6]}' \") conn.sql(f\"create or replace table appl_stock as         select * from '{csv_files[7]}' \") conn.sql(f\"create or replace table sales as         select * from '{csv_files[8]}' \") conn.sql(f\"create or replace table prevalencia as         select * from '{json_files[0]}' \") conn.sql(f\"create or replace table people as         select * from '{json_files[1]}' \") retail = db.connect('./datasets/retail_db.db') retail.sql('show tables') retail_sales_pl = retail.sql('select * from retail_sales').pl() conn.execute(\"create or replace table retail_sales as from retail_sales_pl\"); restaurants = db.connect('./datasets/restaurants.db') restaurants.sql('show tables') restaurants_pl = restaurants.sql('select * from restaurants').pl() conn.execute(\"create or replace table restaurants as from restaurants_pl\"); Data retrieval conn.sql('show databases') conn.sql('show tables') conn.sql('select * from restaurants limit 5').pl() Cloud Data Warehouse with MotherDuck dw = db.connect('md') dw.sql('select current_database()').show() Convert queries from local database to polars airports_pl = conn.sql('select * from airports').pl() appl_stock_pl = conn.sql('select * from appl_stock').pl() cdmx_subway_pl = conn.sql('select * from cdmx_subway').pl() colors_pl = conn.sql('select * from colors').pl() contains_null_pl = conn.sql('select * from contains_null').pl() people_pl = conn.sql('select * from people').pl() prevalencia_pl = conn.sql('select * from prevalencia').pl() restaurants_pl = conn.sql('select * from restaurants').pl() retail_sales_pl = conn.sql('select * from retail_sales').pl() sales_pl = conn.sql('select * from sales').pl() sales_info_pl = conn.sql('select * from sales_info').pl() sets_pl = conn.sql('select * from sets').pl() water_collection_pl = conn.sql('select * from water_collection').pl() Upload dataframes to MotherDuck dw.sql(f\"create or replace table airports as select * from airports_pl\"); dw.sql(f\"create or replace table appl_stock as       select * from appl_stock_pl\"); dw.sql(f\"create or replace table cdmx_subway as       select * from cdmx_subway_pl\"); dw.sql(f\"create or replace table colors as select * from colors_pl\"); dw.sql(f\"create or replace table contains_null as       select * from contains_null_pl\"); dw.sql(f\"create or replace table people as select * from people_pl\"); dw.sql(f\"create or replace table prevalencia as       select * from prevalencia_pl\"); dw.sql(f\"create or replace table restaurants as       select * from restaurants_pl\"); dw.sql(f\"create or replace table retail_sales as       select * from retail_sales_pl\"); dw.sql(f\"create or replace table sales as       select * from sales_pl\"); dw.sql(f\"create or replace table sales_info as       select * from sales_info_pl\"); dw.sql(f\"create or replace table sets as       select * from sets_pl\"); dw.sql(f\"create or replace table water_collection as       select * from water_collection_pl\"); Check uploaded tables dw.sql('show tables') Close all database connections</p>"},{"location":"pandas/09_DuckDB_and_MotherDuck.html#close-db-connections","title":"close db connections","text":"<p>conn.close() retail.close() restaurants.close() dw.close() Conclusions By combining DuckDB\u2019s in-memory processing capabilities with MotherDuck\u2019s cloud-based data warehousing, you can create a powerful and flexible data processing and storage solution. This approach allows you to efficiently handle both local and cloud-based data operations, optimize performance, and scale your data infrastructure as needed. Open in app Search Write Contact Portfolio | Linkedin | Twitter Data Science Python Duckdb Motherduck Etl Published in T3CH Follow 605 Followers \u00b7 Last published\u00a022 hours ago Snoop &amp; Learn about Technology, AI, Hacking, Coding, Software, News, Tools, Leaks, Bug Bounty, OSINT &amp; Cybersecurity\u00a0!\u00a1! But, not limited 2, anything that is Tech Linked\u2026You\u2019ll probably find here\u00a0!\u00a0;) \u2014 Stay ahead with Latest Tech News! -&gt; You write about? Just ping to join\u00a0! Written by Jesus L. Monroy Follow 56 Followers \u00b7 23 Following Economist &amp; Data Scientist No responses yet What are your thoughts? Respond More from Jesus L. Monroy and T3CH In T3CH by Jesus L. Monroy In T3CH by Khaleel Khan Designing Elegant Tables with Unlock Hidden Secrets: How This Great Tables and Python Tool Reveals Everything About An\u2026 Great Tables Python is a powerful and Deep-HLR: An Essential Tool for Fraud versatile library that simplifies the process o\u2026 Prevention and OSINT Investigations Nov 1, 2024 216 Aug 23, 2024 453 14 In T3CH by TRedEye In T3CH by Jesus L. Monroy Advent of Cyber 2024 {All Tasks Building ETL Pipelines in BigQuery Update daily} \u2014 Tryhackme\u2026 with Python Advent of Cyber 2024 BY\u00a0::-&gt; TRedEye In the realm of data analytics, Extract, Transform, Load (ETL) processes play a\u2026 Dec 3, 2024 356 2 Oct 31, 2024 56 See all from Jesus L. Monroy See all from T3CH Recommended from Medium In Python in Plain English by Raphael Schols Tomer Gabay How to Turn PDF Documents into How to Setup Your Macbook for Data Tables with Python Data Science in 2025 Learn how to extract data from PDFs and Easy Steps to Get the Best Experience From structure it into tables Your MacBook as a Data Scientist Jan 13 180 2 Dec 28, 2024 174 1 Lists Predictive Modeling w/ Coding &amp; Development Python 11 stories \u00b7 981 saves 20 stories \u00b7 1789 saves Practical Guides to Machine ChatGPT prompts Learning 51 stories \u00b7 2487 saves 10 stories \u00b7 2165 saves Gen. David L. In Stackademic by Mayur Koshti Python ETL Framework Bonobo: Native Support for Advanced Data Efficiently Perform Data Extractio\u2026 Types in PostgreSQL Doing ETL (Extract, Transform, Load)? Use Practical Examples of Using PostgreSQL\u2019s Python\u2019s Bonobo library! It simplifies comple\u2026 Advanced Data Types Dec 31, 2024 24 1 Jan 13 75 1 Hugo Lu Abhilasha Gulhane \u2122 What dbt  Labs\u2019 acquisition of Data Engineer Topic: Shell SDF Labs means for the data\u2026 Scripting dbt Labs\u2122\u2019 acquisition of SDF Labs spells Here\u2019s an overview of shell scripting, trouble for other SQL-Development\u2026 important commands, examples, and sampl\u2026 6d ago 97 4 Jan 11 18 See more recommendations</p>"},{"location":"pandas/10_DataFrame_Tips.html","title":"10 DataFrame Tips","text":"<p>Member-only story 7 Pandas DataFrame Tricks I Wish I Knew in My Last Job Brent Fischer \u00b7 Follow Published in Python in Plain English \u00b7 4 min read \u00b7 Jan 2, 2025 5 Open in app Search Write Pandas for Experts Pandas is one of the most popular Python libraries for data manipulation and analysis, but it can be overwhelming when you\u2019re starting out or under time pressure at work. Learning these tricks would have saved me a lot of frustration and time in my last job, and I\u2019m here to share them with you so you can work smarter, not harder. 1. Use   and   for Faster Access to Single Values .at .iat Accessing or modifying individual values in a DataFrame is something you\u2019ll do often. While .loc and .iloc are the most common methods, they aren\u2019t always the fastest. For single-value access, .at (label-based) and .iat (integer position-based) are optimized for speed. This can make a big difference in large datasets. import pandas as pd df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})</p>"},{"location":"pandas/10_DataFrame_Tips.html#accessing-a-value","title":"Accessing a value","text":"<p>value = df.at[0, 'A']  # Faster than df.loc[0, 'A']</p>"},{"location":"pandas/10_DataFrame_Tips.html#modifying-a-value","title":"Modifying a value","text":"<p>df.iat[1, 1] = 10      # Faster than df.iloc[1, 1] = 10 In simple terms: Use .at when you know the column and row labels, and .iat when you\u2019re working with index positions. 2. Use   for Cleaner Filtering query Filtering rows with conditions is a daily task. While traditional boolean indexing works, it can get messy, especially with multiple conditions. Enter the query method, which lets you filter rows using SQL-like syntax for readability. df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})</p>"},{"location":"pandas/10_DataFrame_Tips.html#instead-of-this","title":"Instead of this","text":"<p>filtered = df[(df['A'] &gt; 1) &amp; (df['B'] &lt; 6)]</p>"},{"location":"pandas/10_DataFrame_Tips.html#use-query-for-cleaner-code","title":"Use query for cleaner code","text":"<p>filtered = df.query('A &gt; 1 &amp; B &lt; 6') When your filtering logic gets more complex, query makes your code much easier to read and maintain. 3. Vectorized Operations Instead of Loops Loops feel natural to use, but they\u2019re inefficient when working with Pandas. The library is built for vectorized operations, which are faster because they\u2019re executed in C under the hood. If you\u2019re using for loops or apply for simple column-wise calculations, you\u2019re doing it the hard way.</p>"},{"location":"pandas/10_DataFrame_Tips.html#slow-loop","title":"Slow loop","text":"<p>df['C'] = [a + b for a, b in zip(df['A'], df['B'])]</p>"},{"location":"pandas/10_DataFrame_Tips.html#fast-vectorized-operation","title":"Fast vectorized operation","text":"<p>df['C'] = df['A'] + df['B'] Not only is the vectorized approach faster, but it\u2019s also more readable. Think of your DataFrame as a single entity rather than individual rows and columns. 4. Use   to Chain Transformations .assign Data cleaning often involves multiple transformations. Instead of performing one operation, assigning it back to the DataFrame, and repeating, you can use .assign to chain them together. This makes your code more elegant and less error-prone. df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})</p>"},{"location":"pandas/10_DataFrame_Tips.html#add-or-modify-columns-in-one-step","title":"Add or modify columns in one step","text":"<p>df = df.assign(     D=df['A'] * 2,     E=lambda x: x['B'] + 3 ) Now you can add or modify multiple columns without interrupting the workflow. This is especially useful when chaining multiple operations together. 5. Replace   with   or   for Simpler Tasks apply .map .applymap If you\u2019re transforming a single column, use .map instead of apply. It\u2019s faster and more concise. For element-wise transformations across an entire DataFrame, use .applymap. Reserve apply for row-wise or column-wise operations.</p>"},{"location":"pandas/10_DataFrame_Tips.html#use-map-for-single-columns","title":"Use map for single columns","text":"<p>df['A'] = df['A'].map(lambda x: x * 2)</p>"},{"location":"pandas/10_DataFrame_Tips.html#use-applymap-for-element-wise-operations-across-the-dataframe","title":"Use applymap for element-wise operations across the DataFrame","text":"<p>df = df.applymap(lambda x: x * 2) This keeps your code focused and avoids unnecessary complexity. 6. Convert DataFrames to Dictionaries Efficiently When working with APIs or exporting data, you\u2019ll often need to convert a DataFrame to a dictionary. Instead of writing custom code, use Pandas\u2019 built- in to_dict method. Choosing the correct orient argument can save you a lot of headaches.</p>"},{"location":"pandas/10_DataFrame_Tips.html#convert-rows-to-a-list-of-dictionaries","title":"Convert rows to a list of dictionaries","text":"<p>data_dict = df.to_dict(orient='records') This method is perfect for JSON-style outputs, where each dictionary represents a row. It\u2019s a lifesaver when integrating with external systems. 7. Use   with Custom Aggregations .groupby The .groupby function is a powerhouse for summarizing data, but did you know you can combine it with .agg to perform multiple custom aggregations at once? This makes it easy to produce complex summaries in just a few lines. df = pd.DataFrame({'A': ['foo', 'foo', 'bar'], 'B': [1, 2, 3], 'C': [4, 5, 6]})</p>"},{"location":"pandas/10_DataFrame_Tips.html#group-by-column-a-and-compute-custom-aggregations","title":"Group by column 'A' and compute custom aggregations","text":"<p>result = df.groupby('A').agg(     sum_B=('B', 'sum'),    # Sum of column B     mean_C=('C', 'mean')   # Mean of column C ) Instead of chaining multiple .groupby and aggregation calls, this allows you to calculate everything at once, making your code concise and efficient. Thank you for being a part of the community Before you go: \ud83d\udc4f Be sure to clap and follow the writer Follow us: X | LinkedIn | YouTube | Newsletter | Podcast Check out CoFeed, the smart way to stay up-to-date with the latest in \ud83e\uddea tech  \ud83d\ude80 Start your own free AI-powered blog on Differ  \u0000\ud83d\udcbb Join our content creators community on Discord  For more content, visit plainenglish.io + stackademic.com Pandas Python Dataframes Tricks Published in Python in Plain English Follow 36K Followers \u00b7 Last published\u00a013 hours ago New Python content every day. Follow to join our 3.5M+ monthly readers. Written by Brent Fischer Follow 24 Followers \u00b7 12 Following Python Developer, Python Trainer, Geek, RPGs, Pizza, Traveller. Loves Rust, C, Linux. Drop by at friendlybytes.net No responses yet What are your thoughts? Respond More from Brent Fischer and Python in Plain English In Level Up Coding by Brent Fischer In Python in Plain English by Kiran Maan Linux for Pythonistas: Advanced Can Mojo Really Replace Python? File Monitoring and Automation\u2026 Let\u2019s Test It In Linux, inotify (inotify stands for \"inode A few days ago, I got to know about Mojo on notify\") is a powerful kernel subsystem that\u2026 Medium. I just made a pause and read: \u201cMojo\u2026 Dec 12, 2024 95 Dec 19, 2024 697 14 In Python in Plain English by Afsalkh In Python in Plain English by Brent Fischer Why Does round(6.5) Return 6 Python Advanced: The Beginner\u2019s While round(7.5) Returns 8 in\u2026 Guide to Ruff Pythonistas, Can You Explain This Rounding As Python projects grow in size and Oddity? complexity, maintaining clean and consisten\u2026 Dec 3, 2024 697 13 Dec 9, 2024 20 See all from Brent Fischer See all from Python in Plain English Recommended from Medium In Dev Genius by Aleksei Aleinikov De Os 10 Ways to Work with Large Files in Pandas Read Excel 18 Times Faster Python: Effortlessly Handle\u2026 Yesterday, I had to process around 750 Excel Handling large text files in Python can feel files with approximately 165,000 rows each.\u2026 overwhelming. When files grow into\u2026 Dec 1, 2024 304 3 Jul 11, 2024 164 1 Lists Coding &amp; Development Predictive Modeling w/ Python 11 stories \u00b7 964 saves 20 stories \u00b7 1765 saves Practical Guides to Machine ChatGPT Learning 21 stories \u00b7 938 saves 10 stories \u00b7 2138 saves Varun Singh Lorenzo Uriel Python 3.14 Released \u2014 Top 5 Starting a Python Project Features You Must Know This guide documents the steps I take to set Faster Annotations &amp; Mind-Blowing Updates up the foundation of any Python project, wit\u2026 You NEED to Know! Dec 31, 2024 62 Nov 19, 2024 200 1 In Towards Data Engineering by Sandun Lakshan In Towards Data Science by Vladimir Zhyvov 10 Best Python Text User Interface From Default Python Line Chart to (TUI) Libraries for 2025 Journal-Quality Infographics Text-based user interfaces (TUIs) are a simple Transform boring default Matplotlib line way to create interactive applications that ru\u2026 charts into stunning, customized\u2026 Dec 10, 2024 36 1 Dec 30, 2024 1.1K 18 See more recommendations</p>"},{"location":"pandas/11_Dataframe_Examples.html","title":"11 Dataframe Examples","text":"<p>Member-only story Python by Examples: Mastering Pandas DataFrames (1 of 2) MB20261 \u00b7 Follow 12 min read \u00b7 3 days ago Data analysis is a crucial component of modern decision-making, and Pandas DataFrames serve as an invaluable tool in this domain. They provide a versatile structure that allows for easy manipulation and analysis of data, making them a staple in the data scientist\u2019s toolkit. With similarities to spreadsheets, DataFrames offer a familiar yet powerful interface, enabling users to handle everything from small datasets to massive databases with efficiency and ease. The ability to perform various operations \u2014 ranging from simple data selection to complex reshaping \u2014 on DataFrames makes them an essential part of the data analysis workflow. Understanding the intricacies of DataFrames is not just beneficial; it\u2019s necessary to unlock their full potential. In this article, we\u2019ll delve into the different features and operations that Pandas DataFrames offer. Through step-by-step examples, we\u2019ll guide you from basic operations like indexing and selection to advanced topics such as reshaping and time-series manipulation. Whether you\u2019re a beginner looking to understand the fundamentals or an experienced user aiming to refine your skills, this guide will serve as a comprehensive resource in mastering Pandas DataFrames. Introduction to DataFrames DataFrames are at the heart of the Pandas library, offering a powerful and flexible way to manage tabular data. They are two-dimensional structures, similar to spreadsheets, allowing for size-mutable and potentially heterogeneous data storage with labeled axes for rows and columns. This makes them ideal for handling structured data such as CSV files or SQL query results, providing a familiar data structure that supports a wide range of operations and analysis. What is a DataFrame? Understanding the structure of a DataFrame is crucial for efficient data manipulation. A DataFrame comprises rows and columns, where each row can be accessed through either a label or a positional index. Columns can store diverse data types, including integers, floats, and strings, making them suitable for real-world applications where data is rarely homogeneous. DataFrames enable users to organize, manipulate, and analyze datasets effectively. By assigning labels to rows and columns, they provide a user- friendly interface to manage complex data operations seamlessly. The ability to handle various data types further enhances their utility, making them an indispensable tool in data analysis workflows. When to Use: DataFrames should be your go-to structure for any complex, structured data manipulation. They excel when working with large datasets that require operations such as filtering, grouping, and aggregating, as well as whenever you need to import and manipulate data from external sources like CSV files or databases. Their flexibility in dealing with mixed data types makes them ideal for data-centric tasks in both exploratory and production environments. When Not to Use: While DataFrames are powerful, they also come with overhead. If your data is simple or small-scale, and performance is a critical factor, simpler data structures like lists or dictionaries may suffice. DataFrames can add unnecessary complexity if the operations required are minimal or the data size is trivial. Code Examples To illustrate, consider a basic example where we create a DataFrame to store names, ages, and cities for a small set of individuals. This simple yet powerful structure mimics a traditional table and is easy to understand for anyone familiar with spreadsheets. import pandas as pd df = pd.DataFrame({     \"Name\": [\"Alice\", \"Bob\"],     \"Age\": [25, 30],     \"City\": [\"New York\", \"Los Angeles\"] }) print(df)</p>"},{"location":"pandas/11_Dataframe_Examples.html#a-simple-table-with-name-age-and-city-columns","title":"A simple table with Name, Age, and City columns.","text":"<p>Alternatively, you can build a DataFrame from a list of dictionaries. Each dictionary represents a record, allowing for a varied set of attributes per entry. This method showcases how DataFrames can seamlessly integrate data that originates from Python data structures. data = [     {\"Name\": \"Charlie\", \"Age\": 35, \"City\": \"Chicago\"},     {\"Name\": \"David\", \"Age\": 40, \"City\": \"Miami\"} ] df = pd.DataFrame(data) print(df)</p>"},{"location":"pandas/11_Dataframe_Examples.html#converts-a-list-of-dictionaries-to-a-dataframe","title":"Converts a list of dictionaries to a DataFrame.","text":"<p>For custom indexing, pandas allows the assignment of unique labels to rows, diverging from the default numerical index. This feature is particularly useful when working with large datasets where specific identifiers add clarity. df = pd.DataFrame({     \"Name\": [\"Eve\", \"Frank\"],     \"Age\": [28, 34],     \"City\": [\"Boston\", \"Seattle\"] }, index=[\"Row1\", \"Row2\"]) print(df)</p>"},{"location":"pandas/11_Dataframe_Examples.html#uses-custom-labels-for-rows-instead-of-default-numeric-index","title":"Uses custom labels for rows instead of default numeric index.","text":"<p>Creating DataFrames Creating DataFrames can be accomplished using various data formats, highlighting the versatility of the Pandas library. You can initialize DataFrames from lists, dictionaries, or even directly from CSV files. This flexibility allows Pandas to interface with multiple data sources, making it a valuable tool in data preparation and analysis processes. When to Use: Leverage DataFrame creation methods when importing data from external sources. Whether you are ingesting data from a CSV file, converting lists or dictionaries into structured tables, or pulling data directly from databases, DataFrames provide a coherent interface for data integration. They are particularly beneficial in scenarios requiring data cleansing and transformation before analysis. When Not to Use: Avoid converting simple or minimalistic data structures into DataFrames if they add unnecessary complexity without substantial gains in functionality or performance. If the operation is straightforward, like basic iteration, simpler data structures might be more efficient. Code Examples Creating a DataFrame from lists is one of the simplest methods, and it involves structuring data as a collection of records and specifying corresponding column names. df_list = pd.DataFrame([     [\"Grace\", 29],     [\"Henry\", 32] ], columns=[\"Name\", \"Age\"]) print(df_list)</p>"},{"location":"pandas/11_Dataframe_Examples.html#uses-separate-lists-for-each-row-and-specifies-column-labels","title":"Uses separate lists for each row and specifies column labels.","text":"<p>When working with data in dictionary format, you can directly translate this into a DataFrame where keys become column names and values become column data. This approach is particularly intuitive for users transitioning from Python native data types to Pandas structures. data_dict = {\"Name\": [\"Ivy\", \"Jack\"], \"Age\": [24, 36]} df_dict = pd.DataFrame(data_dict) print(df_dict)</p>"},{"location":"pandas/11_Dataframe_Examples.html#directly-maps-keys-to-column-names-and-values-to-column-data","title":"Directly maps keys to column names and values to column data.","text":"<p>Importing data from CSV files into DataFrames is commonplace, enabling the transition from file-based storage to in-memory data manipulation efficiently. This method is straightforward and essential for handling large datasets typically stored in CSV format. df_csv = pd.read_csv(\"data.csv\") print(df_csv)</p>"},{"location":"pandas/11_Dataframe_Examples.html#reads-data-from-a-csv-file-into-a-dataframe-using-the-pandas-read_csv-function","title":"Reads data from a CSV file into a DataFrame using the Pandas read_csv function","text":"<p>In summary, mastering the construction and understanding of DataFrames lays the groundwork for advanced data manipulation tasks in Pandas. Their flexibility in data storage, indexing, and manipulation paves the way for efficient data analysis and processing workflows. Basic Operations Performing basic operations is the first step in manipulating DataFrames. These operations provide essential tools to access, modify, and manage data within your DataFrame efficiently. Whether you are analyzing trends, preparing data for modeling, or conducting exploratory data analysis, mastering these basic operations will significantly enhance your data handling capabilities in Pandas. Selecting and Indexing Selecting and indexing are fundamental techniques in Pandas that allow you to retrieve specific data from your DataFrame based on various criteria or conditions. This process is vital for analyzing and manipulating the desired subsets of your data. Whether you need to access an individual row, column, or a more complex slice of the data, selecting and indexing lay the groundwork for more advanced data manipulation tasks. When using selecting and indexing, it\u2019s essential to understand the three primary methods: <code>.loc</code>, <code>.iloc</code>, and boolean indexing. Each of these methods provides a different approach to accessing data. The <code>.loc</code> method is label-based, meaning it retrieves data using labels rather than positional indexes. This is particularly useful when your DataFrame has a meaningful index or column labels. In contrast, <code>.iloc</code> is position-based and allows you to access data using numerical positions, similar to how you might use indexing in Python lists. Boolean indexing, on the other hand, filters data based on conditions you specify, enabling you to create subsets of your data that meet specific criteria. These methods are pivotal in scenarios where you want to perform analyses on particular data segments. You can use <code>.loc</code> to select rows or columns by specifying their labels. For example, if you want to select a row where the label is \u2018Alice\u2019, or access a specific cell, you would use <code>.loc</code>. Meanwhile, <code>.iloc</code> is advantageous when working with datasets where you need to select data based on its position, such as retrieving the first few rows for an initial exploration. Boolean indexing shines when you\u2019re dealing with large datasets, allowing you to filter results dynamically based on ever-changing conditions. Here\u2019s a demonstration of these concepts with practical examples: import pandas as pd Open in app</p>"},{"location":"pandas/11_Dataframe_Examples.html#create-a-dataframe","title":"Create a DataFrame","text":"<p>Search Write df = pd.DataFrame({     \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],     \"Age\": [25, 30, 35],     \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"] })</p>"},{"location":"pandas/11_Dataframe_Examples.html#use-loc-to-select-by-label","title":"Use .loc to select by label","text":"<p>first_row = df.loc[0]  # Select the first row specific_cell = df.loc[1, 'Age']  # Access a specific cell print(\"First Row:\\n\", first_row) print(\"Specific Cell (Row 1, Age):\", specific_cell)</p>"},{"location":"pandas/11_Dataframe_Examples.html#use-iloc-to-select-by-position","title":"Use .iloc to select by position","text":"<p>first_column = df.iloc[:, 0]  # Select the first column specific_rows = df.iloc[0:2]  # Retrieve the first two rows print(\"First Column:\\n\", first_column) print(\"Specific Rows:\\n\", specific_rows)</p>"},{"location":"pandas/11_Dataframe_Examples.html#apply-boolean-indexing-to-filter-data","title":"Apply boolean indexing to filter data","text":"<p>adults = df[df['Age'] &gt; 25]  # Filter rows where age is greater than 25 los_angeles_residents = df[df['City'] == \"Los Angeles\"] print(\"Adults:\\n\", adults) print(\"Los Angeles Residents:\\n\", los_angeles_residents) In these examples, you\u2019ll see the versatility of each method for different tasks. Using <code>.loc</code>, we can access the first row and a specific cell by label, providing clarity when referencing data. <code>.iloc</code> showcases selection by position, facilitating highly-efficient data slicing. Boolean indexing, meanwhile, filters the DataFrame to return only rows that meet specified conditions, such as ages above 25 or residents of Los Angeles. Modifying DataFrames Modifying DataFrames enables you to add, remove, or alter columns and rows within your dataset, which is crucial for updating your data with new information or tailoring it for specific analyses. These operations ensure that your DataFrame accurately represents the current state of your data and that it\u2019s ready for any subsequent processing or analysis. Adding new columns to a DataFrame allows you to enrich your data with additional information, which can be critical when calculating new metrics or integrating data from different sources. Conversely, removing columns or rows can simplify your datasets, eliminating unnecessary or redundant information. This cleansing helps focus analysis on relevant data, improving your model accuracy and efficiency. Updating specific cell values offers flexibility in correcting errors or integrating newly derived data points. For data scientific workflows, quick updates and batch processing are common as they allow you to handle ever- evolving datasets without the need for extensive rewrites. Consider these examples to see some common modifications in action:</p>"},{"location":"pandas/11_Dataframe_Examples.html#add-a-new-column-height-to-the-dataframe","title":"Add a new column 'Height' to the DataFrame","text":"<p>df['Height'] = [165, 180, 175]  # Heights corresponding to each person print(\"DataFrame with Height:\\n\", df)</p>"},{"location":"pandas/11_Dataframe_Examples.html#remove-the-city-column-from-the-dataframe","title":"Remove the 'City' column from the DataFrame","text":"<p>df = df.drop('City', axis=1) print(\"DataFrame without City:\\n\", df)</p>"},{"location":"pandas/11_Dataframe_Examples.html#updating-specific-cell-values-in-the-dataframe","title":"Updating specific cell values in the DataFrame","text":"<p>df.loc[0, 'Age'] = 26 df.iloc[2, 1] = 36  # Update Charlie's age using integer position print(\"Updated DataFrame:\\n\", df) In the first example, a <code>Height</code> column is added to the DataFrame, providing additional descriptive data. The second example uses the <code>drop</code> method to effectively streamline the DataFrame by removing the <code>City</code> column. Finally, the third example shows how to update specific cells, illustrating the use of both <code>.loc</code> and <code>.iloc</code> for precise modifications. These fundamental operations are crucial for effective data manipulation in Pandas. Whether performing initial data exploration or preparing data for detailed analyses, mastering selecting, indexing, and modification techniques will empower you to handle your datasets with greater agility and precision. Data Cleaning Data cleaning is a crucial step in the data analysis process. The presence of inconsistencies, errors, or incorrect formats can severely affect the quality of insights derived from data. By cleaning data, we aim to correct or remove errors and ensure that the dataset is in a usable format for analysis. This process enhances the accuracy and reliability of any conclusions drawn. Effective data cleaning involves tackling missing values and standardizing data types, both of which are fundamental to preparing data for any kind of computational analysis. Handling missing data is often one of the first challenges encountered in data cleaning. Missing data can appear as <code>NaN</code> (Not a Number) values and arise due to various reasons, such as incomplete data entry or faulty data collection processes. Dealing with these missing values effectively is crucial because they can skew results and negatively impact analysis outcomes. The primary goal is to manage these <code>NaN</code> values by either filling them with meaningful replacements or removing them from the dataset altogether. The <code>fillna()</code> method in Pandas allows us to replace NaN values with specific values, such as a column\u2019s mean or median, or even a custom value that aligns with the dataset\u2019s context. This method is particularly useful when retaining data entries is important, and a sensible estimate can fill the gaps. Meanwhile, the <code>dropna()</code> method is used when the missing values are insignificant, allowing us to remove any rows with NaN values without affecting the overall analysis significantly. It is important to consider the context and potential implications of filling or dropping data to avoid introducing bias or removing critical information inadvertently. Example 1: Filling NaN with Mean import pandas as pd import numpy as np</p>"},{"location":"pandas/11_Dataframe_Examples.html#sample-dataframe","title":"Sample DataFrame","text":"<p>data = {'Name': ['Alice', 'Bob', 'Charlie'],         'Age': [25, np.nan, 30]} df = pd.DataFrame(data)</p>"},{"location":"pandas/11_Dataframe_Examples.html#fill-nan-values-in-age-with-the-mean-of-the-column","title":"Fill NaN values in 'Age' with the mean of the column","text":"<p>df['Age'] = df['Age'].fillna(df['Age'].mean()) print(df)</p>"},{"location":"pandas/11_Dataframe_Examples.html#this-example-demonstrates-replacing-nan-in-the-age-column-with-its-mean-mai","title":"This example demonstrates replacing NaN in the 'Age' column with its mean, mai","text":"<p>In the example above, we see how the <code>fillna()</code> method can be utilized to fill missing values with the mean of a column, preserving the dataset\u2019s structure and integrity. This approach retains all records while providing a logically consistent value where data is missing. Example 2: Removing Rows with NaN import pandas as pd import numpy as np</p>"},{"location":"pandas/11_Dataframe_Examples.html#sample-dataframe_1","title":"Sample DataFrame","text":"<p>data = {'Name': ['Alice', 'Bob', 'Charlie'],         'Age': [25, np.nan, 30],         'City': [np.nan, 'New York', 'Los Angeles']} df = pd.DataFrame(data)</p>"},{"location":"pandas/11_Dataframe_Examples.html#drop-rows-where-any-element-is-nan","title":"Drop rows where any element is NaN","text":"<p>df_clean = df.dropna() print(df_clean)</p>"},{"location":"pandas/11_Dataframe_Examples.html#this-example-removes-rows-with-any-nan-values-which-can-simplify-downstream-a","title":"This example removes rows with any NaN values, which can simplify downstream a","text":"<p>In this example, the <code>dropna()</code> method is used to remove rows with any missing data. This approach is useful when the dataset has enough complete entries, and missing values are not critical for analysis. Example 3: Filling NaN with Forward Fill Method import pandas as pd import numpy as np</p>"},{"location":"pandas/11_Dataframe_Examples.html#sample-dataframe_2","title":"Sample DataFrame","text":"<p>data = {'Name': ['Alice', 'Bob', 'Charlie'],         'Age': [25, np.nan, np.nan],         'City': ['New York', np.nan, 'Los Angeles']} df = pd.DataFrame(data)</p>"},{"location":"pandas/11_Dataframe_Examples.html#forward-fill-nan-values-in-the-dataframe","title":"Forward fill NaN values in the DataFrame","text":"<p>df.ffill(inplace=True) print(df)</p>"},{"location":"pandas/11_Dataframe_Examples.html#forward-fill-uses-the-preceding-valid-observation-to-replace-nans-suitable-fo","title":"Forward fill uses the preceding valid observation to replace NaNs, suitable fo","text":"<p>Here, the forward fill method is employed, which carries forward the last valid observation to fill NaN values. It is particularly handy in time-series data where such sequential filling makes logical sense. Data type conversion is another integral part of data cleaning. The consistency of data types across a DataFrame is essential for effective data manipulation and analysis. Processes such as arithmetic operations, data comparisons, and the application of functions often require data to be in specific formats, like integers, floats, or strings. The <code>astype()</code> method in Pandas is a versatile tool for converting data types to ensure uniformity and compatibility. This is particularly useful when standardizing a dataset to prepare it for machine learning models, where specific data types may be required. Example 4: Converting String to Integer import pandas as pd</p>"},{"location":"pandas/11_Dataframe_Examples.html#sample-dataframe_3","title":"Sample DataFrame","text":"<p>data = {'ID': ['101', '102', '103'],         'Amount': ['1000', '1500', '1200']} df = pd.DataFrame(data)</p>"},{"location":"pandas/11_Dataframe_Examples.html#convert-amount-from-string-to-integer","title":"Convert 'Amount' from string to integer","text":"<p>df['Amount'] = df['Amount'].astype(int) print(df.dtypes)</p>"},{"location":"pandas/11_Dataframe_Examples.html#string-amount-values-are-converted-to-integers-facilitating-numerical-opera","title":"String 'Amount' values are converted to integers, facilitating numerical opera","text":"<p>This example showcases converting a string column to an integer column, allowing for numerical operations and analyses that require numeric data types. Example 5: Converting Integer to Float import pandas as pd</p>"},{"location":"pandas/11_Dataframe_Examples.html#sample-dataframe_4","title":"Sample DataFrame","text":"<p>data = {'Product': ['A', 'B', 'C'],         'Price': [200, 150, 300]} df = pd.DataFrame(data)</p>"},{"location":"pandas/11_Dataframe_Examples.html#convert-price-from-int-to-float","title":"Convert 'Price' from int to float","text":"<p>df['Price'] = df['Price'].astype(float) print(df.dtypes)</p>"},{"location":"pandas/11_Dataframe_Examples.html#converting-integers-to-floats-when-precision-is-needed-for-further-calculation","title":"Converting integers to floats when precision is needed for further calculation","text":"<p>Here, integer values are converted to floats, increasing precision where decimal values are necessary, such as in financial calculations. Example 6: Handling Conversion Errors import pandas as pd</p>"},{"location":"pandas/11_Dataframe_Examples.html#sample-dataframe_5","title":"Sample DataFrame","text":"<p>data = {'Year': ['2020', 'NaN', '2021'],         'Value': ['100', '200.5', '300']} df = pd.DataFrame(data) try:     # Attempt to convert 'Year' to int     df['Year'] = df['Year'].astype(int) except ValueError:     df['Year'] = pd.to_numeric(df['Year'], errors='coerce') print(df)</p>"},{"location":"pandas/11_Dataframe_Examples.html#intelligent-handling-of-conversion-errors-using-coerce-ensuring-erroneous-c","title":"Intelligent handling of conversion errors using 'coerce', ensuring erroneous c","text":"<p>Here, we address potential errors during conversion by coercing problematic conversions to NaN, illustrating a proactive approach to maintaining data integrity. In conclusion, handling missing data and performing data type conversions are fundamental steps in the data cleaning process that ensure data integrity and reliability. Mastering these techniques is critical for producing trustworthy and actionable insights from data. What\u2019s Next? In the next article, we will delve into reshaping and advanced manipulations of DataFrames. You\u2019ll learn how to restructure your data using powerful Pandas functions like concatenation, merging, stacking, and melting. These techniques allow for more flexible data analysis beyond the basics. Additionally, we will explore advanced manipulations, including working with MultiIndex structures and handling time series data. These skills will enable you to tackle complex analytical tasks with ease, enhancing your data transformation capabilities. Python by Examples: Mastering Pandas DataFrames (2 of 2) Data analysis is a crucial component of modern decision-making, and Pandas DataFrames serve as an invaluable tool in\u2026 medium.com Pandas Python Python Programming Data Analysis Data Science Written by MB20261 Follow 106 Followers \u00b7 3 Following Digital Transformation | FinOps | DevOps | AI | Software Architecture/Solutions | Microservices | Data Lake | Kubernetes | Python | SpringBoot | Certifications No responses yet What are your thoughts? Respond More from MB20261 MB20261 MB20261 Python by Examples: Extract PDF NLP By Examples \u2014 Text by PDFMiner.six Classifications with Transformers A PDF (Portable Document Format) file is a In today\u2019s digital landscape, Natural Language flexible file format created by Adobe that\u2026 Processing (NLP) plays a vital role in shapin\u2026 May 15, 2024 3 Oct 18, 2024 1 MB20261 MB20261 Ubuntu 22.04 (Jammy Jellyfish) Python by Examples: Abstract Minimal Server Installation Base Classes and Interfaces After Ubuntu 18.04, the minimal ISO file is no Abstract Base Classes (ABCs) and interfaces longer supported. Below shows the minimal\u2026 in Python provide a way to define a common\u2026 Feb 5, 2023 1 Aug 22, 2024 See all from MB20261 Recommended from Medium MB20261 fg-research Python by Examples: Mastering ECG anomaly detection with the Pandas Series for Data Analysis (1\u2026 LSTM-AD SageMaker algorithm In the world of data science, the Pandas Detecting anomalies in electrocardiogram library in Python stands out as a superhero\u2026 (ECG) signals is critical for the diagnosis and\u2026 4d ago Nov 21, 2024 Lists Predictive Modeling w/ Coding &amp; Development Python 11 stories \u00b7 983 saves 20 stories \u00b7 1792 saves Practical Guides to Machine ChatGPT prompts Learning 51 stories \u00b7 2498 saves 10 stories \u00b7 2171 saves In The Pythoneers by Bhargav Sridhar Kyle Jones Handling XML data using Python State Space Models and Kalman Filtering for Time Series Analysis XML data management in Python Techniques for understanding the hidden states of time series data 4d ago 6d ago 164 3 In Python in Plain English by CyCoderX In Towards Dev by Ben Hui Visualizing Data in Terminal with 5 Cool Jupyter Notebook Tips Python Bashplotlib Jupyter Notebook is one of the most popular Explore how to create data visualizations integrated development environments (IDEs\u2026 directly in your terminal with Python\u2026 Jan 15 104 1 6d ago 57 See more recommendations</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html","title":"12 Data Exploration (1)","text":"<p>Must-Know Python Pandas Functions for Effortless Data Exploration Gen. David L. \u00b7 Follow 5 min read \u00b7 Nov 27, 2024 19 Open in app Search Write The key point of data analysis lies in uncovering the stories behind the data. To achieve this, it\u2019s essential to build a solid foundation by thoroughly exploring and understanding the data. In this process, Python\u2019s Pandas library plays a crucial role. Pandas not only offers a powerful set of features but also provides high flexibility, making data exploration both easy and efficient. In this post we will introduce several must-know Pandas methods for effective data exploration. Create a CSV sample dataset To better illustrate and demonstrate how to use Pandas functions, let\u2019s first create a sample CSV dataset. import random import csv from faker import Faker</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#initialize-faker","title":"Initialize Faker","text":"<p>fake = Faker()</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#list-of-products-and-their-categories","title":"List of products and their categories","text":"<p>products = [     {\"name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 899.99},     {\"name\": \"Smartphone\", \"category\": \"Electronics\", \"price\": 699.99},     {\"name\": \"Headphones\", \"category\": \"Accessories\", \"price\": 49.99},     {\"name\": \"Coffee Maker\", \"category\": \"Home Appliances\", \"price\": 79.99},     {\"name\": \"Sneakers\", \"category\": \"Fashion\", \"price\": 59.99},     {\"name\": \"Backpack\", \"category\": \"Fashion\", \"price\": 39.99},     {\"name\": \"Blender\", \"category\": \"Home Appliances\", \"price\": 99.99},     {\"name\": \"Desk Chair\", \"category\": \"Furniture\", \"price\": 129.99},     {\"name\": \"Water Bottle\", \"category\": \"Accessories\", \"price\": 19.99},     {\"name\": \"Notebook\", \"category\": \"Stationery\", \"price\": 5.99}, ]</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#define-a-function-to-generate-order-data","title":"Define a function to generate order data","text":"<p>def generate_order_data(num_rows):     data = []     for _ in range(num_rows):         product = random.choice(products)         quantity = random.randint(1, 10)         total = round(product[\"price\"] * quantity, 2)         order = {             \"Order ID\": fake.uuid4(),             \"Customer Name\": fake.name(),             \"Customer Email\": fake.email(),             \"Product Name\": product[\"name\"],             \"Category\": product[\"category\"],             \"Quantity\": quantity,             \"Price\": product[\"price\"],             \"Total\": total,             \"Order Date\": fake.date_this_year(),             \"Shipping Address\": fake.address(),         }         data.append(order)     return data</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#generate-1000-rows-of-data","title":"Generate 1000 rows of data","text":"<p>num_rows = 1000 order_data = generate_order_data(num_rows)</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#save-the-data-to-a-csv-file","title":"Save the data to a CSV file","text":"<p>output_file = \"sample_orders.csv\" with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:     writer = csv.DictWriter(file, fieldnames=order_data[0].keys())     writer.writeheader()     writer.writerows(order_data) print(f\"Sample dataset with {num_rows} rows has been saved to '{output_file}'.\") This code ensures that the sample dataset is saved as a structured CSV file (sample_orders.csv) for further data analysis by pandas functions. Head function head() head(): Used to preview the top rows of the sample dataset. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#display-the-first-10-rows-of-the-dataset","title":"Display the first 10 rows of the dataset","text":"<p>print(df.head(10)) Tail function tail() tail(): Used to preview the bottom rows of the sample dataset. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_1","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#display-the-last-10-rows-of-the-dataset","title":"Display the last 10 rows of the dataset","text":"<p>print(df.tail(10)) Sample function sample() sample(): This function is highly valuable when working with large datasets. When we need to extract and analyze a smaller subset from a larger DataFrame, <code>sample()</code> helps efficiently retrieve random samples, enabling preliminary data exploration or performance evaluation. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_2","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-and-display-the-random-10-rows-from-the-dataset","title":"Read and display the random 10 rows from the dataset","text":"<p>print(df.sample(10)) Information function info() info(): This function provides a summary of the dataset, including the number of entries, column names, data types, and memory usage. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_3","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#display-a-summary-of-the-dataset","title":"Display a summary of the dataset","text":"<p>print(df.info()) Describe function describe() describe(): This function provides basic statistical information about the dataset, such as mean, standard deviation, minimum and maximum values, and quartiles. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_4","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#display-the-basic-statistical-information-about-the-dataset","title":"Display the basic statistical information about the dataset","text":"<p>print(df.describe()) Value counts function value_counts() value_counts(): This method returns the count of all unique values in a column or a pandas Series. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_5","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#display-the-count-of-all-unique-values-in-a-columnsuch-as-category","title":"Display the count of all unique values in a column,such as \"Category\"","text":"<p>print(df[\"Category\"].value_counts()) Shape attribute shape: This attribute returns the number of rows and columns in the dataset. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_6","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#display-the-number-of-rows-and-columns-in-the-dataset","title":"Display the number of rows and columns in the dataset","text":"<p>print(df.shape) Dataframe dtypes attribute df.dtypes: This attribute returns the data types of all columns. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_7","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#display-the-data-types-of-all-columns","title":"Display the data types of all columns","text":"<p>print(df.dtypes) Unique function unique() unique(): This method returns all unique values in a column or a pandas Series. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_8","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#display-all-unique-values-in-a-column","title":"Display all unique values in a column.","text":"<p>print(df[\"Category\"].unique()) Nunique function nunique() nunique(): This function returns the number of unique values in a DataFrame. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_9","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration%20%281%29.html#display-the-count-of-unique-values-in-the-dataset-sorted-in-descending-order","title":"Display the count of unique values in the dataset, sorted in descending order","text":"<p>df.nunique().sort_values(ascending=False) In this post we provide a detailed introduction to essential core functions in the Pandas library, which are crucial for data analysis. These functions make data exploration more straightforward and efficient, helping analysts quickly grasp the structure and characteristics of a dataset. With these must-know functions, data analysts can gain deeper insights into the data, uncover the stories behind it, and provide data-driven support for decision-making. These Pandas functions are an indispensable part of the data analysis workflow, enhancing both the efficiency and the depth of analysis. Thanks for your reading. Python Pandas Pandas Tutorial Pandas Data Preprocessing Python Pandas Dataframe Written by Gen. David L. Follow 453 Followers \u00b7 4 Following AI practitioner &amp; python coder to record what I learned in python project development No responses yet What are your thoughts? Respond More from Gen. David L. Gen. David L. Gen. David L. Advanced Pandas Features: ETL-PIPES: An Efficient Python ETL Enhance Your Data Processing\u2026 Data Processing Library The essence of data analysis lies in ETL-pipes is a powerful and flexible Python uncovering the stories behind the data. In thi\u2026 library specifically designed for ETL (Extract\u2026 Dec 5, 2024 105 Nov 30, 2024 45 6 Gen. David L. Gen. David L. How to Map Column Values in a Four Ways to Package a Python Pandas DataFrame? Project into an executable EXE\u2026 Mapping column values refers to replacing In Python, packaging a project into an specific values in a column with other values,\u2026 executable EXE file is a common task,\u2026 Dec 24, 2024 81 Sep 14, 2024 94 See all from Gen. David L. Recommended from Medium Anita Gupta Harold Finch Statistics Handbook for Data Top 25 Python Scripts To Automate Analysts Your Daily Tasks Why This Handbook? Python is an excellent tool for automating daily tasks, thanks to its simplicity and a wid\u2026 Sep 14, 2024 422 11 Nov 19, 2024 337 5 Lists Staff picks Stories to Help You Level-Up at Work 798 stories \u00b7 1568 saves 19 stories \u00b7 917 saves Self-Improvement 101 Productivity 101 20 stories \u00b7 3214 saves 20 stories \u00b7 2716 saves In The Pythoneers by Kevin Meneses Gonz\u00e1lez In Python in Plain English by Satyam Sahu 5 Ways to Make Money with Python How I Automated Data Cleaning in in 2025 Python Using Functions and\u2026 The future belongs to those who believe in the Discover the key Python techniques that beauty of their dreams.\u201d \u2014 Eleanor Roosevelt transformed my data-cleaning workflow fro\u2026 Dec 8, 2024 282 10 Nov 4, 2024 216 7 Gen. David L. Lorenzo Uriel Starting a Python Project How to Split Columns in a This guide documents the steps I take to set DataFrame Using Python Pandas up the foundation of any Python project, wit\u2026 Splitting columns is a common data manipulation operation in Pandas. It allows u\u2026 Nov 19, 2024 208 1 Dec 21, 2024 62 See more recommendations</p>"},{"location":"pandas/12_Data%20Exploration.html","title":"12 Data Exploration","text":"<p>Must-Know Python Pandas Functions for Effortless Data Exploration Gen. David L. \u00b7 Follow 5 min read \u00b7 Nov 27, 2024 19 Open in app Search Write The key point of data analysis lies in uncovering the stories behind the data. To achieve this, it\u2019s essential to build a solid foundation by thoroughly exploring and understanding the data. In this process, Python\u2019s Pandas library plays a crucial role. Pandas not only offers a powerful set of features but also provides high flexibility, making data exploration both easy and efficient. In this post we will introduce several must-know Pandas methods for effective data exploration. Create a CSV sample dataset To better illustrate and demonstrate how to use Pandas functions, let\u2019s first create a sample CSV dataset. import random import csv from faker import Faker</p>"},{"location":"pandas/12_Data%20Exploration.html#initialize-faker","title":"Initialize Faker","text":"<p>fake = Faker()</p>"},{"location":"pandas/12_Data%20Exploration.html#list-of-products-and-their-categories","title":"List of products and their categories","text":"<p>products = [     {\"name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 899.99},     {\"name\": \"Smartphone\", \"category\": \"Electronics\", \"price\": 699.99},     {\"name\": \"Headphones\", \"category\": \"Accessories\", \"price\": 49.99},     {\"name\": \"Coffee Maker\", \"category\": \"Home Appliances\", \"price\": 79.99},     {\"name\": \"Sneakers\", \"category\": \"Fashion\", \"price\": 59.99},     {\"name\": \"Backpack\", \"category\": \"Fashion\", \"price\": 39.99},     {\"name\": \"Blender\", \"category\": \"Home Appliances\", \"price\": 99.99},     {\"name\": \"Desk Chair\", \"category\": \"Furniture\", \"price\": 129.99},     {\"name\": \"Water Bottle\", \"category\": \"Accessories\", \"price\": 19.99},     {\"name\": \"Notebook\", \"category\": \"Stationery\", \"price\": 5.99}, ]</p>"},{"location":"pandas/12_Data%20Exploration.html#define-a-function-to-generate-order-data","title":"Define a function to generate order data","text":"<p>def generate_order_data(num_rows):     data = []     for _ in range(num_rows):         product = random.choice(products)         quantity = random.randint(1, 10)         total = round(product[\"price\"] * quantity, 2)         order = {             \"Order ID\": fake.uuid4(),             \"Customer Name\": fake.name(),             \"Customer Email\": fake.email(),             \"Product Name\": product[\"name\"],             \"Category\": product[\"category\"],             \"Quantity\": quantity,             \"Price\": product[\"price\"],             \"Total\": total,             \"Order Date\": fake.date_this_year(),             \"Shipping Address\": fake.address(),         }         data.append(order)     return data</p>"},{"location":"pandas/12_Data%20Exploration.html#generate-1000-rows-of-data","title":"Generate 1000 rows of data","text":"<p>num_rows = 1000 order_data = generate_order_data(num_rows)</p>"},{"location":"pandas/12_Data%20Exploration.html#save-the-data-to-a-csv-file","title":"Save the data to a CSV file","text":"<p>output_file = \"sample_orders.csv\" with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:     writer = csv.DictWriter(file, fieldnames=order_data[0].keys())     writer.writeheader()     writer.writerows(order_data) print(f\"Sample dataset with {num_rows} rows has been saved to '{output_file}'.\") This code ensures that the sample dataset is saved as a structured CSV file (sample_orders.csv) for further data analysis by pandas functions. Head function head() head(): Used to preview the top rows of the sample dataset. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration.html#display-the-first-10-rows-of-the-dataset","title":"Display the first 10 rows of the dataset","text":"<p>print(df.head(10)) Tail function tail() tail(): Used to preview the bottom rows of the sample dataset. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_1","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration.html#display-the-last-10-rows-of-the-dataset","title":"Display the last 10 rows of the dataset","text":"<p>print(df.tail(10)) Sample function sample() sample(): This function is highly valuable when working with large datasets. When we need to extract and analyze a smaller subset from a larger DataFrame, <code>sample()</code> helps efficiently retrieve random samples, enabling preliminary data exploration or performance evaluation. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_2","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration.html#read-and-display-the-random-10-rows-from-the-dataset","title":"Read and display the random 10 rows from the dataset","text":"<p>print(df.sample(10)) Information function info() info(): This function provides a summary of the dataset, including the number of entries, column names, data types, and memory usage. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_3","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration.html#display-a-summary-of-the-dataset","title":"Display a summary of the dataset","text":"<p>print(df.info()) Describe function describe() describe(): This function provides basic statistical information about the dataset, such as mean, standard deviation, minimum and maximum values, and quartiles. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_4","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration.html#display-the-basic-statistical-information-about-the-dataset","title":"Display the basic statistical information about the dataset","text":"<p>print(df.describe()) Value counts function value_counts() value_counts(): This method returns the count of all unique values in a column or a pandas Series. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_5","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration.html#display-the-count-of-all-unique-values-in-a-columnsuch-as-category","title":"Display the count of all unique values in a column,such as \"Category\"","text":"<p>print(df[\"Category\"].value_counts()) Shape attribute shape: This attribute returns the number of rows and columns in the dataset. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_6","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration.html#display-the-number-of-rows-and-columns-in-the-dataset","title":"Display the number of rows and columns in the dataset","text":"<p>print(df.shape) Dataframe dtypes attribute df.dtypes: This attribute returns the data types of all columns. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_7","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration.html#display-the-data-types-of-all-columns","title":"Display the data types of all columns","text":"<p>print(df.dtypes) Unique function unique() unique(): This method returns all unique values in a column or a pandas Series. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_8","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration.html#display-all-unique-values-in-a-column","title":"Display all unique values in a column.","text":"<p>print(df[\"Category\"].unique()) Nunique function nunique() nunique(): This function returns the number of unique values in a DataFrame. import pandas as pd</p>"},{"location":"pandas/12_Data%20Exploration.html#read-the-sample_orderscsv-file-into-a-pandas-dataframe_9","title":"Read the sample_orders.csv file into a Pandas DataFrame","text":"<p>df = pd.read_csv(\"sample_orders.csv\")</p>"},{"location":"pandas/12_Data%20Exploration.html#display-the-count-of-unique-values-in-the-dataset-sorted-in-descending-order","title":"Display the count of unique values in the dataset, sorted in descending order","text":"<p>df.nunique().sort_values(ascending=False) In this post we provide a detailed introduction to essential core functions in the Pandas library, which are crucial for data analysis. These functions make data exploration more straightforward and efficient, helping analysts quickly grasp the structure and characteristics of a dataset. With these must-know functions, data analysts can gain deeper insights into the data, uncover the stories behind it, and provide data-driven support for decision-making. These Pandas functions are an indispensable part of the data analysis workflow, enhancing both the efficiency and the depth of analysis. Thanks for your reading. Python Pandas Pandas Tutorial Pandas Data Preprocessing Python Pandas Dataframe Written by Gen. David L. Follow 453 Followers \u00b7 4 Following AI practitioner &amp; python coder to record what I learned in python project development No responses yet What are your thoughts? Respond More from Gen. David L. Gen. David L. Gen. David L. Advanced Pandas Features: ETL-PIPES: An Efficient Python ETL Enhance Your Data Processing\u2026 Data Processing Library The essence of data analysis lies in ETL-pipes is a powerful and flexible Python uncovering the stories behind the data. In thi\u2026 library specifically designed for ETL (Extract\u2026 Dec 5, 2024 105 Nov 30, 2024 45 6 Gen. David L. Gen. David L. How to Map Column Values in a Four Ways to Package a Python Pandas DataFrame? Project into an executable EXE\u2026 Mapping column values refers to replacing In Python, packaging a project into an specific values in a column with other values,\u2026 executable EXE file is a common task,\u2026 Dec 24, 2024 81 Sep 14, 2024 94 See all from Gen. David L. Recommended from Medium Anita Gupta Harold Finch Statistics Handbook for Data Top 25 Python Scripts To Automate Analysts Your Daily Tasks Why This Handbook? Python is an excellent tool for automating daily tasks, thanks to its simplicity and a wid\u2026 Sep 14, 2024 422 11 Nov 19, 2024 337 5 Lists Staff picks Stories to Help You Level-Up at Work 798 stories \u00b7 1568 saves 19 stories \u00b7 917 saves Self-Improvement 101 Productivity 101 20 stories \u00b7 3214 saves 20 stories \u00b7 2716 saves In The Pythoneers by Kevin Meneses Gonz\u00e1lez In Python in Plain English by Satyam Sahu 5 Ways to Make Money with Python How I Automated Data Cleaning in in 2025 Python Using Functions and\u2026 The future belongs to those who believe in the Discover the key Python techniques that beauty of their dreams.\u201d \u2014 Eleanor Roosevelt transformed my data-cleaning workflow fro\u2026 Dec 8, 2024 282 10 Nov 4, 2024 216 7 Gen. David L. Lorenzo Uriel Starting a Python Project How to Split Columns in a This guide documents the steps I take to set DataFrame Using Python Pandas up the foundation of any Python project, wit\u2026 Splitting columns is a common data manipulation operation in Pandas. It allows u\u2026 Nov 19, 2024 208 1 Dec 21, 2024 62 See more recommendations</p>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html","title":"Selecting and Filtering Data","text":"<p>Selecting and filtering data are fundamental operations when working with Pandas DataFrames. Pandas provides multiple methods for selecting specific rows and columns based on labels, positions, and conditions.</p>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#1-selecting-data","title":"1. Selecting Data","text":""},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#11-selecting-columns","title":"1.1 Selecting Columns","text":"<ul> <li>Using column names: <pre><code>import pandas as pd\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    \"Age\": [25, 30, 35, 40],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n})\n\n# Select a single column\nprint(df[\"Name\"])\n\n# Select multiple columns\nprint(df[[\"Name\", \"City\"]])\n</code></pre></li> </ul>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#12-selecting-rows","title":"1.2 Selecting Rows","text":"<ul> <li> <p>Using <code>.loc[]</code> (label-based indexing): <pre><code># Select a row by index label\nprint(df.loc[0])  # First row\n</code></pre></p> </li> <li> <p>Using <code>.iloc[]</code> (position-based indexing): <pre><code># Select a row by position\nprint(df.iloc[2])  # Third row\n</code></pre></p> </li> <li> <p>Selecting multiple rows: <pre><code>print(df.loc[0:2])  # Select rows 0 to 2\nprint(df.iloc[0:2])  # Select first two rows\n</code></pre></p> </li> </ul>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#2-filtering-data","title":"2. Filtering Data","text":"<p>Filtering is used to extract subsets of a DataFrame that meet specific conditions.</p>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#21-filtering-rows-based-on-condition","title":"2.1 Filtering Rows Based on Condition","text":"<pre><code># Select rows where Age &gt; 30\nprint(df[df[\"Age\"] &gt; 30])\n</code></pre>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#22-filtering-with-multiple-conditions","title":"2.2 Filtering with Multiple Conditions","text":"<p>Using <code>&amp;</code> (AND condition): <pre><code># Select rows where Age &gt; 30 and City is \"Chicago\"\nprint(df[(df[\"Age\"] &gt; 30) &amp; (df[\"City\"] == \"Chicago\")])\n</code></pre></p> <p>Using <code>|</code> (OR condition): <pre><code># Select rows where Age &lt; 30 or City is \"Houston\"\nprint(df[(df[\"Age\"] &lt; 30) | (df[\"City\"] == \"Houston\")])\n</code></pre></p> <p>Using <code>.isin()</code> for filtering specific values: <pre><code># Select rows where City is in [\"New York\", \"Houston\"]\nprint(df[df[\"City\"].isin([\"New York\", \"Houston\"])])\n</code></pre></p> <p>Using <code>.between()</code> for range-based filtering: <pre><code># Select rows where Age is between 25 and 35\nprint(df[df[\"Age\"].between(25, 35)])\n</code></pre></p> <p>Using <code>.query()</code> for SQL-like filtering: <pre><code>print(df.query(\"Age &gt; 30 and City == 'Chicago'\"))\n</code></pre></p>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#3-selecting-specific-rows-and-columns","title":"3. Selecting Specific Rows and Columns","text":"<pre><code># Select specific rows and columns\nprint(df.loc[df[\"Age\"] &gt; 30, [\"Name\", \"City\"]])\n</code></pre> <pre><code># Select first two rows and first two columns\nprint(df.iloc[0:2, 0:2])\n</code></pre>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#mini-tutorial-filtering-a-real-dataset","title":"Mini Tutorial: Filtering a Real Dataset","text":"<p>Let's assume we have a dataset <code>data.csv</code> with employee information:</p>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#step-1-load-the-dataset","title":"Step 1: Load the Dataset","text":"<pre><code>df = pd.read_csv(\"data.csv\")\n</code></pre>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#step-2-explore-the-data","title":"Step 2: Explore the Data","text":"<pre><code>print(df.head())  # View the first few rows\nprint(df.info())  # Get summary information\n</code></pre>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#step-3-apply-filtering","title":"Step 3: Apply Filtering","text":"<pre><code># Employees older than 40\nolder_employees = df[df[\"Age\"] &gt; 40]\n\n# Employees in the IT department\nit_employees = df[df[\"Department\"] == \"IT\"]\n\n# Employees with salary between 50K and 100K\nmid_salary_employees = df[df[\"Salary\"].between(50000, 100000)]\n\n# IT employees in New York\nny_it_employees = df[(df[\"Department\"] == \"IT\") &amp; (df[\"City\"] == \"New York\")]\n</code></pre>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#step-4-save-the-filtered-data","title":"Step 4: Save the Filtered Data","text":"<pre><code>ny_it_employees.to_csv(\"filtered_data.csv\", index=False)\n</code></pre>"},{"location":"pandas/13_Select%20and%20Filter_Data%20%281%29.html#conclusion","title":"Conclusion","text":"<ul> <li>Use <code>.loc[]</code> and <code>.iloc[]</code> for row and column selection.</li> <li>Use conditional filtering with boolean operators (<code>&amp;</code>, <code>|</code>).</li> <li>Use <code>.query()</code> for SQL-like filtering.</li> <li>Use <code>.isin()</code> and <code>.between()</code> for specialized filtering.</li> </ul> <p>Would you like an example using PandasGUI for interactive filtering? \ud83d\ude80</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html","title":"13 Select and Filter Data","text":"<p>Member-only story Python Pandas: loc and iloc How to Select and Filter Data in Python Python Fundamentals \u00b7 Following Published in Towards Dev \u00b7 5 min read \u00b7 Mar 16, 2024 27 Python pandas library provides several methods for selecting and filtering data, such as loc, iloc, [ ] bracket operator, query, isin, between. Open in app Search Write Image from pexels.com This article will guide you through the essential techniques and functions for data selection and filtering using pandas. Whether you need to extract specific rows or columns or apply conditional filtering, pandas has got you covered. Let\u2019s dive in! Table of Contents 1. Selecting Columns : [ ] operator, loc, iloc 2. Filtering Rows : [ ] operator, loc, iloc, isin, query, between, string methods 3. Updating Values : loc, iloc, replace 1. Selecting Columns loc[ ] : This accessor selects rows and columns by labels. Example: df.loc[row_label, column_label] *** You can also use loc for slicing operations: df.loc['row1_label':'row2_label' , 'column1_label':'column2_label']</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-loc-for-label-based-selection","title":"Using loc for label-based selection","text":"<p>df.loc[:, 'Customer Country':'Customer State'] all the rows &amp; columns from Customer Country to Customer State</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-loc-for-label-based-selection_1","title":"Using loc for label-based selection","text":"<p>df.loc[[0,1,2], 'Customer Country':'Customer State'] rows 0,1,2 &amp; columns from Customer Country to Customer State iloc[ ] : This accessor selects rows and columns by integer location. Example: df.iloc[row_position, column_position] *** You can also use iloc for slicing operations: df.iloc['row1_position':'row2_position','col1_position':'col2_position']</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-iloc-for-index-based-selection","title":"Using iloc for index-based selection","text":"<p>df.iloc[[0,1,2,3] , [3,4,5,6,7,8]]</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#or","title":"or","text":"<p>df.iloc[[0,1,2,3] , 3:9] rows 0,1,2,3 &amp; columns 3 to 8</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-iloc-for-index-based-selection_1","title":"Using iloc for index-based selection","text":"<p>df.iloc[:, 3:8] all the rows &amp; columns from 3 to 8 [ ] Bracket operator : It allows to select one or multiple columns. Example: df[['column_label']] or df[['column1', 'column2']]</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#selecting-a-single-column","title":"Selecting a single column","text":"<p>df[['Customer Country']] a single column</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#selecting-multiple-columns","title":"Selecting multiple columns","text":"<p>df[['Customer Country', 'Customer State']] multiple columns 2. Filtering Rows loc[ ] : It filters rows by labels. Example: df.loc[condition]</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-loc-for-filtering-rows","title":"Using loc for filtering rows","text":"<p>condition = df['Order Quantity'] &gt; 3 df.loc[condition]</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#or_1","title":"or","text":"<p>df.loc[df['Order Quantity'] &gt; 3] dataframe with the Order Quantity &gt; 3</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-loc-for-filtering-rows_1","title":"Using loc for filtering rows","text":"<p>df.loc[df['Customer Country'] == 'United States'] dataframe with the Customer Country = United States iloc() : It filters rows by integer positions.</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-iloc-for-filtering-rows","title":"Using iloc for filtering rows","text":"<p>df.iloc[[0, 2, 4]] rows with integer index 0, 2 and 4</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-iloc-for-filtering-rows_1","title":"Using iloc for filtering rows","text":"<p>df.iloc[:3, :2] [ ] Bracket operator : It allows filtering rows based on a condition. Example: df[condition]</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-bracket-operator-for-filtering-rows-using-bracket-operator-for-fi","title":"Using [] bracket operator for filtering rows# Using [] bracket operator for fi","text":"<p>condition = df['Order Quantity'] &gt; 3 df[condition]</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#or_2","title":"or","text":"<p>df[df['Order Quantity'] &gt; 3] dataframe with the order quantity &gt; 3 isin([ ]) : It is used to filter data based on a list. Example: df[df['column_name'].isin(['value1', 'value2'])]</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-isin-for-filtering-rows","title":"Using isin for filtering rows","text":"<p>df[df['Customer Country'].isin(['United States', 'Puerto Rico'])] rows where column \u2018Customer Country\u2019 is \u2018United States\u2019 or \u2018Puerto Rico\u2019</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#filter-rows-based-on-values-in-a-list-and-select-spesific-columns","title":"Filter rows based on values in a list and select spesific columns","text":"<p>df[[\"Customer Id\", \"Order Region\"]][df['Order Region'].isin(['Central America',  it selected Customer Id and Order Region columns, where Order Region is Central America or Caribbean</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-not-isin-for-filtering-rows","title":"Using NOT isin for filtering rows","text":"<p>df[~df['Customer Country'].isin(['United States'])] rows where column \u2018Customer Country\u2019 is NOT \u2018United States\u2019 query() : This method is used to select data based on a SQL-like expression. Example: df.query('condition') In case your column names contain spaces or special characters, first you should use the rename() function to rename them.</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#rename-the-columns-before-performing-the-query","title":"Rename the columns before performing the query","text":"<p>df.rename(columns={'Order Quantity' : 'Order_Quantity', \"Customer Fname\" : \"Cust</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-query-for-filtering-rows-with-a-single-condition","title":"Using query for filtering rows with a single condition","text":"<p>df.query('Order_Quantity &gt; 3') dataframe with the order quantity &gt; 3</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-query-for-filtering-rows-with-multiple-conditions","title":"Using query for filtering rows with multiple conditions","text":"<p>df.query('Order_Quantity &gt; 3 and Customer_Fname == \"Mary\"') dataframe with the order quantity &gt; 3 and Customer Fname = Mary between() : Filters rows based on values that fall within a specified range. Example: df[df['column_name'].between(start, end)]</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#filter-rows-based-on-values-within-a-range","title":"Filter rows based on values within a range","text":"<p>df[df['Order Quantity'].between(3, 5)] dataframe with the order quantity between 3 and 5 String methods : Filters rows based on string matching conditions. Example: str.startswith(), str.endswith(), str.contains()</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-strstartswith-for-filtering-rows","title":"Using str.startswith() for filtering rows","text":"<p>df[df['Category Name'].str.startswith('Cardio')]</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#using-strcontains-for-filtering-rows","title":"Using str.contains() for filtering rows","text":"<p>df[df['Customer Segment'].str.contains('Office')] 3. Updating Values loc[ ] : This accessor selects specific rows and columns in the DataFrame and assigns new values.</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#update-values-in-a-column-based-on-a-condition","title":"Update values in a column based on a condition","text":"<p>df.loc[df['Customer Country'] == 'United States', 'Customer Country'] = 'USA' It changed the United Stated to USA, in Customer Country column iloc[ ] : This accessor selects specific rows and columns in the DataFrame and assigns new values.</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#update-values-in-a-column-based-on-a-condition_1","title":"Update values in a column based on a condition","text":"<p>df.iloc[df['Order Quantity'] &gt; 3, 15] = 'greater than 3'</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#or_3","title":"or","text":"<p>condition = df['Order Quantity'] &gt; 3 df.iloc[condition, 15] = 'greater than 3' It changed the values greater than 3 in Order Quantity column to \u201cgreater than 3\u201d text replace() : It replaces specific values in a DataFrame with new values. Ex: df.['column_name'].replace(old_value, new_value, inplace=True)</p>"},{"location":"pandas/13_Select%20and%20Filter_Data.html#replace-specific-values-in-a-column","title":"Replace specific values in a column","text":"<p>df['Order Quantity'].replace(5, 'equals 5', inplace=True) It changed the \u201c5\u201d to \u201cequals 5\u201d in Order Quantity column Conclusion Python pandas provides several functions and techniques for selecting and filtering data within a DataFrame. By mastering these techniques, you\u2019ll be well-equipped to explore and analyze your data effectively. Remember, practice makes perfect. Thank you for reading! Python Fundamentals \ud83d\ude80 Thank you for your time and interest!  \ud83d\udcab You can find even more content at Python Fundamentals  Python Data Science Data Analysis Data Analytics Data Scientist Published in Towards Dev Follow 6.3K Followers \u00b7 Last published\u00a017 hours ago A publication for sharing projects, ideas, codes, and new theories. Written by Python Fundamentals Following 2.5K Followers \u00b7 1.1K Following Learn Python with new contents every day No responses yet What are your thoughts? Respond More from Python Fundamentals and Towards Dev In Python in Plain English by Python Fundamentals In Towards Dev by AshokReddy Selecting and Filtering Data in Swagger UI is Gone in\u00a0.NET 9: Python Pandas: loc and iloc Here\u2019s What You Need to Do Next Python pandas library provides several Introduction: methods for selecting and filtering data, suc\u2026 Jan 27 12 Jan 27 66 2 In Towards Dev by Bill WANG Python Fundamentals Running DeepSeek R1 Locally on 10 Useful Python Pandas Functions Mac in Two Simple Commands for Data Exploration Follow up on my DeepSeek blogs about Data exploration and data understanding are crucial steps to take before diving into data\u2026 Jan 26 79 4 May 4, 2024 20 See all from Python Fundamentals See all from Towards Dev Recommended from Medium J. Aashish Kumar Building a Data Pipeline with 25 Amazing Python Tricks That Python: A Step-by-Step Guide to\u2026 Will Instantly Improve Your Code Photo by Tudor Baciu on Unsplash Nov 4, 2024 21 Feb 17 1 Lists Predictive Modeling w/ Coding &amp; Development Python 11 stories \u00b7 1016 saves 20 stories \u00b7 1838 saves Practical Guides to Machine ChatGPT prompts Learning 51 stories \u00b7 2599 saves 10 stories \u00b7 2214 saves PURRFECT SOFTWARE LIMITED In Stackademic by Khouloud Haddad Amamou 10 Python Libraries for Web Data Analysis with Python Pandas Scraping That Changed the Game\u2026 and Matplotlib Join Me as I Share the Essential Python 1. Introduction Libraries That Made Web Scraping a Breeze Feb 16 10 Jan 30 150 8 In Python in Plain English by Kiran Maan Ime Eti-mfon Don\u2019t Use This Way to Copy the Data Wrangling with Python Objects in Python Merging, Joining, and Concatenating Data You won\u2019t regret learning this!!! Feb 17 187 Nov 7, 2024 10 1 See more recommendations</p>"},{"location":"pandas/14_Mapping_Column_Values.html","title":"14 Mapping Column Values","text":"<p>Open in app Search Write How to Map Column Values in a Pandas DataFrame? Gen. Devin DL. \u00b7 Follow 3 min read \u00b7 Dec 24, 2024 86 Mapping column values refers to replacing specific values in a column with other values, commonly used in data cleaning and transformation. There are many scenarios where mapping column values is useful. Here are a few common use cases: 1. Mapping certain string values in a column to numbers. For example, mapping \u201cMale\u201d and \u201cFemale\u201d to 0 and 1, respectively, to facilitate training and prediction in machine learning algorithms. 2. Replacing abbreviations with full names. For instance, replacing \u201cUSA\u201d and \u201cUK\u201d with \u201cUnited States\u201d and \u201cUnited Kingdom\u201d to make the data more readable. 3. Correcting mis-spelled words by replacing them with the correct spelling. For example, replacing \u201cCocacola\u201d with \u201cCoca-Cola\u201d to avoid errors in statistics and analysis. In this post we will introduce several common mapping techniques. Using the map function The map function is one of the simplest and most direct methods. For example, in the following case, mapping gender values to 0 and 1. import pandas as pd df = pd.DataFrame({     \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Frank\"],     \"sex\": [\"female\", \"male\", \"male\", \"male\", \"female\", \"male\"],     \"age\": [23, 34, 29, 42, 25, 31], }) df.sex = df.sex.map({     \"female\": 0,      \"male\": 1, }) print(df) Before mapping: After mapping: Factorize Mapping Using the map function to map column values is the most straightforward method. However, when there are many distinct values in the column, mapping them one by one can be cumbersome. For example, in the case of the grade column in the following example, unlike the sex column which only has two values, it contains more possible values. In this case, we can use the factorize method to perform the mapping. import pandas as pd df = pd.DataFrame({     \"name\": [\"John\", \"Emily\", \"Michael\", \"Sarah\", \"James\", \"Olivia\"],     \"sex\": [\"male\", \"female\", \"male\", \"female\", \"male\", \"female\"],     \"grade\": [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"], }) df.sex = df.sex.factorize()[0] df.grade = df.grade.factorize()[0] print(df) Before mapping: After mapping: The factorize function returns a tuple with two elements. The first element is an array of numbers representing the mapped values, and the second element is an index type, where the index values correspond to the unique values in the column. df.grade.factorize() So the code uses factorize()[0]. If we want to binarize the values after mapping, for example, in the case of the grade column, where there are four different values representing different grade levels. If we only want two categories \u2014 fail (F) and pass (non-F), then code will be\uff1a df.grade = df.grade.factorize()[0] df.grade = (df.grade == 2).astype(\"int\") print(df) After mapping: Here A and B are mapped to 0, and C is mapped to 1. Thanks for your reading. Pandas Dataframe Python Data Preprocessing Pandas Written by Gen. Devin DL. Follow 505 Followers \u00b7 4 Following AI practitioner &amp; python coder to record what I learned in AI &amp; Data Science &amp; Python project development No responses yet What are your thoughts? Respond More from Gen. Devin DL. Gen. Devin DL. Gen. Devin DL. Four Ways to Package a Python Building Python ETL Data Pipelines Project into an executable EXE\u2026 with Five Typical Cases In Python, packaging a project into an Data pipelines are the backbone of data executable EXE file is a common task,\u2026 processing. Simply put, it\u2019s about moving da\u2026 Sep 14, 2024 104 Jan 26 71 Gen. Devin DL. Gen. Devin DL. ETL-PIPES: An Efficient Python ETL Comparison of Flask, Django, and Data Processing Library FastAPI: Advantages,\u2026 ETL-pipes is a powerful and flexible Python In Python web development, Flask, Django, library specifically designed for ETL (Extract\u2026 and FastAPI are all popular frameworks. This\u2026 Nov 30, 2024 117 6 Oct 17, 2023 92 See all from Gen. Devin DL. Recommended from Medium Sarowar Jahan Saurav Vijay Gadhave Beyond the Basics: 11 Complex When to Use COUNT(*) vs Statistical Algorithms to Elevate\u2026 COUNT(1) in SQL Queries Data Science is more than just running Note: If you\u2019re not a medium member, CLICK standard algorithms or crafting elegant\u2026 HERE Sep 13, 2024 107 Jan 14 399 25 Lists Staff picks Stories to Help You Level-Up at Work 817 stories \u00b7 1632 saves 19 stories \u00b7 943 saves Self-Improvement 101 Productivity 101 20 stories \u00b7 3316 saves 20 stories \u00b7 2791 saves In Stackademic by Khouloud Haddad Amamou Alice Yang Data Analysis with Python Pandas How to Create, Update and Remove and Matplotlib Tables in Excel with Python 1. Introduction Tables in Excel are a powerful tool that helps us manage, organize, and analyze data\u2026 Jan 30 150 8 Oct 10, 2024 11 In Artificial Intelligence in Plain En\u2026 by Ritesh Gu\u2026 Python Coding Data Science All Algorithm 9 Confusing Python Concepts That Cheatsheet 2025 Frustrate Most Developers Stories, strategies, and secrets to choosing 1. The is vs == Confusion the perfect algorithm. Jan 5 1.3K 27 Oct 2, 2024 162 2 See more recommendations</p>"},{"location":"pandas/15_Extract_Transform_Load.html","title":"15 Extract Transform Load","text":"<p>Open in app Search Write Five Common Python Libraries for ETL Processing Gen. David L. \u00b7 Follow 5 min read \u00b7 Dec 10, 2024 130 3 ETL is a critical concept in data warehousing technology, representing three main processing steps: Extract, Transform, and Load. ETL is commonly used for data integration, merging data from various sources into a unified view for analysis and reporting. ETL Detailed Process 1. Extract (E): This step involves extracting data from various sources, which could include relational databases, file systems, cloud storage, or APIs. The extraction process may involve copying or moving data and recognizing and parsing its format. 2. Transform (T): Once the data is extracted, it usually needs to be transformed into a format suitable for analysis. This process may include: Data cleansing (e.g., removing duplicates, correcting errors, and resolving inconsistencies). Data aggregation (e.g., calculating sums, averages, maximum/minimum values). Data normalization (e.g., converting data into a unified format or unit). Data enrichment (e.g., adding additional contextual information). The transformation step is the most critical part of ETL as it directly affects the quality and usability of the data in the final data warehouse. 3. Load (L): The loading step involves writing the transformed data into the target system, which could be a data warehouse, data lake, data mart, or another form of data storage. During this step, the data may be indexed, partitioned, or archived to optimize query performance and storage efficiency. ETL processes can be either batch-based or real-time. Batch ETL typically runs at scheduled intervals, such as daily, weekly, or monthly, whereas real- time ETL requires the system to continuously process and update data. Common Python ETL Libraries 1. Pandas Advantages: Pandas provides data structures and analytical tools that are highly suitable for data cleaning and transformation. It simplifies data manipulation tasks in the ETL process, such as adding new columns and filtering data. Disadvantages: Pandas may encounter performance bottlenecks when handling large datasets, as it primarily operates on in-memory data. Use Cases: Suitable for small to medium-sized datasets and for rapid prototyping. Code Example: import pandas as pd</p>"},{"location":"pandas/15_Extract_Transform_Load.html#read-data-from-a-csv-file","title":"Read data from a CSV file","text":"<p>df = pd.read_csv('my_example.csv')</p>"},{"location":"pandas/15_Extract_Transform_Load.html#data-transformation-such-as-adding-a-new-column","title":"Data transformation, such as adding a new column","text":"<p>df['new_A_column'] = df['existing_A_column'].apply(lambda x: x * 5.4)</p>"},{"location":"pandas/15_Extract_Transform_Load.html#save-the-transformed-data-to-a-new-csv-file","title":"Save the transformed data to a new CSV file","text":"<p>df.to_csv('transformed_example.csv', index=False) 2. Apache Spark (PySpark) Advantages: Apache Spark is a unified analytics engine designed for large- scale data processing. The PySpark API makes it simple to handle Spark jobs within Python workflows. Disadvantages: Compared to Pandas, Spark has a steeper learning curve and requires more resources for setup and operation. Use Cases: Ideal for processing large datasets, especially in distributed computing environments. Code Example: from pyspark.sql import SparkSession</p>"},{"location":"pandas/15_Extract_Transform_Load.html#initialize-a-spark-session","title":"Initialize a Spark session","text":"<p>spark = SparkSession.builder.appName('etl_example').getOrCreate()</p>"},{"location":"pandas/15_Extract_Transform_Load.html#read-data-from-a-csv-file_1","title":"Read data from a CSV file","text":"<p>df = spark.read.csv('input_example_data.csv', header=True, inferSchema=True)</p>"},{"location":"pandas/15_Extract_Transform_Load.html#data-transformation","title":"Data transformation","text":"<p>df = df.withColumn('new_A_column', df['existing_A_column'] * 5.4)</p>"},{"location":"pandas/15_Extract_Transform_Load.html#write-the-data-to-a-new-csv-file","title":"Write the data to a new CSV file","text":"<p>df.write.csv('output_example_data.csv') 3. Haul Advantages: Haul is a lightweight and modular Python library designed specifically for ETL tasks. It focuses on simplifying data extraction, transformation, and loading by providing clean, reusable components that integrate seamlessly into modern workflows. Disadvantages: Haul is relatively new and less feature-rich compared to more established ETL frameworks like Airflow or Luigi. It may not be suitable for highly complex ETL pipelines or large-scale distributed systems. Use Cases: Ideal for small to medium-sized ETL workflows, rapid prototyping, and scenarios requiring flexibility and simplicity. Code Example: from haul import Extractor, Transformer, Loader</p>"},{"location":"pandas/15_Extract_Transform_Load.html#define-an-extractor-to-load-data-from-a-csv-file","title":"Define an extractor to load data from a CSV file","text":"<p>extractor = Extractor.from_csv('input_example_data.csv')</p>"},{"location":"pandas/15_Extract_Transform_Load.html#define-a-transformer-to-add-a-new-column","title":"Define a transformer to add a new column","text":"<p>class MultiplyTransformer(Transformer):     def transform(self, row):         row['new_A_column'] = row['existing_A_column'] * 5.4         return row transformer = MultiplyTransformer()</p>"},{"location":"pandas/15_Extract_Transform_Load.html#define-a-loader-to-save-the-data-to-a-new-csv-file","title":"Define a loader to save the data to a new CSV file","text":"<p>loader = Loader.to_csv('output_example_data.csv')</p>"},{"location":"pandas/15_Extract_Transform_Load.html#execute-the-etl-process","title":"Execute the ETL process","text":"<p>etl_pipeline = extractor | transformer | loader etl_pipeline.run() 4. Luigi Advantages: Luigi is a Python library designed for building complex batch job pipelines. It handles dependency resolution, workflow management, visualization, and failure handling. Disadvantages: Luigi does not automatically synchronize tasks to worker nodes and lacks built-in scheduling, alerting, or monitoring features. Use Cases: Suitable for automating simple ETL workflows, such as log processing. Code Example: from luigi import Task, ExternalTask, Parameter, LocalTarget import pandas as pd  </p>"},{"location":"pandas/15_Extract_Transform_Load.html#define-a-task-to-extract-data-from-a-source-url","title":"Define a task to extract data from a source URL","text":"<p>class ExtractData(Task):     source_url = Parameter()  # Define a parameter for the source URL of the dat     def run(self):         # Read data from the source URL into a pandas DataFrame         df = pd.read_csv(self.source_url)         # Write the DataFrame to a CSV file at the specified output location         self.output().open('w').write(df.to_csv(index=False))     def output(self):         # Define the output target for this task (a local CSV file)         return LocalTarget('output_data_example.csv')</p>"},{"location":"pandas/15_Extract_Transform_Load.html#define-a-task-to-transform-the-data","title":"Define a task to transform the data","text":"<p>class TransformData(ExternalTask):     source_url = Parameter()  # Define a parameter for the source URL of the inp     def run(self):         # Read data from the source URL into a pandas DataFrame         df = pd.read_csv(self.source_url)         # Perform a hypothetical transformation: adding a new column         df['new_A_column'] = df['existing_A_column'] * 5.4         # Write the transformed DataFrame to a CSV file at the specified output          self.output().open('w').write(df.to_csv(index=False))     def output(self):         # Define the output target for this task (a transformed local CSV file)         return LocalTarget('transformed_example_data.csv')</p>"},{"location":"pandas/15_Extract_Transform_Load.html#main-entry-point-for-luigi-tasks","title":"Main entry point for Luigi tasks","text":"<p>if name == 'main':     # Run the TransformData task, which depends on 'example_data.csv' as input     luigi.build([TransformData('example_data.csv')]) 5. Airflow Advantages: Apache Airflow is a powerful workflow orchestration tool, allowing users to programmatically author, schedule, and monitor workflows. Its flexibility and modular architecture make it ideal for creating complex ETL pipelines that can handle dependencies and integrate with various data sources. Disadvantages: Airflow has a steep learning curve, especially for users unfamiliar with its DAG (Directed Acyclic Graph) paradigm. Additionally, it requires careful setup and maintenance, as scaling can introduce performance and resource challenges. Use Cases: Best suited for orchestrating ETL workflows in environments with multiple steps or dependencies. It is particularly effective for managing workflows in distributed or cloud-based systems. Code Example: from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime</p>"},{"location":"pandas/15_Extract_Transform_Load.html#define-the-etl-functions","title":"Define the ETL functions","text":"<p>def extract():     print(\"Extracting data...\")     print(\"more code here to do extract job\") def transform():     print(\"Transforming data...\")     print(\"more code here to do transform job\") def load():     print(\"Loading data...\")     print(\"more code here to do load job\")</p>"},{"location":"pandas/15_Extract_Transform_Load.html#initialize-the-dag","title":"Initialize the DAG","text":"<p>default_args = {     'owner': 'airflow',     'start_date': datetime(2024, 1, 1),     'retries': 1, } dag = DAG(     'example_etl',     default_args=default_args,     schedule_interval='@daily', )</p>"},{"location":"pandas/15_Extract_Transform_Load.html#define-the-tasks","title":"Define the tasks","text":"<p>extract_task = PythonOperator(task_id='extract', python_callable=extract, dag=da transform_task = PythonOperator(task_id='transform', python_callable=transform,  load_task = PythonOperator(task_id='load', python_callable=load, dag=dag)</p>"},{"location":"pandas/15_Extract_Transform_Load.html#set-task-dependencies","title":"Set task dependencies","text":"<p>extract_task &gt;&gt; transform_task &gt;&gt; load_task Summary These libraries and tools each have their strengths, and the choice of which to use depends on the specific project requirements, data scale, and the familiarity of the development team. The ETL process forms the foundation of data management and analysis, ensuring data consistency, accuracy, and availability. This enables data analysis and business intelligence (BI) tools to deliver valuable insights and support decision-making. With technological advancements, the ETL process continues to evolve, giving rise to variations like ELT (Extract, Load, Transform), where data is first loaded into the target system before transformation. This approach can improve efficiency when dealing with large-scale datasets. Thanks for your reading. Python Etl Etl Tool Etl Pipeline Python Data Preprocessing Written by Gen. David L. Follow 453 Followers \u00b7 4 Following AI practitioner &amp; python coder to record what I learned in python project development Responses (3) What are your thoughts? Respond david libert (\u201cdadoo\u201d) 29 days ago Hello thank you. You forgot Mage.ai, very cool and strong solution \ud83d\ude09 4 Reply Dinu Gherman 27 days ago Missing prefect.io. 2 Reply R. Ganesh 16 days ago these frameworks not libraries Reply More from Gen. David L. Gen. David L. Gen. David L. Advanced Pandas Features: ETL-PIPES: An Efficient Python ETL Enhance Your Data Processing\u2026 Data Processing Library The essence of data analysis lies in ETL-pipes is a powerful and flexible Python uncovering the stories behind the data. In thi\u2026 library specifically designed for ETL (Extract\u2026 Dec 5, 2024 105 Nov 30, 2024 45 6 Gen. David L. Gen. David L. How to Map Column Values in a Must-Know Python Pandas Pandas DataFrame? Functions for Effortless Data\u2026 Mapping column values refers to replacing The key point of data analysis lies in specific values in a column with other values,\u2026 uncovering the stories behind the data. To\u2026 Dec 24, 2024 81 Nov 27, 2024 19 See all from Gen. David L. Recommended from Medium Harold Finch Vijay Gadhave Top 25 Python Scripts To Automate Must-Have Data Engineering Your Daily Tasks Certifications Python is an excellent tool for automating Note: If you\u2019re not a medium member, CLICK daily tasks, thanks to its simplicity and a wid\u2026 HERE Nov 19, 2024 337 5 Dec 3, 2024 300 5 Lists Staff picks Stories to Help You Level-Up at Work 798 stories \u00b7 1568 saves 19 stories \u00b7 916 saves Self-Improvement 101 Productivity 101 20 stories \u00b7 3214 saves 20 stories \u00b7 2716 saves In Wren AI by Howard Chi Varun Singh How Uber is Saving 140,000 Hours Python 3.14 Released \u2014 Top 5 Each Month Using Text-to-SQL \u2014 \u2026 Features You Must Know Discover how Uber\u2019s Text-to-SQL technology Faster Annotations &amp; Mind-Blowing Updates streamlines data queries and learn how to\u2026 You NEED to Know! Jan 2 405 7 Dec 31, 2024 613 4 Sai Parvathaneni Esra Soylu Data Quality Checks (DQCs): A Advanced SQL Techniques Guide for Data Engineers In this article, we will focus on advanced SQL As a data engineer in financial services, I\u2019ve techniques. I will try to explain the methods\u2026 learned one critical lesson: the quality of you\u2026 Dec 8, 2024 62 2 Nov 23, 2024 263 3 See more recommendations</p>"},{"location":"pandas/advanced_techniques.html","title":"Advanced Techniques","text":"<p>Advanced Pandas Techniques for Data Processing and Performance</p>"},{"location":"pandas/advanced_techniques.html#1-data-chunking","title":"1. Data chunking","text":"<p>There are often scenarios we encounter where the size of the data is more than the available memory (RAM) we have. In such cases, it\u2019s a good idea to read data in chunks from the file so that the system doesn\u2019t run out of memory.</p> <pre><code>1 # Initialize an empty list to store the chunks\n2 private_rooms = []\n3\n4 # Read the CSV file in chunks\n5 for chunk in pd.read_csv('data/listings.csv', chunksize=1000):\n6     # Process each chunk (for example, filter listings where room_type is \"Private room\")\n7     processed_chunk = chunk[chunk[\"room_type\"] == \"Private room\"]\n8     \n9     # Append the processed chunk to the list\n10     private_rooms.append(processed_chunk)\n11\n12 # Combine all processed chunks into a dataframe\n13 private_rooms = pd.concat(private_rooms)\n</code></pre>"},{"location":"pandas/advanced_techniques.html#2-progress-bar","title":"2. Progress bar","text":"<p>Pandas\u2019 apply() functions, which are commonly used to perform element- wise operations on DataFrame columns or rows.</p> <pre><code>1 import time\n2 from datetime import datetime\n3\n4 from tqdm import tqdm\n5 tqdm.pandas()  # add this to integrate progress bar functionality to pandas\n6\n7\n8 def convert_to_datetime(date):\n9     # return date if type of date is not string\n10     if type(date) != str:\n11         return date\n12     \n13     # time.sleep() added to demonstrate the progress bar more effectively \n14     time.sleep(0.1)\n15\n16     # returns a datetime object\n17     return datetime.strptime(date, \"%Y-%m-%d\")\n18\n19 private_rooms[\"last_review\"] = private_rooms[\"last_review\"].progress_apply(convert_to_datetime)\n</code></pre>"},{"location":"pandas/advanced_techniques.html#3-populate-multiple-columns","title":"3. Populate Multiple Columns","text":"<p>Solution: apply() with result_type=\"expand\"</p> <pre><code>1 def get_date_and_month_name(last_review_date):\n2     # returns the day and month\n3     return last_review_date.day_name(), last_review_date.month_name()\n4\n5\n6 private_rooms[[\"Day\", \"Month\"]] = private_rooms.apply(\n7     lambda x : get_date_and_month_name(x[\"last_review\"]),\n8     axis=1,\n9     result_type=\"expand\"\n10 )\n</code></pre>"},{"location":"pandas/advanced_techniques.html#4-parallel-processing","title":"4. Parallel Processing","text":"<p>Use multiprocessing module.  (using np.array_split()) into multiple chunks (depending on the number of cores available in your system) Once the chunks are processed and the desired output for each chunk is obtained, it can be concatenated back to a single DataFrame.</p> <pre><code>1 import time\n2 import random\n3 from multiprocessing import Pool\n4\n5 import numpy as np\n6 import pandas as pd\n7 from tqdm import tqdm\n8\n9 tqdm.pandas()\n10\n11\n12 def predict_sentiment(review):\n13     time.sleep(0.1)  # simulating time taken by prediction model\n14     return \"Positive\" if random.randint(1,10) &gt; 5 else \"Negative\"\n15\n16\n17 def batch_predict_sentiment(review_df):\n18     review_df[\"sentiment\"] = review_df[\"comments\"].progress_apply(predict_sentiment)\n19     return review_df\n20\n21\n22 def fetch_sentiment_for_review():\n23     n_cores = 64\n24     reviews = pd.read_csv(\"data/reviews.csv\")\n25     review_batches = np.array_split(reviews, n_cores)  # split into same number of batches as n_c\n26     \n27     # Processing Parallely\n28     with Pool(n_cores) as pool:\n29         sentiment_prediction_batches = pool.map(batch_predict_sentiment, review_batches)\n30\n31     # Once all the batches are processed, concatenate list of DataFrames into a single DataFrame\n32     sentiment_prediction = pd.concat(sentiment_prediction_batches)\n33     return sentiment_prediction\n34\n35\n36 reviews_with_sentiment = fetch_sentiment_for_review()\n</code></pre> <p>From more than 13 hours to less than 13 minutes! Each batch consists of 7521 reviews and there are a total of 64 batches. In this scenario</p> <p>Tip: set n_cores more than the actual number of cores my system has.  This is because during the execution of time.sleep(0.1) the CPU remains idle and each process interleaves for other process to execute. If your process is CPU intensive, it is recommended to keep n_cores less than the actual number of cores your system has.</p>"},{"location":"pandas/advanced_techniques.html#5-complex-merging","title":"5. Complex Merging","text":"<p>Merging is quite a common operation performed by individuals who deal with data. However, sometimes it can get quite complicated to understand if any particular data points were lost during the merging process. It might be due to a plethora of reasons \u2014 the worst one being, malformed or faulty data.</p> <p>The indicator=True.  When enabled it creates a new column named _merge which can denote three different scenarios based on the type of merge operation performed.</p> <ul> <li>left_only \u2014 indicates that the row\u2019s key only exists in the left DataFrame and it couldn\u2019t find a match in the right DataFrame</li> <li>right_only \u2014 indicates that the row\u2019s key only exists in the right</li> <li>both \u2014 indicates that the row\u2019s key exists in both the DataFrames and data</li> </ul> <pre><code>merged_df = listings.merge(\n    reviews,\n    left_on=\"id\",\n    right_on=\"listing_id\",\n    how=\"outer\",\n    indicator=True\n)\nmerged_df.shape\n</code></pre>"},{"location":"pandas/advanced_techniques.html#6-data-segmentation","title":"6. Data Segmentation","text":"<p>pd.cut() is a powerful function that can be used when you need to segment data into multiple bins. It can also act as a way to convert continuous values into categorical values. One such scenario is demonstrated in the example below. We will be Segmenting the price of each listing into multiple price brackets (bins). We can set a predetermined number of price brackets \u2014 \u201c$0 \u2014 $100\u201d, \u201c$101 \u2014 $250\u201d, \u201c$251 \u2014 $500\u201d, \u201c$500 \u2014 $1000\u201d, and \u201c$1000+\u201d.</p> <p>Create bins and label for each bin <pre><code>bins = [0, 100, 250, 500, 1000, float('inf')]\nlabels = [\"$0 - $100\", \"$101 - $250\", \"$251 - $500\", \"$500 - $1000\", \"$1000+\"]\nlistings[\"price_bucket\"] = pd.cut(listings[\"price\"], bins=bins, labels=labels)\n</code></pre></p> <p>Please note here that the number of labels (5) is less than number of bins (6) by one. This is because the initial two values in the bin belong to the first label.</p>"},{"location":"pandas/advanced_techniques.html#7-cross-tabulation-analysis","title":"7. Cross-Tabulation Analysis","text":"<p>Using the above data, we can go one step further and perform a cross- tabulation between the price brackets and room types available for those price brackets. Here\u2019s where pd.crosstab() comes into play. It can be used to perform a simple cross-tabulation between price_bucket and room_type.</p> <pre><code>room_type_breakup = pd.crosstab(\n    listings[\"price_bucket\"],\n    listings[\"room_type\"],\n    margins=True  # This will add the row and column totals\n)\n</code></pre>"},{"location":"pandas/cheat_sheet.html","title":"Cheatsheet","text":""},{"location":"pandas/cheat_sheet.html#selecting-and-filtering-data","title":"Selecting and Filtering Data","text":"<p>Selecting and filtering data are fundamental operations when working with Pandas DataFrames. Pandas provides multiple methods for selecting specific rows and columns based on labels, positions, and conditions.</p>"},{"location":"pandas/cheat_sheet.html#1-selecting-data","title":"1. Selecting Data","text":""},{"location":"pandas/cheat_sheet.html#11-selecting-columns","title":"1.1 Selecting Columns","text":"<ul> <li>Using column names:</li> </ul> <pre><code>import pandas as pd\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    \"Age\": [25, 30, 35, 40],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n})\n\n# Select a single column\nprint(df[\"Name\"])\n\n# Select multiple columns\nprint(df[[\"Name\", \"City\"]])\n</code></pre>"},{"location":"pandas/cheat_sheet.html#12-selecting-rows","title":"1.2 Selecting Rows","text":"<ul> <li>Using <code>.loc[]</code> (label-based indexing):</li> </ul> <pre><code># Select a row by index label\nprint(df.loc[0])  # First row\n</code></pre> <ul> <li>Using <code>.iloc[]</code> (position-based indexing):</li> </ul> <pre><code># Select a row by position\nprint(df.iloc[2])  # Third row\n</code></pre> <ul> <li>Selecting multiple rows:</li> </ul> <pre><code>print(df.loc[0:2])  # Select rows 0 to 2\nprint(df.iloc[0:2])  # Select first two rows\n</code></pre>"},{"location":"pandas/cheat_sheet.html#2-filtering-data","title":"2. Filtering Data","text":"<p>Filtering is used to extract subsets of a DataFrame that meet specific conditions.</p>"},{"location":"pandas/cheat_sheet.html#21-filtering-rows-based-on-condition","title":"2.1 Filtering Rows Based on Condition","text":"<pre><code># Select rows where Age &gt; 30\nprint(df[df[\"Age\"] &gt; 30])\n</code></pre>"},{"location":"pandas/cheat_sheet.html#22-filtering-with-multiple-conditions","title":"2.2 Filtering with Multiple Conditions","text":"<p>Using <code>&amp;</code> (AND condition):</p> <pre><code># Select rows where Age &gt; 30 and City is \"Chicago\"\nprint(df[(df[\"Age\"] &gt; 30) &amp; (df[\"City\"] == \"Chicago\")])\n</code></pre> <p>Using <code>|</code> (OR condition):</p> <pre><code># Select rows where Age &lt; 30 or City is \"Houston\"\nprint(df[(df[\"Age\"] &lt; 30) | (df[\"City\"] == \"Houston\")])\n</code></pre> <p>Using <code>.isin()</code> for filtering specific values:</p> <pre><code># Select rows where City is in [\"New York\", \"Houston\"]\nprint(df[df[\"City\"].isin([\"New York\", \"Houston\"])])\n</code></pre> <p>Using <code>.between()</code> for range-based filtering:</p> <pre><code># Select rows where Age is between 25 and 35\nprint(df[df[\"Age\"].between(25, 35)])\n</code></pre> <p>Using <code>.query()</code> for SQL-like filtering:</p> <pre><code>print(df.query(\"Age &gt; 30 and City == 'Chicago'\"))\n</code></pre>"},{"location":"pandas/cheat_sheet.html#3-selecting-specific-rows-and-columns","title":"3. Selecting Specific Rows and Columns","text":"<pre><code># Select specific rows and columns\nprint(df.loc[df[\"Age\"] &gt; 30, [\"Name\", \"City\"]])\n</code></pre> <pre><code># Select first two rows and first two columns\nprint(df.iloc[0:2, 0:2])\n</code></pre>"},{"location":"pandas/cheat_sheet.html#mini-tutorial-filtering-a-real-dataset","title":"Mini Tutorial: Filtering a Real Dataset","text":"<p>Let's assume we have a dataset <code>data.csv</code> with employee information:</p>"},{"location":"pandas/cheat_sheet.html#step-1-load-the-dataset","title":"Step 1: Load the Dataset","text":"<pre><code>df = pd.read_csv(\"data.csv\")\n</code></pre>"},{"location":"pandas/cheat_sheet.html#step-2-explore-the-data","title":"Step 2: Explore the Data","text":"<pre><code>print(df.head())  # View the first few rows\nprint(df.info())  # Get summary information\n</code></pre>"},{"location":"pandas/cheat_sheet.html#step-3-apply-filtering","title":"Step 3: Apply Filtering","text":"<pre><code># Employees older than 40\nolder_employees = df[df[\"Age\"] &gt; 40]\n\n# Employees in the IT department\nit_employees = df[df[\"Department\"] == \"IT\"]\n\n# Employees with salary between 50K and 100K\nmid_salary_employees = df[df[\"Salary\"].between(50000, 100000)]\n\n# IT employees in New York\nny_it_employees = df[(df[\"Department\"] == \"IT\") &amp; (df[\"City\"] == \"New York\")]\n</code></pre>"},{"location":"pandas/cheat_sheet.html#step-4-save-the-filtered-data","title":"Step 4: Save the Filtered Data","text":"<pre><code>ny_it_employees.to_csv(\"filtered_data.csv\", index=False)\n</code></pre>"},{"location":"pandas/cheat_sheet.html#conclusion","title":"Conclusion","text":"<ul> <li>Use <code>.loc[]</code> and <code>.iloc[]</code> for row and column selection.</li> <li>Use conditional filtering with boolean operators (<code>&amp;</code>, <code>|</code>).</li> <li>Use <code>.query()</code> for SQL-like filtering.</li> <li>Use <code>.isin()</code> and <code>.between()</code> for specialized filtering.</li> </ul> <p>Would you like an example using PandasGUI for interactive filtering? \ud83d\ude80</p>"},{"location":"pandas/contents.html","title":"Contents","text":"<p>For data scientists, pandas is the most important python module in their repertoire.</p> <p>In this workspace,  the following topics will be covered:</p> <ul> <li>Cheatsheet</li> <li>Selecting and Filtering</li> </ul>"},{"location":"pandas/generate_data.html","title":"Generate Data","text":""},{"location":"pandas/generate_data.html#modules","title":"Modules","text":"<ul> <li>faker In this post we will introduce several must-know Pandas methods for effective data exploration.</li> </ul>"},{"location":"pandas/generate_data.html#pandas-functions","title":"Pandas Functions","text":""},{"location":"pandas/generate_data.html#faker","title":"Faker","text":"<p>TODO: Use stock data to demonstrated this.</p> <p>Although not part of pandas,  we need a dataset to demonstrate various pandas functions.</p> <p>Create a CSV sample dataset using faker.</p> <pre><code>import random\nimport csv\nfrom faker import Faker\n\n# Initialize Faker\n\nfake = Faker()\n\n# List of products and their categories\n\nproducts = [\n    {\"name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 899.99},\n    {\"name\": \"Smartphone\", \"category\": \"Electronics\", \"price\": 699.99},\n    {\"name\": \"Headphones\", \"category\": \"Accessories\", \"price\": 49.99},\n    {\"name\": \"Coffee Maker\", \"category\": \"Home Appliances\", \"price\": 79.99},\n    {\"name\": \"Sneakers\", \"category\": \"Fashion\", \"price\": 59.99},\n    {\"name\": \"Backpack\", \"category\": \"Fashion\", \"price\": 39.99},\n    {\"name\": \"Blender\", \"category\": \"Home Appliances\", \"price\": 99.99},\n    {\"name\": \"Desk Chair\", \"category\": \"Furniture\", \"price\": 129.99},\n    {\"name\": \"Water Bottle\", \"category\": \"Accessories\", \"price\": 19.99},\n    {\"name\": \"Notebook\", \"category\": \"Stationery\", \"price\": 5.99},\n]\n\n# Define a function to generate order data\n\ndef generate_order_data(num_rows):\n    data = []\n    for_ in range(num_rows):\n        product = random.choice(products)\n        quantity = random.randint(1, 10)\n        total = round(product[\"price\"] * quantity, 2)\n        order = {\n            \"Order ID\": fake.uuid4(),\n            \"Customer Name\": fake.name(),\n            \"Customer Email\": fake.email(),\n            \"Product Name\": product[\"name\"],\n            \"Category\": product[\"category\"],\n            \"Quantity\": quantity,\n            \"Price\": product[\"price\"],\n            \"Total\": total,\n            \"Order Date\": fake.date_this_year(),\n            \"Shipping Address\": fake.address(),\n        }\n        data.append(order)\n    return data\n\n# Generate 1000 rows of data\n\nnum_rows = 1000\norder_data = generate_order_data(num_rows)\n\n# Save the data to a CSV file\n\noutput_file = \"sample_orders.csv\"\nwith open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n    writer = csv.DictWriter(file, fieldnames=order_data[0].keys())\n    writer.writeheader()\n    writer.writerows(order_data)\nprint(f\"Sample dataset with {num_rows} rows has been saved to '{output_file}'.\")\n</code></pre> <p>This code ensures that the sample dataset is saved as a structured CSV file (sample_orders.csv) for further data analysis by pandas functions. Head function head() head(): Used to preview the top rows of the sample dataset.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the first 10 rows of the dataset\n\nprint(df.head(10))\n</code></pre>"},{"location":"pandas/generate_data.html#head-tail","title":"Head Tail","text":"<p>head(): Use to preview the bottom rows of the sample dataset. tail(): Use to preview the bottom rows of the sample dataset.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the last 10 rows of the dataset\n\nprint(df.head(10))\nprint(df.tail(10))\n</code></pre>"},{"location":"pandas/generate_data.html#sample","title":"Sample","text":"<p>sample(): This function is highly valuable when working with large datasets. When we need to extract and analyze a smaller subset from a larger DataFrame, <code>sample()</code> helps efficiently retrieve random samples, enabling preliminary data exploration or performance evaluation.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Read and display the random 10 rows from the dataset\n\nprint(df.sample(10))\n</code></pre>"},{"location":"pandas/generate_data.html#info","title":"Info","text":"<p>Information function info() info(): This function provides a summary of the dataset, including the number of entries, column names, data types, and memory usage.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display a summary of the dataset\n\nprint(df.info())\n</code></pre>"},{"location":"pandas/generate_data.html#describe","title":"Describe","text":"<p>describe(): This function provides basic statistical information about the dataset, such as mean, standard deviation, minimum and maximum values, and quartiles.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the basic statistical information about the dataset\n\nprint(df.describe())\n</code></pre>"},{"location":"pandas/generate_data.html#value-counts","title":"Value Counts","text":"<p>Value counts function value_counts() value_counts(): This method returns the count of all unique values in a column or a pandas Series.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the count of all unique values in a column,such as \"Category\"\n\nprint(df[\"Category\"].value_counts())\n</code></pre>"},{"location":"pandas/generate_data.html#shape","title":"Shape","text":"<p>shape: This attribute returns the number of rows and columns in the dataset.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the number of rows and columns in the dataset\n\nprint(df.shape)\n~~~~\n\n#### Dtypes\n\nDataframe dtypes attribute\ndf.dtypes: This attribute returns the data types of all columns.\n\n~~~python\nimport pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the data types of all columns\n\nprint(df.dtypes)\n</code></pre>"},{"location":"pandas/generate_data.html#unique","title":"Unique","text":"<p>Unique function unique() unique(): This method returns all unique values in a column or a pandas Series.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display all unique values in a column\n\nprint(df[\"Category\"].unique())\n</code></pre>"},{"location":"pandas/generate_data.html#non-unique","title":"Non-unique","text":"<p>Nunique function nunique() nunique(): This function returns the number of unique values in a DataFrame.</p> <pre><code>import pandas as pd\n\n# Read the sample_orders.csv file into a Pandas DataFrame\n\ndf = pd.read_csv(\"sample_orders.csv\")\n\n# Display the count of unique values in the dataset, sorted in descending order\n\ndf.nunique().sort_values(ascending=False)\n</code></pre>"},{"location":"pandas/google_sheets.html","title":"Google Sheets","text":""},{"location":"pandas/google_sheets.html#overview","title":"Overview","text":"<p>Medium article: Automate Google Sheets CRUD Automation</p> <p>This article covers setting up a Google Sheets API that will allow python to perform CRUD operations \u2014 Create, Read, Update, and Delete \u2014 directly from your python script to your google spreadsheet.</p> <p>This process is handy if you need to automate reports, sync live data, or managing spreadsheets programmatically, this tutorial will give you the tools to make your\u00a0Google Sheets dynamically update in real time\u00a0with Python!</p>"},{"location":"pandas/google_sheets.html#use-cases","title":"Use Cases","text":"<ul> <li>Automated reports and dashboards \u2014 Fetch and update live data for business reports.</li> <li>Stock and crypto price tracking \u2014 Automatically log and update real-time financial data.</li> <li>Inventory management \u2014 Keep track of product stock levels dynamically.</li> <li>Task and project management \u2014 Use Google Sheets as a simple task tracker with automatic updates.</li> <li>News and web scraping logs \u2014 Extract and store live news headlines or scraped web data.</li> <li>API data logging \u2014 Pull in weather, sales, or user analytics from APIs and store in Sheets.</li> <li>Student and employee records \u2014 Automate updates to attendance, grades, or work logs.</li> <li>Database alternative \u2014 Use Google Sheets as a lightweight database for simple applications.</li> <li>Form submission processing \u2014 Automate handling of Google Forms data in real-time.</li> </ul>"},{"location":"pandas/google_sheets.html#steps","title":"Steps","text":"<ol> <li>Go to Google Cloud Console</li> <li>gsheets-453804-c772b0c9403a.json</li> </ol>"},{"location":"pandas/google_sheets.html#implemented","title":"Implemented.","text":""},{"location":"pandas/list_function.html","title":"List Function","text":""},{"location":"pandas/numpy.html","title":"NumPy","text":"<p>Explore pandas functions that can also use with numpy</p> <ul> <li>Pandas for data manipulation and analysis or </li> <li>NumPy for numerical computing</li> </ul>"},{"location":"pandas/numpy.html#compare-pandas-and-numpy","title":"Compare Pandas and Numpy","text":""},{"location":"pandas/numpy.html#1-shift","title":"1. Shift","text":"<p>The pandas.Series.shift() function shifts the values of a particular position in a Series up or down by a specified number of periods. </p> <p>Pandas: Shifts the values of a series by a certain number of periods. NumPy Equivalent: You can use np.roll() to achieve a similar effect.</p> <p>Example <pre><code># Pandas\ndf['shifted'] = df['column'].shift(1)\n\n#  NumPy\ndf['shifted'] = np.roll(df['column'].values, 1)\n</code></pre></p>"},{"location":"pandas/numpy.html#2-diff","title":"2. Diff","text":"<p>pandas.Series.diff() is used to calculate the difference between consecutive elements in a given series.</p> <p>Interestingly, you can achieve the same result with NumPy using np.diff().</p> <p>Example <pre><code># Pandas\ndf['difference'] = df['column'].diff()\n# NumPy\ndf['difference'] = np.diff(df['column'], prepend=np.nan)\n</code></pre></p> <p>Output <pre><code>output\n   column  difference  difference_np\n0      30         NaN             NaN\n1      32         2.0             2.0\n2      35         3.0             3.0\n3      31        -4.0            -4.0\n4      29        -2.0            -2.0\n</code></pre></p>"},{"location":"pandas/numpy.html#3-apply","title":"3. Apply","text":"<p>pandas.DataFrame.apply() you can add two rows and form a new one as an answer.</p> <p>Pandas: Apply a function to each row or column. NumPy Equivalent: Use np.apply_along_axis() for similar functionality on NumPy arrays.</p> <p>Example</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}\ndf = pd.DataFrame(data)\n# Using Pandas\ndf['result'] = df.apply(lambda row: row['a'] + row['b'], axis=1)\n# Using NumPy\nresult = np.apply_along_axis(lambda row: row[0] + row[1], 1, df[['a', 'b']].valu\nprint(df)\nprint(\"NumPy Result:\", result)\n</code></pre> <p>Output <pre><code>   a  b  result\n0  1  4       5\n1  2  5       7\n2  3  6       9\nNumPy Result: [5 7 9]\n</code></pre></p>"},{"location":"pandas/numpy.html#4-rank","title":"4. Rank","text":"<p>pandas.Series.rank()</p> <p>Pandas: Computes the rank of each element in a Series. NumPy Equivalent: You can achieve similar functionality using np.argsort().</p> <p>Example: <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'column': [50, 20, 80, 60]}\ndf = pd.DataFrame(data)\n# Using Pandas\ndf['rank'] = df['column'].rank()\n# Using NumPy\nranks = np.argsort(np.argsort(df['column'].values))\ndf['numpy_rank'] = ranks + 1  # Adding 1 because ranks start from 1 in Pandas\nprint(df)\n</code></pre></p> <p>Output <pre><code>   column  rank  numpy_rank\n0      50   2.0           2\n1      20   1.0           1\n2      80   4.0           4\n3      60   3.0           3\n</code></pre></p>"},{"location":"pandas/numpy.html#5-isin","title":"5. IsIn","text":"<p>pandas.Series.isin() Pandas: Check if elements in a Series are in a given list or array. NumPy Equivalent: Use np.in1d() to perform this task.</p> <p>Example: <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {\n    'product': ['TV', 'Sofa', 'Laptop', 'Table', 'Shirt', 'Headphones', 'Shoes']\n    'category': ['Electronics', 'Furniture', 'Electronics', 'Furniture', 'Clothi\n}\ndf = pd.DataFrame(data)\n# Categories to check\ntarget_categories = ['Electronics', 'Furniture', 'Clothing']\n# Using Pandas\ndf['is_in'] = df['category'].isin(target_categories)\n# Using NumPy\ndf['is_in_np'] = np.in1d(df['category'], target_categories)\n# Display the DataFrame\nprint(df)\n</code></pre></p> <p>Output: <pre><code>      product    category  is_in  is_in_np\n0          TV  Electronics   True      True\n1        Sofa   Furniture    True      True\n2      Laptop  Electronics   True      True\n3       Table   Furniture    True      True\n4       Shirt   Clothing     True      True\n5  Headphones  Electronics   True      True\n6       Shoes     Apparel   False     False\n</code></pre></p>"},{"location":"pandas/numpy.html#6-cumsum","title":"6. CumSum","text":"<p>Pandas: Calculates the cumulative sum of the values in a series. NumPy Equivalent: Use np.cumsum() for cumulative summation.</p> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'column': [1, 2, 3, 4, 5]}\ndf = pd.DataFrame(data)\n# Using Pandas to calculate cumulative sum\ndf['cumsum_pandas'] = df['column'].cumsum()\n# Using NumPy to calculate cumulative sum\ndf['cumsum_numpy'] = np.cumsum(df['column'].values)\nprint(df)\n</code></pre> <p>Output: <pre><code>   column  cumsum_pandas  cumsum_numpy\n0       1              1             1\n1       2              3             3\n2       3              6             6\n3       4             10            10\n4       5             15            15\n</code></pre></p>"},{"location":"pandas/numpy.html#7-expanding","title":"7. Expanding","text":"<p>Now, we can achieve this using the pandas.Series.expanding() function, which expands a window over the data to compute cumulative statistics like the running mean.</p> <p>NumPy Equivalent: You can achieve this manually with np.cumsum() and computing the desired statistic over expanding windows.</p> <p>Example: <pre><code>import pandas as pd\nimport numpy as np\n# Sample DataFrame\ndata = {'column': [1, 2, 3, 4, 5]}\ndf = pd.DataFrame(data)\n# Using Pandas to calculate the expanding mean\ndf['expanding_mean'] = df['column'].expanding().mean()\n# Using NumPy to calculate the manual expanding mean\nexpanding_mean = np.cumsum(df['column'].values) / np.arange(1, len(df) + 1)\n# Adding the NumPy result to the DataFrame\ndf['expanding_mean_numpy'] = expanding_mean\n# Display the DataFrame\nprint(df)\n</code></pre></p> <p>Output: <pre><code>   column  expanding_mean  expanding_mean_numpy\n0       1             1.0                  1.0\n1       2             1.5                  1.5\n2       3             2.0                  2.0\n3       4             2.5                  2.5\n4       5             3.0                  3.0\n</code></pre></p>"},{"location":"pandas/numpy.html#8-pct_change","title":"8. Pct_Change","text":"<p>Pandas: Computes the percentage change between the current and prior element. NumPy Equivalent: Use a combination of NumPy array operations to calculate the percentage change.</p> <p>Example: <pre><code>import pandas as pd\nimport numpy as np\n# Sample sales data\ndata = {'day': ['Day 1', 'Day 2', 'Day 3', 'Day 4', 'Day 5'],\n        'sales': [100, 110, 150, 120, 130]}\n# Create DataFrame\ndf = pd.DataFrame(data)\n# Using Pandas to compute percentage change\ndf['pct_change_pandas'] = df['sales'].pct_change()\n# Using NumPy to compute percentage change manually\npct_change_numpy = np.diff(df['sales'].values) / df['sales'].values[:-1]\npct_change_numpy = np.insert(pct_change_numpy, 0, np.nan)  # Insert NaN at the s\n# Add the NumPy result to the DataFrame\ndf['pct_change_numpy'] = pct_change_numpy\n# Display the DataFrame\nprint(df)\n</code></pre></p> <p>Output <pre><code>     day  sales  pct_change_pandas  pct_change_numpy\n0  Day 1    100                NaN               NaN\n1  Day 2    110           0.100000          0.100000\n2  Day 3    150           0.363636          0.363636\n3  Day 4    120          -0.200000         -0.200000\n4  Day 5    130           0.083333          0.083333\n</code></pre></p>"},{"location":"pandas/numpy.html#9-fill-na","title":"9. Fill NA","text":"<p>Pandas: Fills NaN values with a specified value or method. NumPy Equivalent: Use np.where() to replace NaN values in a NumPy array.</p> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample data with NaN values\ndata = {'scores': [100, 200, np.nan, 300, np.nan]}\n# Create DataFrame\ndf = pd.DataFrame(data)\n# Using Pandas to fill NaN values\ndf['filled_pandas'] = df['scores'].fillna(0)\n# Using NumPy to fill NaN values manually\ndf['filled_numpy'] = np.where(np.isnan(df['scores'].values), 0, df['scores'].val\n# Display the DataFrame\nprint(df)\n\n\nOutput:\n~~~plain\n   scores  filled_pandas  filled_numpy\n0   100.0          100.0         100.0\n1   200.0          200.0         200.0\n2     NaN            0.0           0.0\n3   300.0          300.0         300.0\n4     NaN            0.0           0.0\n</code></pre>"},{"location":"pandas/numpy.html#10-drop-na","title":"10. Drop NA","text":"<p>The pandas.DataFrame.dropna() function removes rows (or columns) from a</p> <p>Pandas: Drops rows or columns with NaN values. NumPy Equivalent: Use ~np.isnan() to filter out rows containing NaN.</p> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\n# Sample data with NaN values\ndata = {'scores': [100, 200, np.nan, 300, 150]}\n# Create DataFrame\ndf = pd.DataFrame(data)\n# Using Pandas to drop rows with NaN values\ndf_cleaned_pandas = df.dropna()\n# Using NumPy to manually filter out rows with NaN values\ndf_cleaned_numpy = df[~np.isnan(df['scores'].values)]\n# Display the results\nprint(\"Original DataFrame:\\n\", df)\nprint(\"\\nPandas dropna():\\n\", df_cleaned_pandas)\nprint(\"\\nNumPy equivalent:\\n\", df_cleaned_numpy)\n</code></pre> <p>Output <pre><code>Original DataFrame:\n    scores\n0   100.0\n1   200.0\n2     NaN\n3   300.0\n4   150.0\nPandas dropna():\n    scores\n0   100.0\n1   200.0\n3   300.0\n4   150.0\nNumPy equivalent:\n    scores\n0   100.0\n1   200.0\n3   300.0\n4   150.0\n</code></pre></p>"},{"location":"pandas/numpy.html#11-value-counts","title":"11. Value Counts","text":"<p>Pandas: Counts the unique values in a series. NumPy Equivalent: Use np.unique() with return_counts=True to get unique values and their counts. Example: Responses: ['A', 'B', 'A', 'C', 'B', 'A', 'B'] <pre><code>import pandas as pd\nimport numpy as np\n# Sample data\ndata = {'responses': ['A', 'B', 'A', 'C', 'B', 'A', 'B']}\n# Create DataFrame\ndf = pd.DataFrame(data)\n# Using Pandas to count unique values\nvalue_counts_pandas = df['responses'].value_counts()\n# Using NumPy to count unique values manually\nunique, counts = np.unique(df['responses'].values, return_counts=True)\nvalue_counts_numpy = dict(zip(unique, counts))  # Combine unique values and coun\n# Display the results\nprint(\"Pandas value_counts():\\n\", value_counts_pandas)\nprint(\"\\nNumPy equivalent:\\n\", value_counts_numpy)\n</code></pre></p> <p>Output: <pre><code>Pandas value_counts():\n A    3\n B    3\n C    1\nName: responses, dtype: int64\nNumPy equivalent:\n {'A': 3, 'B': 3, 'C': 1}\n</code></pre></p>"},{"location":"pandas/profiling.html","title":"Profiling","text":"<p>TODO:  Use chatgpt to generate content</p> <ul> <li>Generate Reports Using Pandas Profiling, </li> <li>Deploy Using Streamlit Kaustubh Gupta 7 Last Updated : 23 Oct, 2024 Pandas library offers a wide range of functions, making it an indispensable tool for data manipulation that caters to almost every task. One convenient feature, often employed for gaining quick insights into a dataset, is the pandas describe function. This function gives users a descriptive statistical summary of all the features, helping them understand the data\u2019s overall characteristics. However, for a more comprehensive analysis, the pandas profiling Package is an additional valuable tool in the Pandas ecosystem. Pandas profiling is the solution to this problem. It offers report generation for the dataset with lots of features and customizations for the report generated. In this article, we will explore this library, look at all the features provided, and some advanced use cases and integrations that can be useful to create stunning reports out of the data frames! This article was published as a part of the Data Science Blogathon. Table of cWoen utsee ncotoskies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Accept all cookies Use necessary cookies Add MetaData Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Controlling parameters of the Report</li> <li>Integrations Widget  in Jupyter notebook</li> <li>How to Make it a Part of Streamlit App? Step 1: Install the streamlit_pandas_profiling Step 2: Create a Python file</li> <li>Conclusion Installation Like every other Python package, pandas profiling can be easily installed via the pip package manager: Copy Code pip install pandas-profiling It can also be installed via Conda package manager too: Copy Code conda env create -n pandas-profiling conda activate pandas-profiling conda install -c conda-forge pandas-profiling Dataset and Setup Now it\u2019s time to see how to start the Python pandas profiling library and generate the We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; report out of the data frames. First things first,  let\u2019s import a dataset for which we will Cookies Policy. be generatinSgh powro dfieleta rilesport. I am using the agriculture dataset which contains the State_name, District_name, Crop_year, Season, Crop, Area, and Production. Copy Code import pandPaesrs oansa lipzedd GenAI Learning Path 2025\u2728  Crafted Just for YOU! df = pd.read_csv(\"crops data.csv\") Before I discuss the Python pandas profiling, have a look at the pandas describe function output for the dataframe: Copy Code df.describe(include='all') (Notice that I have used the include parameter of the describe function set to \u201call\u201d which forces pandas to include all the data types of the dataset to be included in the summary. The string type values are accompanied by options such as unique, top, and frequency) Let\u2019s import the Python pandas profiling library: We use cookies essential for this site to function well. Please click to help us improve its Copy Code from pandasu_sperfuolnfeislsi wnigth  iadmdpiotiornta l Pcroookfieisl.e LReeaprno rabtout our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details To start profiling a dataframe, you have two ways:</li> <li>You can call the \u2018.profile_report()\u2019 function on pandas dataframe. This function is Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! not part of the pandas API but as soon as you import the profiling library, it adds this function to dataframe objects. You can pass the dataframe object to the profiling function and then call the function object created to start the generation of the profile. You will get the same output report in either of the methods. I am using the second method to generate the report for the imported agriculture dataset. Copy Code profile = ProfileReport(df) profile Animation Showing report generation Sections of the Report Now that the report is generated, let\u2019s explore all the sections of the report one by one. Overview This section consists of the 3 tabs: Overview, Warnings, and Reproduction. We use cookies essential for this site to function well. Please click to help us improve its The Overview generated by pandas profiling provides a comprehensive dataset usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. summary, encompassing various key statistics. It covers the fundamental Show details characteristics such as the Number of variables (features or columns of the data frame) and the Number of observations (rows of the data frame). Additionally, it sheds Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! light on data quality by revealing insights into Missing cells and their corresponding percentage, offering a quick assessment of the dataset\u2019s completeness. The Duplicate rows section provides information on the presence of identical rows, including the percentage of duplicate rows. As a holistic touch, the overview concludes with the total memory size, encapsulating the overall footprint of the dataset. Integrating pandas profiling seamlessly facilitates a profound understanding of these essential aspects, enhancing the efficiency of exploratory data analysis. The warnings tab contains any warnings related to cardinality, correlation with other variables, missing values, zeroes, skewness of the variables, and many others. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! The reproduction tab displays information related to the report generation. It shows the start and end times of the analysis, the time taken to generate the report, the software version of pandas profiling, and a configuration download option. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! We will discuss the configuration file in this article\u2019s advanced use case section. Variables This section of the report gives a detailed analysis of all the variables/columns/features of the dataset. The information presented varies depending upon the data type of variable. Let\u2019s break it down. Numeric Variables You get information about the distinct values, missing values, min-max, mean, and negative values count for numeric data type features. You also get small representation values in the form of a Histogram. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! The toggle button expands to the Statistics, Histogram, Common Values, and Extreme Values tabs. The statistics tab includes:</li> <li>Quantile statistics: Min-Max, percentiles, median, range, and IQR (Inter Quartile range)</li> <li>Descriptive statistics: Standard Deviation, Coefficient of variance, Kurtosis, mean, skewness, variance, and monotonicity. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! The histogram tab displays the frequency of variables or distribution of numeric data. The common values tab is basically value_counts of the variables presented as both counts and percentage frequency. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! String Variables For string-type variables, you get Distinct (unique) values, distinct percentages, missing missing percentages, memory size, and a horizontal bar presentation of all the unique values with count presentation. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details (It also reports any warnings associated with the variable irrespective of its data type) Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! The toggle button expands to the Overview, Categories, Words, and Characters tab.  The Overview tab displays the max-min median mean length, total characters, distinct characters, distinct categories, unique characters, and sample from the dataset for string type values. The categories tab displays a histogram and sometimes a pie chart of the feature\u2019s value counts. The table contains the value, count, and percentage frequency. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! The words and the characters tab does the same job as the categories tab in terms of presenting the data in tabular and histogram format. Still, it can go much deeper into the lower case, upper case, punctuation, special characters categories count too! Correlations Correlation describes the degree to which two variables move in coordination with one another. The pandas profiling python report provides five types of correlation coefficients: Pearson\u2019s r, Spearman\u2019s \u03c1, Kendall\u2019s \u03c4, Phik (\u03c6k), and Cram\u00e9r\u2019s V (\u03c6c). We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! You can also click the toggle button for details about the correlation coefficients. Missing values The report generated also contains visualizations for the missing values in the dataset. You get three types of plots: count, matrix, and dendrogram. The count plot is a basic bar plot with an x-axis as column names, and the length of the bar represents the We use cookies essential for this site to function well. Please click to help us improve its number of vausluefeulsne spsr weisthe anddt it(iownaitl hcoooukite sn. uLella rvna albuouets o)u.r  uSsiem ofi lcaorolkyie,s t ihn eou mr Parivtaricxy  Paonlicdy  &amp;the Cookies Policy. dendrogram are. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Sample This section displays the first and last 10 rows of the dataset. How to Save the Report? So far, you\u2019ve learned how to generate dataframe reports with a single line of code or function and explored the report\u2019s included features. You may want to export this analysis to an external file for integration with other applications or web publishing We use cookies essential for this site to function well. Please click to help us improve its Guess what?us eYfuolnue scsa wnith s aaddviteio ntahl icso orkeieps.o Lreta! rYn oabuo uct aounr  ussae vofe c otohkiiess  rine opuor rPtr ivinac y\u2013 Policy &amp; Cookies Policy. Show details</li> <li>HTML format</li> <li>JSON format The save function remains the same for any of the formats. Change the file extension Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! while saving. To save the report, call the \u201c.to_file()\u201d function on the profile object: Copy Code profile.to_file(\"Analysis.html\") profile.to_file(\"Analysis.json\") Advanced Usage The report generated by Pandas profiling Python is a complete analysis without any input from the user except the dataframe object. All the report elements are chosen automatically, and default values are preferred. There might be some elements in the report that you don\u2019t want to include, or you need to add your metadata for the final report. There comes the advanced usage of this library. You can control every aspect of your report by changing the default configurations. Let\u2019s see some of how you can customize your reports. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Add MetaData Cookies Policy. Show details You can add information such as \u201ctitle\u201d, \u201cdescription\u201d, \u201ccreator\u201d, \u201cauthor\u201d, \u201cURL\u201d, Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! \u201ccopyright_year\u201d, and  \u201ccopyright_holder\u201d. This information will appear in the dataset overview section. A new tab called \u201cdataset\u201d will be created for this metadata. To add this data to the report, use the dataset parameter in the ProfileReport function and pass this data as a dictionary: Copy Code profile = ProfileReport(df,                         title=\"Agriculture Data\",         dataset={         \"description\": \"This profiling report was generated for Analytics Vidhya Blog\",         \"copyright_holder\": \"Analytics Vidhya\",         \"copyright_year\": \"2021\",         \"url\": \"https://www.analyticsvidhya.com/blog/\",     },) profile You can also add information about the variables used in the dataset using the variables parameter. This takes in the dictionary with descriptions as the key and value as another dictionary with a key-value pair, where the key is the variable name and the value is the description of the variable. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Copy Code variables={ Cookies Policy.         \"descriptions\": { Show details             \"State_Name\": \"Name of the state\",             \"District_Name\": \"Name of district\",             \"Crop_Year\": \"Year when it was seeded\",             \"Season\": \"Crop year\",            P e\"rsCornoapli\"ze:d  \"GWehniAcI hLe acrrnoinpg  Pwaatsh  2s0e25e\u2728de dC?\"ra,fted Just for YOU!             \"Area\": \"How much area was allocated to the crop?\",             \"Production\": \"How much production?\",         }     } When you add this to your ProfileReport function, a separate tab will be created named \u201cVariables\u201d under the overview section: Controlling parameters of the Report Suppose you don\u2019t want to display all types of correlation coefficients. You can disable other coefficients by using the configuration for correlations. This is also a dictionary object and can be passed to the ProfileReport function: We use cookies essential for this site to function well. Please click to help us improve its Copy Code profile = ProfileReport(df, usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp;            C o o k ie s  P o l ic y.   title=\"Agriculture Data\",            S  h o w   d e t a i ls  correlations={                                         \"pearson\": {\"calculate\": True},                                         \"spearman\": {\"calculate\": False},                                         \"kendall\": {\"calculate\": False},                                         \"phi_k\": {\"calculate\": False},     }) Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Similarly, you can customize every report section, the HTML format, plots, and everything. \u201c Check out this page of the documentation for details. Integrations After making your reports stunning by configuring every aspect of it, you might want to publish it anyhow. You can export it to HTML format and upload it to the web. But there are some other methods to make your report stand out. Widget  in Jupyter notebook While running the panda profiling in your Jupyter notebooks, you will get the HTML rendered in the code cell only. This disturbs the experience of the user. You can make it act like a widget that is easily accessible and offers a compact view. To do this, simply call \u201c.to_widgets()\u201d on your profile object: We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! How to Make it a Part of Streamlit App? Yes! You can make this report as a part of a streamlit app, too. Streamlit is a powerful package that enables GUI web app building with minimal code. The applications are interactive and compatible with almost every device. You can make your reports as a part of the streamlit app by following this code: Step 1: Install the streamlit_pandas_profiling  Copy Code pip install streamlit-pandas-profiling Step 2: Create a Python file Create a python file and write code in this format: We use cookies essential for this site to function well. Please click to help us improve its Copy Code import panduasse fualnse spsd with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. import pandSash_opwr odfeitlaiilnsg import streamlit as st from streamlit_pandas_profiling import st_profile_report Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! from pandas_profiling import ProfileReport df = pd.read_csv(\"crops data.csv\", na_values=['=']) profile = ProfileReport(df,                         title=\"Agriculture Data\",         dataset={         \"description\": \"This profiling report was generated for Analytics Vidhya Blog\",         \"copyright_holder\": \"Analytics Vidhya\",         \"copyright_year\": \"2021\",         \"url\": \"https://www.analyticsvidhya.com/blog/\",     },     variables={         \"descriptions\": {             \"State_Name\": \"Name of the state\",             \"District_Name\": \"Name of district\",             \"Crop_Year\": \"Year when it was seeded\",             \"Season\": \"Crop year\",            W \"eC ursoep c\"o:o ki\"eWs heisscehn ticarl foopr  thwias ss ites etoe fduendct?io\"n, well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy.             \"Area\": \"How much area was allocated to the crop?\", Show details             \"Production\": \"How much production?\", Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU!         }     } ) st.title(\"Pandas Profiling in Streamlit!\") st.write(df) st_profile_report(profile) Step 3: Run your streamlit app In the terminal, type: streamlit run .py We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Exploratory Data Analysis Using Pandas Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Exploratory Data Analysis (EDA) is like exploring a new place. You start by looking around to understand what\u2019s there. Similarly, in EDA, you look at a dataset to see what\u2019s in it and what it can tell you. It\u2019s essentially the initial data exploration stage in data science, where you delve into the dataset statistics and examine its intricacies. Here\u2019s what you do during EDA: Look at the Numbers: You start by checking basic things like averages, ranges, and the spread of the numbers. Make Pictures: Instead of just staring at numbers, you make charts and graphs to show the data visually. It\u2019s like drawing a map of your exploration. Clean Up: Sometimes, data can be messy with missing pieces or weird values. So, you clean it up by filling in missing parts or removing the weird stuff. Create New Ideas: You might develop new ideas or ways to look at the data, like combining different parts or changing how you measure things. Find Connections: You try to see if different parts of the data are related. For example, if one thing goes up, does another also go up? Make Things Simple: If the data is too complicated, you might simplify it to see the big picture more clearly. Look at Time: If your data changes over time, you\u2019ll examine how it changes and whether there are any repeating patterns. We use cookies essential for this site to function well. Please click to help us improve its Test Ideuassef:u lFneisnsa wliltyh, a yddoituio ntael scoto ykioesu. rL eiadrne aabso utto o usr eusee  ioff  cthooekyie sm ina okure P rsiveacnys Peo liacyn &amp;d if the Cookies Policy. patterns you see are real or just random. Show details Overall, EDA helps you understand your data better before doing any fancy analysis or Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! making big conclusions. It\u2019s like exploring a map before going on a big adventure! Conclusion In this article, you are introduced to a new tool, \u201cPandas Profiling,\u201d a one-stop solution for generating reports out of the Pandas dataframe. We explore all the features of this tool, different sections, and their content. Then, we move on to saving the report generated. Later, we look at some of the advanced use cases of this library and finally integrate the Streamlit app to make the reports more promising and interactive. The media shown in this article are not owned by Analytics Vidhya and are used at the Author\u2019s discretion. Kaustubh Gupta Kaustubh Gupta is a skilled engineer with a B.Tech in Information Technology from Maharaja Agrasen Institute of Technology. With experience as a CS Analyst and Analyst Intern at Prodigal Technologies, Kaustubh excels in Python, SQL, Libraries, and various engineering tools. He has developed core components of product intent engines, created gold tables in Databricks, and built internal tools and dashboards using Streamlit and Tableau. Recognized as India\u2019s Top 5 Community Contributor 2023 by Analytics Vidhya, Kaustubh is also a prolific writer and mentor, contributing significantly to the tech community through speaking sessions and workshops. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Data Engineering Data Exploration Data Visualization Intermediate Listicle Machine Learning Project Python Python Streamlit Structured Data Technique Free Courses  4.7 Generative AI - A Way of Life Explore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics.  4.5 Getting Started with Large Language Models Master Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple. We use cookies e s4s.e6ntial for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Building LLM Applications using Prompt Engineering Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! This free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data.  4.8 Improving Real World RAG Systems: Key Challenges &amp; Practical Solutions Explore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications.  4.7 Microsoft Excel: Formulas &amp; Functions Master MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Responses From Readers What are your thoughts?... Submit reply Santosh Kesava Hi , This is really a informative post thank you for posting. My scenario is same but the only missing part is how to We uscer ceoaotkeie sa e tsasebn twiailt fho rc thoims spitlee ttoe f udnacttiaon a wnedll.  bPuletatsoen c lticok  dtoo hwenlpl ousa idm pinrove its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; excel or csv ? Please help. Thank you Santosh Kesava Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Niladri Chakraborty Nice article. I'm trying to run pandas profiling in my python 3.8.x version. But I'm getting error message saying PydanticImportError: BaseSettingshas been moved to thepydantic-settings package while I'm running from pandas_profiling import ProfileReport. Can you please guide me to resolve the issue? We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Frequently Asked Questions How to use pandas-profiling? A. To use pandas-profiling, you should first install it using pip. Then, import it into your Python script or Jupyter Notebook. Load your dataset with Pandas, create a ProfileReport object, and call its to_file() or to_widgets() methods to obtain a detailed analysis and visualization of your data. What is Pandas profiling? Why use pandas profiling? Q4. How to pip install pandas-profiling? Write for us  Write, captivate, and earn accolades and rewards for your work We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Reach Cao Goklioesb Paol lAicyu.dience Get ExpSehrot wF edeedtabialsck Build Your Brand &amp; Audience Cash In on Your Knowledge Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Join a Thriving Community Level Up Your Data Science Game Flagship Courses GenAI Pinnacle Program | AI/ML BlackBelt Courses Free Courses Generative AI | Large Language Models | Building LLM Applications using Prompt Engineering | Building Your first RAG System using LlamaIndex | Stability.AI | MidJourney | Building Production Ready RAG systems using LlamaIndex | Building LLMs for Code | Deep Learning | Python | Microsoft ExcWele| u Msea ccohokiniees  eLseseanrtniailn fogr |th Dis esicteis tioo fnun Tctrioene wse|ll .P Palenadsea csl icfok rto D haeltpa u As inmaplryosveis its| Ensemble usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Learning | NLP | NLP using Deep Learning | Neural Networks | Loan Prediction Practice Cookies Policy. Problem | Time Series Forecasting | Tableau | Business Analytics Show details Popular Categories Generative AI | Prompt Engineering | Generative AI Application | News | Technical Guides | AI Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! Tools | Interview Preparation | Research Papers | Success Stories | Quiz | Use Cases | Listicles Generative AI Tools and Techniques GANs | VAEs | Transformers | StyleGAN | Pix2Pix | Autoencoders | GPT | BERT | Word2Vec | LSTM | Attention Mechanisms | Diffusion Models | LLMs | SLMs | StyleGAN | Encoder Decoder Models | Prompt Engineering | LangChain | LlamaIndex | RAG | Fine-tuning | LangChain AI Agent | Multimodal Models | RNNs | DCGAN | ProGAN | Text-to-Image Models | DDPM | Document Question Answering | Imagen | T5 (Text-to-Text Transfer Transformer) | Seq2seq Models | WaveNet | Attention Is All You Need (Transformer Architecture) Popular GenAI Models Llama 3.1 | Llama 3 | Llama 2 | GPT 4o Mini | GPT 4o | GPT 3 | Claude 3 Haiku | Claude 3.5 Sonnet | Phi 3.5 | Phi 3 | Mistral Large 2 | Mistral NeMo | Mistral-7b | Gemini 1.5 Pro | Gemini Flash 1.5 | Bedrock | Vertex AI | DALL.E | Midjourney | Stable Diffusion Data Science Tools and Techniques Python | R | SQL | Jupyter Notebooks | TensorFlow | Scikit-learn | PyTorch | Tableau | Apache Spark | Matplotlib | Seaborn | Pandas | Hadoop | Docker | Git | Keras | Apache Kafka | AWS | NLP | Random Forest | Computer Vision | Data Visualization | Data Exploration | Big Data | Common Machine Learning Algorithms | Machine Learning Company Discover About Us Blogs Contact Us Expert Sessions We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Careers Learning Paths Cookies Policy. Show details Comprehensive Guides Learn Engage Free Courses Community Personalized GenAI Learning Path 2025\u2728  Crafted Just for YOU! AI&amp;ML Program Hackathons GenAI Program Events Agentic AI Program Podcasts Contribute Enterprise Become an Author Our Offerings Become a Speaker Trainings Become a Mentor Data Culture Become an Instructor AI Newsletter Terms &amp; conditions   Refund Policy   Privacy Policy   Cookies Policy  \u00a9 Analytics Vidhya 2025.All rights reserved. We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy &amp; Cookies Policy. Show details</li> </ul>"},{"location":"pandas/quick_examples.html","title":"Quick Examples","text":""},{"location":"pandas/quick_examples.html#example-code","title":"Example Code","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n# 1. Load Data with read_csv\ndf = pd.read_csv('data.csv', usecols=['Column1', 'Column2'])\n\n# 2. View the First Few Rows with head()\ndf.head()\n\n# 3. Check missing values\ndf.isna().sum()\n\n# 4. Remove duplicates\ndf.drop_duplicates(inplace=True)\n\n# 5. Rename columns\ndf.rename(columns={'old_name':'new_name'}, inplace=True)\n\n# 6. Convert Data Types with astype()\ndf['Column1'] = df['Column1'].astype(float)\n\n# 7. Handle Missing Data with fillna()\ndf.fillna(method='ffill', inplace=True)\n\n# 8. Set a Column as the Index\ndf.set_index('Column1', inplace=True)\n\n# 9. Use groupby() for Aggregation\ndf.groupby('Category').agg({'Value': 'sum'})\n\n# 10. Sort Values with sort_values()\ndf.sort_values(by='Column1', ascending=False)\n\n# 11. Use apply() for Custom Functions\ndf['new_column'] = df['Column1'].apply(lambda x: x * 2)\n\n# 12. Filter Rows Based on Conditions\ndf_filtered = df[df['Column1'] &gt; 10]\n\n# 13. Use query() for More Complex Filters\ndf.query('Column1 &gt; 10 and Column2 == \"Yes\"')\n\n# 14. Concatenate DataFrames with concat()\ndf_combined = pd.concat([df1, df2], axis=0)\n\n# 15. Merge DataFrames with merge()\ndf_merged = pd.merge(df1, df2, on='ID')\n\n# 16. Reset Index with reset_index()\ndf.reset_index(drop=True, inplace=True)\n\n# 17. Export Data to Excel with to_excel()\ndf.to_excel('output.xlsx', index=False)\n\n# 18. Save dataframe to CSV\ndf.to_csv('output.csv', index=False)\n\n# 19. Quickly Get Descriptive Statistics with describe()\ndf.describe()\n\n# 20. Lambda Functions on Columns\ndf['new_column'] = df.apply(lambda row: row['Column1'] + row['Column2'], axis=1)\n\n# 21. Drop Unwanted Columns with drop()\ndf.drop(columns=['Column1', 'Column2'], inplace=True)\n\n# 22. Random sampling \u2014 use sample()\ndf_sample = df.sample(n=5)\n\n# 23. Extract Year, Month, or Day from DateTime Columns\ndf['year'] = df['DateColumn'].dt.year\ndf['month'] = df['DateColumn'].dt.month\n\n# 24. Convert Column to Categorical Type\ndf['Category'] = df['Category'].astype('category')\n\n# 25. Find Duplicates Across Specific Columns\ndf[df.duplicated(subset=['Column1', 'Column2'])]\n\n# 26. Use shift() for Lagging Data\ndf['lagged_column'] = df['Value'].shift(1)\n\n# 27. Pivot Data with pivot_table()\ndf_pivot = df.pivot_table(values='Value', index='Category', columns='Region', ag\n\n# 28. Plot with Pandas\u2019 Built-In Plotting\nYou can also plot quick plots with pandas integrated with Matplotlib.\ndf['Value'].plot(kind='line')\n\n# 29. Working with Strings on the Columns\ndf['name'] = df['name'].str.lower()\n\n# 30. Make a New Column Using a Condition\nimport numpy as np\ndf['NewColumn'] = np.where(df['Value'] &gt; 10, 'High', 'Low')\n\n# 31. Using nunique() for Unique Values Count\ndf['Category'].nunique()\n\n# 32. Get Column Data Types with dtypes\ndf.dtypes\n\n# 33. Find Correlations with corr()\ndf.corr()\n\n# 34. Change Display Options\npd.set_option('display.max_rows', 100)\n\n35. Efficient Row-wise Operations\nfor idx, row in df.iterrows():\n    print(row['Column1'])\n</code></pre>"},{"location":"pandas/quick_examples.html#conclusion","title":"Conclusion","text":""},{"location":"pandas/select_filter.html","title":"Select and Filter","text":""},{"location":"pandas/select_filter.html#selecting-and-filtering-data","title":"Selecting and Filtering Data","text":"<p>Selecting and filtering data are fundamental operations when working with Pandas DataFrames. Pandas provides multiple methods for selecting specific rows and columns based on labels, positions, and conditions.</p>"},{"location":"pandas/select_filter.html#1-selecting-data","title":"1. Selecting Data","text":""},{"location":"pandas/select_filter.html#11-selecting-columns","title":"1.1 Selecting Columns","text":"<ul> <li>Using column names: <pre><code>import pandas as pd\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    \"Age\": [25, 30, 35, 40],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n})\n\n# Select a single column\nprint(df[\"Name\"])\n\n# Select multiple columns\nprint(df[[\"Name\", \"City\"]])\n</code></pre></li> </ul>"},{"location":"pandas/select_filter.html#12-selecting-rows","title":"1.2 Selecting Rows","text":"<ul> <li> <p>Using <code>.loc[]</code> (label-based indexing): <pre><code># Select a row by index label\nprint(df.loc[0])  # First row\n</code></pre></p> </li> <li> <p>Using <code>.iloc[]</code> (position-based indexing): <pre><code># Select a row by position\nprint(df.iloc[2])  # Third row\n</code></pre></p> </li> <li> <p>Selecting multiple rows: <pre><code>print(df.loc[0:2])  # Select rows 0 to 2\nprint(df.iloc[0:2])  # Select first two rows\n</code></pre></p> </li> </ul>"},{"location":"pandas/select_filter.html#2-filtering-data","title":"2. Filtering Data","text":"<p>Filtering is used to extract subsets of a DataFrame that meet specific conditions.</p>"},{"location":"pandas/select_filter.html#21-filtering-rows-based-on-condition","title":"2.1 Filtering Rows Based on Condition","text":"<pre><code># Select rows where Age &gt; 30\nprint(df[df[\"Age\"] &gt; 30])\n</code></pre>"},{"location":"pandas/select_filter.html#22-filtering-with-multiple-conditions","title":"2.2 Filtering with Multiple Conditions","text":"<p>Using <code>&amp;</code> (AND condition): <pre><code># Select rows where Age &gt; 30 and City is \"Chicago\"\nprint(df[(df[\"Age\"] &gt; 30) &amp; (df[\"City\"] == \"Chicago\")])\n</code></pre></p> <p>Using <code>|</code> (OR condition): <pre><code># Select rows where Age &lt; 30 or City is \"Houston\"\nprint(df[(df[\"Age\"] &lt; 30) | (df[\"City\"] == \"Houston\")])\n</code></pre></p> <p>Using <code>.isin()</code> for filtering specific values: <pre><code># Select rows where City is in [\"New York\", \"Houston\"]\nprint(df[df[\"City\"].isin([\"New York\", \"Houston\"])])\n</code></pre></p> <p>Using <code>.between()</code> for range-based filtering: <pre><code># Select rows where Age is between 25 and 35\nprint(df[df[\"Age\"].between(25, 35)])\n</code></pre></p> <p>Using <code>.query()</code> for SQL-like filtering: <pre><code>print(df.query(\"Age &gt; 30 and City == 'Chicago'\"))\n</code></pre></p>"},{"location":"pandas/select_filter.html#3-selecting-specific-rows-and-columns","title":"3. Selecting Specific Rows and Columns","text":"<pre><code># Select specific rows and columns\nprint(df.loc[df[\"Age\"] &gt; 30, [\"Name\", \"City\"]])\n</code></pre> <pre><code># Select first two rows and first two columns\nprint(df.iloc[0:2, 0:2])\n</code></pre>"},{"location":"pandas/select_filter.html#mini-tutorial-filtering-a-real-dataset","title":"Mini Tutorial: Filtering a Real Dataset","text":"<p>Let's assume we have a dataset <code>data.csv</code> with employee information:</p>"},{"location":"pandas/select_filter.html#step-1-load-the-dataset","title":"Step 1: Load the Dataset","text":"<pre><code>df = pd.read_csv(\"data.csv\")\n</code></pre>"},{"location":"pandas/select_filter.html#step-2-explore-the-data","title":"Step 2: Explore the Data","text":"<pre><code>print(df.head())  # View the first few rows\nprint(df.info())  # Get summary information\n</code></pre>"},{"location":"pandas/select_filter.html#step-3-apply-filtering","title":"Step 3: Apply Filtering","text":"<pre><code># Employees older than 40\nolder_employees = df[df[\"Age\"] &gt; 40]\n\n# Employees in the IT department\nit_employees = df[df[\"Department\"] == \"IT\"]\n\n# Employees with salary between 50K and 100K\nmid_salary_employees = df[df[\"Salary\"].between(50000, 100000)]\n\n# IT employees in New York\nny_it_employees = df[(df[\"Department\"] == \"IT\") &amp; (df[\"City\"] == \"New York\")]\n</code></pre>"},{"location":"pandas/select_filter.html#step-4-save-the-filtered-data","title":"Step 4: Save the Filtered Data","text":"<pre><code>ny_it_employees.to_csv(\"filtered_data.csv\", index=False)\n</code></pre>"},{"location":"pandas/select_filter.html#conclusion","title":"Conclusion","text":"<ul> <li>Use <code>.loc[]</code> and <code>.iloc[]</code> for row and column selection.</li> <li>Use conditional filtering with boolean operators (<code>&amp;</code>, <code>|</code>).</li> <li>Use <code>.query()</code> for SQL-like filtering.</li> <li>Use <code>.isin()</code> and <code>.between()</code> for specialized filtering.</li> </ul> <p>Would you like an example using PandasGUI for interactive filtering? \ud83d\ude80</p>"},{"location":"pandas/skimpy.html","title":"Skimpy","text":"<p>Here\u2019s what Skimpy offers: Data Shape: Shows the number of rows and columns. Column Data Types: Groups your columns by data type for clarity. Summary Statistics: Includes mean, median, and other key stats. Missing Values: Highlights missing data for each column. Visual Insights: Offers distribution charts to spot patterns quickly.</p>"},{"location":"pandas/skimpy.html#implementation","title":"Implementation","text":"<p>Install <pre><code>pip install polars==0.18.4 \npip install summarytools \npip install skimpy\n</code></pre></p> <p>Example</p> <pre><code>import polars as pl\nimport pandas as pd\nimport seaborn as sns\nfrom summarytools import dfSummary\nfrom skimpy import skim\ndf_pd = sns.load_dataset('iris')\ndf_pl = pl.from_pandas(df_pd)\nskim(df_pd)\n\nOpeninapp\nOpen in app\nskim(df_pl) # works with Polars DataFrame\nSearch Write\ndfSummary(df_pd)\n</code></pre> <p>Illustration: </p> <pre><code>from skimpy import skim\nimport pandas as pd\n# Create a sample DataFrame\ndata = {'Age': [25, 30, 35, 40, None], \n        'Salary': [50000, 60000, 70000, 80000, 90000], \n        'Department': ['HR', 'IT', 'Finance', 'IT', 'HR']}\ndf = pd.DataFrame(data)\n\n# Generate a summary\nskim(df)\nWhen you run this code in a Jupyter Notebook, Skimpy creates a beautiful,\nstructured report that\u2019s way better than the plain output of df.describe().\nBonus: Skimpy also works with Polars, which is a fast and efficient\nalternative to Pandas for large datasets.\nHere\u2019s a quick example of how to use Skimpy to summarize a dataset. We\u2019ll\ncreate a sample dataset and use Skimpy to generate a comprehensive\nsummary.\nStep 1: Install Skimpy\nFirst, ensure that you have the Skimpy library installed. You can install it\nusing pip:\npip install skimpy\nStep 2: Import Libraries and Create a Sample Dataset\nWe\u2019ll use Pandas to create a DataFrame, then summarize it with Skimpy.\nfrom skimpy import skim\nimport pandas as pd\n# Create a sample dataset\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, None],\n    'Salary': [50000, 60000, 70000, 80000, 90000],\n    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR'],\n    'Joining Date': ['2020-01-15', '2019-06-20', '2021-03-10', '2018-12-25', '20\n}\n# Convert Joining Date to datetime\ndf = pd.DataFrame(data)\ndf['Joining Date'] = pd.to_datetime(df['Joining Date'])\nStep 3: Generate the Summary\nUsing Skimpy is straightforward. Pass the DataFrame to the skim() function.\n# Generate a summary of the dataset\nskim(df)\n</code></pre> <p>What You\u2019ll Get: <pre><code>The output will include:\nGeneral Overview: Number of rows, columns, and missing values.\nData Types: Organized by type (e.g., numeric, categorical).\nStatistics: Mean, median, min, max, and standard deviation for numeric\ncolumns.\nUnique Values: For categorical columns.\nDistribution Insights: Charts for numeric columns (when supported in\nthe environment).\n</code></pre></p> <pre><code>import polars as pl\nfrom skimpy import skim\n# Create a Polars DataFrame\ndata = pl.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, None],\n    'Salary': [50000, 60000, 70000, 80000, 90000],\n    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR'],\n    'Joining Date': ['2020-01-15', '2019-06-20', '2021-03-10', '2018-12-25', '20\n})\n# Summarize the dataset\nskim(data)\n</code></pre>"},{"location":"pandas/summarizer.html","title":"SummaryTools","text":"<p>SummaryTools is another library that provides a detailed overview of your dataset. It\u2019s similar to Skimpy but adds a few extra features, like: Collapsible Summaries: Perfect for when you have large datasets and want a clean overview. Tabbed Summaries: Makes it easy to switch between different views of your data.</p>"},{"location":"pandas/summarizer.html#implamentation","title":"Implamentation","text":"<p>Here\u2019s how to use SummaryTools:</p> <pre><code>from summarytools import dfSummary\nimport pandas as pd\n# Create a sample DataFrame\ndata = {'Age': [25, 30, 35, 40, None], \n        'Salary': [50000, 60000, 70000, 80000, 90000], \n        'Department': ['HR', 'IT', 'Finance', 'IT', 'HR']}\ndf = pd.DataFrame(data)\n# Generate a collapsible summary\nsummary = dfSummary(df)\nsummary.to_notebook()  # Use this to display the report in Jupyter Notebook\n</code></pre> <pre><code>from summarytools import dfSummary\nimport pandas as pd\n# Create a sample dataset\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, None],\n    'Salary': [50000, 60000, 70000, 80000, 90000],\n    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR'],\n    'Joining Date': ['2020-01-15', '2019-06-20', '2021-03-10', '2018-12-25', '20\n}\ndf = pd.DataFrame(data)\ndf['Joining Date'] = pd.to_datetime(df['Joining Date'])  # Convert Joining Date \n\n# Step 3: Generate a Summary with SummaryTools\nUse the dfSummary function to create a summary and display it in a Jupyter\nNotebook.\n# Generate a summary of the dataset\nsummary = dfSummary(df) \n# Display the summary in a Jupyter Notebook\nsummary.to_notebook()\n</code></pre> <pre><code>What the Summary Includes\nOverview: Number of rows, columns, and missing values.\nColumn-Level Insights:\nData type.\nMean, median, and standard deviation for numeric columns.\nUnique values for categorical columns.\nDistribution charts (where applicable).\nInteractive Features:\nCollapsible sections for large datasets.\nTabbed views for switching between summaries.\n</code></pre> <p>Example</p> <pre><code>Example Output\nGeneral Overview:\nRows: 5\nColumns: 5\nMissing Values: Age (1 missing)\nColumn Details:\nAdvanced Features\nSave the Summary to HTML\nYou can save the summary report as an HTML file for sharing:\nsummary.to_html('dataset_summary.html')\nExport as JSON or CSV\nYou can export the data insights for programmatic use:\nsummary.to_json('dataset_summary.json') \nsummary.to_csv('dataset_summary.csv')\n</code></pre>"},{"location":"python/00_Contents.html","title":"Contents","text":"<p>Welcome to this comprehensive Python guide, specifically tailored for data scientists, machine learning experts, and predictive modeling specialists. This resource aims to equip you with best practices, detailed examples, and practical use cases to streamline your data science projects and enhance your productivity. Whether you are a seasoned professional or a beginner looking to deepen your expertise, you'll find structured, actionable information to elevate your skills in Python programming, data analysis, and predictive modeling.</p> <p>Chapters</p> <ul> <li>01 Overview</li> <li>02 Setup &amp; Installation</li> <li>03 Core Concepts</li> <li>04 Advanced Concepts</li> <li>05 Syntax and Variables</li> <li>06 Control Flow</li> <li>07 Data Structures</li> <li>08 Functions and Modules</li> <li>09 Object-Oriented Programming</li> <li>10 Python Standard Library</li> <li>11 File Handling and I/O</li> <li>12 Exception Handling and Debugging</li> <li>13 Web Development</li> <li>14 Data Science</li> <li>15 Machine Learning and AI</li> <li>16 Predictive Modeling</li> <li>17 Databases</li> </ul> <p>Future - 15 \ud83c\udfae Game Development (Pygame, Godot) - 16 \ud83d\udd12 Cybersecurity &amp; Ethical Hacking - 17 \ud83e\udd16 Automation &amp; Scripting - 18 \ud83d\udd78\ufe0f Web Scraping (BeautifulSoup, Scrapy) - 19 \ud83c\udfa8 GUI Development (Tkinter, PyQt) - 20 \ud83d\ude80 Performance Optimization &amp; Best Practices - 21 \ud83c\udfed Working with Databases (SQLite, PostgreSQL, MySQL) - 22 \ud83c\udfd7\ufe0f API Development &amp; RESTful Services - 23 \ud83c\udf0d Deployment &amp; Cloud (AWS, Docker, Heroku) - 24 \ud83d\udce6 Virtual Environments &amp; Package Management - 25 \ud83d\udcdd Testing &amp; Debugging (unittest, pytest) - 26 \u23f3 Concurrency &amp; Parallel Processing - 27 \u2699\ufe0f DevOps &amp; CI/CD for Python Projects - 28 \ud83d\udd04 Functional Programming in Python - 29 \ud83e\uddea Scientific Computing (SciPy, SymPy) - 30 \ud83d\udcdc Writing Pythonic Code &amp; Best Practices</p>"},{"location":"python/00_Contents_Jazzy.html","title":"Python for Data Science &amp; Predictive Modeling","text":"docs/python/images/python_logo.jpg    Welcome! <p>This comprehensive Python guide is specifically tailored for data scientists, machine learning experts, and predictive modeling specialists. Learn best practices, detailed examples, and practical use cases to streamline your projects and boost your productivity.</p> Get Started"},{"location":"python/00_Contents_Jazzy.html#chapters","title":"\ud83d\udcd6 Chapters","text":"01 Overview 02 Setup &amp; Installation 03 Core Concepts 04 Advanced Concepts 05 Syntax and Variables 06 Control Flow 07 Data Structures 08 Functions and Modules 09 Object-Oriented Programming 10 Python Standard Library 11 File Handling and I/O 12 Exception Handling &amp; Debugging 13 Web Development 14 Data Science 15 Machine Learning and AI 16 Predictive Modeling 17 Databases    Ready to dive in? Start your Python journey now!"},{"location":"python/00_landing.html","title":"Welcome","text":"<p>This comprehensive Python guide is specifically tailored for data scientists, machine learning experts, and predictive modeling specialists. Learn best practices, detailed examples, and practical use cases to streamline your projects and boost your productivity.</p> Get Started"},{"location":"python/01_Overview.html","title":"01 \ud83d\udcd8 Overview","text":"<p>Python is a high-level, interpreted programming language known for its simplicity and readability. Created by   Guido van Rossum   and first released in 1991, Python has become one of the most popular languages due to its ease of use, vast ecosystem, and broad applicability.</p> <p>Python supports multiple programming paradigms, including   procedural, object-oriented, and functional programming  , making it a versatile choice for a variety of applications, such as   web development, data science, artificial intelligence, automation, and scientific computing  .</p>"},{"location":"python/01_Overview.html#features-of-python","title":"Features of Python","text":""},{"location":"python/01_Overview.html#1-simple-readable-syntax","title":"1. Simple &amp; Readable Syntax","text":"<ul> <li>Python\u2019s syntax is designed to be   easy to read and write  , reducing the learning curve for beginners.</li> <li>Code readability is emphasized with   indentation-based structuring   (instead of braces <code>{}</code> like in C/C++ or Java).</li> </ul>"},{"location":"python/01_Overview.html#2-interpreted-language","title":"2. Interpreted Language","text":"<ul> <li>Python is an   interpreted language  , meaning code is executed   line by line   without requiring compilation.</li> <li>This makes debugging easier but may affect execution speed compared to compiled languages.</li> </ul>"},{"location":"python/01_Overview.html#3-dynamically-typed","title":"3. Dynamically Typed","text":"<ul> <li>You   don\u2019t need to declare data types   explicitly; Python automatically detects them at runtime.</li> </ul> <pre><code>x = 10  # Integer\ny = \"Hello\"  # String\nz = 3.14  # Float\n</code></pre>"},{"location":"python/01_Overview.html#4-object-oriented-functional-programming","title":"4. Object-Oriented &amp; Functional Programming","text":"<ul> <li>Supports   object-oriented programming (OOP)  , allowing for encapsulation, inheritance, and polymorphism.</li> <li>Also supports   functional programming  , allowing the use of   map, filter, lambda functions, and higher-order functions  .</li> </ul>"},{"location":"python/01_Overview.html#5-cross-platform-compatibility","title":"5. Cross-Platform Compatibility","text":"<ul> <li>Python is   portable   and can run on   Windows, macOS, Linux, and even embedded systems   without modification.</li> </ul>"},{"location":"python/01_Overview.html#6-large-standard-library","title":"6. Large Standard Library","text":"<ul> <li>Python comes with a   rich standard library   that provides modules for file handling, networking, regular expressions, data structures, and more.</li> </ul> <pre><code>import math  # Using the built-in math module\nprint(math.sqrt(16))  # Output: 4.0\n</code></pre>"},{"location":"python/01_Overview.html#7-extensive-third-party-libraries","title":"7. Extensive Third-Party Libraries","text":"<ul> <li>Python has a vast ecosystem of libraries, such as:</li> <li>NumPy, Pandas, Matplotlib   (for Data Science)</li> <li>TensorFlow, PyTorch, Scikit-learn   (for Machine Learning &amp; AI)</li> <li>Flask, Django, FastAPI   (for Web Development)</li> <li>Requests, BeautifulSoup, Scrapy   (for Web Scraping)</li> <li>PyQt, Tkinter   (for GUI Development)</li> </ul>"},{"location":"python/01_Overview.html#8-automatic-memory-management","title":"8. Automatic Memory Management","text":"<ul> <li>Python handles memory allocation and deallocation   automatically   using   Garbage Collection (GC)  .</li> </ul>"},{"location":"python/01_Overview.html#9-multi-purpose-language","title":"9. Multi-Purpose Language","text":"<ul> <li>Used for:</li> <li>Web Development  </li> <li>Data Science &amp; Analytics  </li> <li>Machine Learning &amp; AI  </li> <li>Automation &amp; Scripting  </li> <li>Cybersecurity  </li> <li>Game Development  </li> <li>Embedded Systems (MicroPython, Raspberry Pi)  </li> </ul>"},{"location":"python/01_Overview.html#operating-systems","title":"Operating Systems","text":"<p>Python is a highly portable language that runs on a wide variety of operating systems including:</p> <ul> <li>Windows \u2013 Supports Windows 10, 11, and older versions (7, 8, Server editions).</li> <li>macOS \u2013 Available on Intel and Apple Silicon (M1, M2, M3 chips).</li> <li>Linux \u2013 Supports major distributions:</li> <li>Ubuntu</li> <li>Debian</li> <li>Fedora</li> <li>CentOS</li> <li>Red Hat Enterprise Linux (RHEL)</li> <li>Arch Linux</li> <li>openSUSE</li> <li>Manjaro, etc.</li> <li>Unix-based OS:</li> <li>FreeBSD</li> <li>OpenBSD</li> <li>NetBSD</li> <li>Solaris</li> <li>AIX (IBM Unix)</li> <li>Android \u2013 Python can run via Termux or custom builds.</li> <li>iOS/iPadOS \u2013 Python can be used via apps like Pythonista or Pyto.</li> </ul>"},{"location":"python/01_Overview.html#platforms","title":"Platforms","text":"<p>Platforms include:</p> <ul> <li>x86 (32-bit and 64-bit) \u2013 Common on Windows, Linux, and older macOS systems.</li> <li>ARM (32-bit and 64-bit) \u2013 Used in Raspberry Pi, Android devices, and Apple Silicon (via native builds).</li> <li>RISC-V \u2013 Growing support for open-source hardware.</li> <li>Web (Browser-based execution via Pyodide or Brython).</li> <li>Embedded Systems (Microcontrollers like Raspberry Pi Pico, ESP32 using MicroPython or CircuitPython).</li> <li>Mainframes (IBM z/OS supports Python for enterprise applications).</li> <li>Cloud Platforms \u2013 Runs on AWS, Azure, Google Cloud, and other cloud environments.</li> <li>Docker &amp; Containers \u2013 Python is widely used in containerized environments.</li> <li>Virtual Machines \u2013 Can run inside VMs like VirtualBox, VMware, and Hyper-V.</li> </ul> <p>Python\u2019s versatility ensures it can run on almost any modern computing environment.</p>"},{"location":"python/01_Overview.html#conclusion","title":"Conclusion","text":"<p>Python is a powerful, easy-to-learn language with a vast ecosystem, making it suitable for beginners and professionals alike. Its   simplicity, flexibility, and extensive libraries   make it a top choice for   AI, web development, data science, and automation  . \ud83d\ude80</p>"},{"location":"python/02_Setup_Installation.html","title":"02 \ud83d\udee0\ufe0f Setup and Installation","text":""},{"location":"python/02_Setup_Installation.html#environment-setup","title":"Environment Setup","text":""},{"location":"python/02_Setup_Installation.html#python-installation","title":"Python Installation","text":"<p>Setting up Python correctly is essential. We recommend downloading the latest stable Python version from the official Python website. Ensure you select the appropriate version compatible with your operating system (Windows, macOS, Linux). Follow the installer instructions, selecting the option to add Python to your PATH environment variable.</p> <p>Verify your installation by running:</p> <pre><code>python --version\n</code></pre>"},{"location":"python/02_Setup_Installation.html#virtual-environment","title":"Virtual Environment","text":"<p>Using virtual environments helps isolate your project dependencies and avoids conflicts.</p>"},{"location":"python/02_Setup_Installation.html#pip","title":"pip","text":"<p><code>pip</code> is Python's package installer, allowing easy management of packages. It comes bundled with Python 3 by default. Verify pip installation by running:</p> <pre><code>pip --version\n</code></pre>"},{"location":"python/02_Setup_Installation.html#virtual-environment-venv","title":"Virtual Environment (venv)","text":"<p>Virtual environments isolate project dependencies. Set up a virtual environment with Python's built-in <code>venv</code>:</p>"},{"location":"python/02_Setup_Installation.html#creating-and-activating-a-virtual-environment","title":"Creating and Activating a Virtual Environment","text":"<pre><code>python -m venv myenv\n\n# Activate the environment\n# On Windows:\nmyenv\\Scripts\\activate\n\n# On macOS/Linux\nsource myenv/bin/activate\n</code></pre> <p>VSCode</p> <p>In VSCode the default library when Creating Environment is .venv instead of venv</p>"},{"location":"python/02_Setup_Installation.html#installing-packages","title":"Installing Packages","text":"<p>Once activated, install packages using:</p> <pre><code>pip install numpy pandas matplotlib\n</code></pre>"},{"location":"python/02_Setup_Installation.html#integrated-development-environment","title":"Integrated Development Environment","text":"<p>Choosing the right Integrated Development Environment (IDE) is crucial for efficiency.</p>"},{"location":"python/02_Setup_Installation.html#idle","title":"IDLE","text":"<p>IDLE comes bundled with Python and is suitable for basic scripting and quick experiments.</p> <ul> <li>Launch IDLE from your Python installation.</li> <li>Provides a straightforward interactive shell and simple editor.</li> <li>Ideal for beginners or small tasks.</li> </ul>"},{"location":"python/02_Setup_Installation.html#vscode","title":"VSCode","text":"<ul> <li>Highly recommended for professional development.</li> <li>Feature-rich editor with extensive Python support through extensions.</li> <li>Easy integration with Jupyter Notebooks, Git, and debugging tools.</li> <li>Installation: VSCode.</li> </ul>"},{"location":"python/02_Setup_Installation.html#pycharm","title":"PyCharm","text":"<p>A powerful, dedicated Python IDE ideal for larger, complex projects.</p> <ul> <li>Excellent code completion, debugging, and version control integration.</li> <li>Offers Community (free) and Professional (paid) editions.</li> </ul>"},{"location":"python/02_Setup_Installation.html#jupyterlab","title":"JupyterLab","text":"<p>Web-based IDE highly popular among data scientists.</p> <ul> <li>Combines notebooks, terminal, text editors, and visualization in one interface.</li> <li>Perfect for interactive data exploration, visualization, and documentation.</li> <li>Install via pip:</li> </ul> <pre><code>pip install jupyterlab\n</code></pre> <p>Launch by running:</p> <pre><code>jupyter lab\n</code></pre> <p>This setup ensures a solid foundation to maximize productivity in your Python-based projects.</p> <p>Bug</p> <p>Add topic Python CLI</p>"},{"location":"python/03_Core_Concepts.html","title":"03 \u2699\ufe0f Core Concepts","text":""},{"location":"python/03_Core_Concepts.html#basic","title":"Basic","text":""},{"location":"python/03_Core_Concepts.html#1-variables-data-types","title":"1. Variables &amp; Data Types","text":"<p>Python supports multiple data types:</p> <pre><code>x = 10         # Integer\ny = 3.14       # Float\nz = \"Python\"   # String\na = True       # Boolean\nb = [1, 2, 3]  # List\nc = (4, 5, 6)  # Tuple\nd = {\"key\": \"value\"}  # Dictionary\n</code></pre>"},{"location":"python/03_Core_Concepts.html#2-conditional-statements","title":"2. Conditional Statements","text":"<pre><code>x = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelif x == 5:\n    print(\"x is 5\")\nelse:\n    print(\"x is less than 5\")\n</code></pre>"},{"location":"python/03_Core_Concepts.html#3-loops-for-while","title":"3. Loops (For &amp; While)","text":"<pre><code># For loop\nfor i in range(5):\n    print(i)\n\n# While loop\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1\n</code></pre>"},{"location":"python/03_Core_Concepts.html#4-functions","title":"4. Functions","text":"<pre><code>def greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"Ted\"))  # Output: Hello, Ted!\n</code></pre>"},{"location":"python/03_Core_Concepts.html#5-object-oriented-programming-oop","title":"5. Object-Oriented Programming (OOP)","text":"<pre><code>class Car:\n    def __init__(self, brand, model):\n        self.brand = brand\n        self.model = model\n\n    def display(self):\n        return f\"Car: {self.brand} {self.model}\"\n\ncar1 = Car(\"Toyota\", \"Corolla\")\nprint(car1.display())  # Output: Car: Toyota Corolla\n</code></pre>"},{"location":"python/03_Core_Concepts.html#6-exception-handling","title":"6. Exception Handling","text":"<pre><code>try:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\nfinally:\n    print(\"Execution completed.\")\n</code></pre>"},{"location":"python/03_Core_Concepts.html#7-file-handling","title":"7. File Handling","text":"<pre><code>with open(\"example.txt\", \"w\") as file:\n    file.write(\"Hello, Python!\")\n\nwith open(\"example.txt\", \"r\") as file:\n    content = file.read()\n    print(content)\n</code></pre>"},{"location":"python/03_Core_Concepts.html#8-modules-libraries","title":"8. Modules &amp; Libraries","text":"<pre><code>import math\nprint(math.factorial(5))  # Output: 120\n</code></pre> <p>Mastering these core Python concepts enables Data Scientists, Machine Learning Engineers, and AI developers to build efficient, scalable, and high-performance solutions. \ud83d\ude80</p> <p>Later,  code challenges or exercises to reinforce these topics will be covered. \ud83e\udd14</p>"},{"location":"python/04_Advanced_Concepts.html","title":"04 \u26a1 Advanced Concepts","text":"<p>Python's versatility makes it the go-to language for Data Science, Machine Learning, and AI. Mastering core concepts such as Object-Oriented Programming (OOP), Decorators, Generators, Iterators, Comprehensions, Multithreading, and Asynchronous Programming is crucial for writing efficient, scalable, and maintainable code.</p> <p>This chapter covers essential Python concepts that empower professionals in AI and data science to build optimized pipelines, parallel computations, and reusable components.</p>"},{"location":"python/04_Advanced_Concepts.html#object-oriented-programming-oop","title":"\ud83c\udfd7\ufe0f Object-Oriented Programming (OOP)","text":"<p>Object-Oriented Programming (OOP) is a programming paradigm that enables modularity, code reusability, and encapsulation. It is widely used in Machine Learning for building models, creating reusable components, and managing data pipelines.</p>"},{"location":"python/04_Advanced_Concepts.html#key-oop-concepts","title":"\ud83d\udd39 Key OOP Concepts","text":"<ul> <li>Class &amp; Object \u2013 A class is a blueprint, and an object is an instance of a class.</li> <li>Encapsulation \u2013 Restrict direct access to variables and protect data integrity.</li> <li>Inheritance \u2013 Reuse attributes and methods from a parent class.</li> <li>Polymorphism \u2013 Different classes can implement the same method.</li> </ul>"},{"location":"python/04_Advanced_Concepts.html#example-oop-in-machine-learning","title":"\ud83d\udd39 Example: OOP in Machine Learning","text":"<pre><code>class Model:\n    def __init__(self, name):\n        self.name = name\n\n    def train(self):\n        print(f\"{self.name} model is training...\")\n\nclass NeuralNetwork(Model):\n    def train(self):\n        print(f\"Training deep learning model: {self.name}\")\n\n# Usage\nmodel1 = Model(\"Linear Regression\")\nmodel2 = NeuralNetwork(\"CNN\")\n\nmodel1.train()  # Output: Linear Regression model is training...\nmodel2.train()  # Output: Training deep learning model: CNN\n</code></pre> <p>\u2705 Use Case: OOP allows structured design in ML model pipelines, hyperparameter tuning, and deployment frameworks.</p>"},{"location":"python/04_Advanced_Concepts.html#decorators-generators","title":"\ud83c\udfad Decorators &amp; Generators","text":"<p>Python decorators and generators help optimize code efficiency, making them essential in data pipelines and AI model training.</p>"},{"location":"python/04_Advanced_Concepts.html#decorators-function-wrappers","title":"\ud83d\udd39 Decorators (Function Wrappers)","text":"<p>Decorators modify the behavior of functions without changing their code.</p> <p>Example: Timing an ML Function</p> <pre><code>import time\n\ndef timer(func):\n    def wrapper(*args, kwargs):\n        start = time.time()\n        result = func(*args, kwargs)\n        end = time.time()\n        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n        return result\n    return wrapper\n\n@timer\ndef train_model():\n    time.sleep(2)  # Simulating model training time\n    print(\"Model training complete!\")\n\ntrain_model()\n</code></pre> <p>\u2705 Use Case: Logging, debugging, measuring execution time, monitoring ML pipelines.</p>"},{"location":"python/04_Advanced_Concepts.html#generators-memory-efficient-iteration","title":"\ud83d\udd39 Generators (Memory-Efficient Iteration)","text":"<p>Generators are functions that return iterators lazily, saving memory when handling large datasets.</p> <p>Example: Processing Large Data Efficiently</p> <pre><code>def read_large_file(file_path):\n    with open(file_path, \"r\") as file:\n        for line in file:\n            yield line  # Yields one line at a time\n\n# Usage\nfor line in read_large_file(\"data.csv\"):\n    process(line)  # Process each line lazily\n</code></pre> <p>\u2705 Use Case: Streaming large datasets, real-time data processing in AI applications.</p>"},{"location":"python/04_Advanced_Concepts.html#iterators-comprehensions","title":"\ud83d\udd17 Iterators &amp; Comprehensions","text":"<p>Efficient data handling is critical in Machine Learning when working with large datasets and feature engineering.</p>"},{"location":"python/04_Advanced_Concepts.html#iterators","title":"\ud83d\udd39 Iterators","text":"<p>An iterator is an object that allows traversal of elements one at a time.</p> <p>Example: Custom Data Iterator</p> <pre><code>class DataLoader:\n    def __init__(self, data):\n        self.data = data\n        self.index = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.index &gt;= len(self.data):\n            raise StopIteration\n        result = self.data[self.index]\n        self.index += 1\n        return result\n\n# Usage\ndata = DataLoader([\"image1\", \"image2\", \"image3\"])\nfor d in data:\n    print(d)\n</code></pre> <p>\u2705 Use Case: Data loaders, streaming large datasets for ML models.</p>"},{"location":"python/04_Advanced_Concepts.html#list-dict-and-set-comprehensions","title":"\ud83d\udd39 List, Dict, and Set Comprehensions","text":"<p>Comprehensions make data transformation concise and are widely used in feature engineering and preprocessing.</p> <pre><code># Convert temperature from Celsius to Fahrenheit using list comprehension\ncelsius = [0, 10, 20, 30]\nfahrenheit = [((temp * 9/5) + 32) for temp in celsius]\nprint(fahrenheit)  # Output: [32.0, 50.0, 68.0, 86.0]\n</code></pre> <p>\u2705 Use Case: Feature scaling, data transformation, filtering large datasets.</p>"},{"location":"python/04_Advanced_Concepts.html#multithreading-multiprocessing","title":"\ud83d\udd04 Multithreading &amp; Multiprocessing","text":"<p>Parallel execution is essential for speeding up computations in AI and ML.</p>"},{"location":"python/04_Advanced_Concepts.html#multithreading-efficient-for-io-bound-tasks","title":"\ud83d\udd39 Multithreading (Efficient for I/O-bound tasks)","text":"<p>Example: Fetching multiple datasets in parallel</p> <pre><code>import threading\n\ndef fetch_data(source):\n    print(f\"Fetching from {source}\")\n\nsources = [\"dataset1.csv\", \"dataset2.csv\", \"dataset3.csv\"]\nthreads = [threading.Thread(target=fetch_data, args=(src,)) for src in sources]\n\nfor thread in threads:\n    thread.start()\nfor thread in threads:\n    thread.join()\n</code></pre> <p>\u2705 Use Case: Downloading datasets, web scraping, handling I/O-heavy tasks.</p>"},{"location":"python/04_Advanced_Concepts.html#multiprocessing-efficient-for-cpu-bound-tasks","title":"\ud83d\udd39 Multiprocessing (Efficient for CPU-bound tasks)","text":"<p>Multiprocessing utilizes multiple CPU cores, making it ideal for heavy computations.</p> <p>Example: Parallel Model Training</p> <pre><code>from multiprocessing import Pool\n\ndef train_model(model_id):\n    return f\"Training model {model_id}\"\n\nmodels = [1, 2, 3, 4]\nwith Pool(4) as p:\n    results = p.map(train_model, models)\nprint(results)\n</code></pre> <p>\u2705 Use Case: Parallel model training, large dataset processing, hyperparameter tuning.</p>"},{"location":"python/04_Advanced_Concepts.html#async-await","title":"\ud83e\uddf5 Async &amp; Await","text":"<p>Asynchronous programming is critical for handling large-scale web-based AI applications, real-time data processing, and API calls.</p>"},{"location":"python/04_Advanced_Concepts.html#async-for-efficient-io-operations","title":"\ud83d\udd39 Async for Efficient I/O Operations","text":"<pre><code>import asyncio\n\nasync def fetch_data():\n    print(\"Fetching data...\")\n    await asyncio.sleep(2)  # Simulating delay\n    print(\"Data fetched!\")\n\nasync def main():\n    await asyncio.gather(fetch_data(), fetch_data(), fetch_data())\n\nasyncio.run(main())\n</code></pre> <p>\u2705 Use Case: Handling multiple API requests, web scraping for AI datasets, real-time ML monitoring.</p>"},{"location":"python/04_Advanced_Concepts.html#summary","title":"\ud83d\ude80 Summary","text":"Concept Use Case \ud83c\udfd7\ufe0f OOP ML model architecture, data pipeline design \ud83c\udfad Decorators Logging, debugging, function optimization \ud83c\udfad Generators Handling large datasets efficiently \ud83d\udd17 Iterators Streaming datasets, loading ML batches \ud83d\udd17 Comprehensions Feature engineering, data transformation \ud83d\udd04 Multithreading I/O-bound tasks (API calls, web scraping) \ud83d\udd04 Multiprocessing CPU-bound tasks (ML training, parallel computations) \ud83e\uddf5 Async/Await Real-time AI applications, non-blocking API calls"},{"location":"python/04_Advanced_Concepts.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Mastering these core Python concepts enables Data Scientists, Machine Learning Engineers, and AI developers to build efficient, scalable, and high-performance solutions. \ud83d\ude80</p> <p>Would you like to add code challenges or exercises to reinforce these topics? \ud83e\udd14</p>"},{"location":"python/05_Syntax_and_Variables.html","title":"05 \ud83d\udcdd Syntax and Variables","text":"<p>Python's simple and readable syntax makes it a favorite among beginners and experts alike. Understanding its basic syntax and variable handling is essential for writing clean and efficient code.</p>"},{"location":"python/05_Syntax_and_Variables.html#31-python-syntax-basics","title":"\ud83d\udd39 3.1 Python Syntax Basics","text":"<p>Python follows an indentation-based syntax rather than using <code>{}</code> like C or Java. This makes the code cleaner and more readable.</p>"},{"location":"python/05_Syntax_and_Variables.html#example-python-syntax","title":"\u2705 Example: Python Syntax","text":"<pre><code># Correct indentation\nif True:\n    print(\"Hello, Python!\")  # Indented block\n</code></pre> <pre><code># \u274c Incorrect indentation (will raise an error)\nif True:\nprint(\"Hello, Python!\")  # IndentationError\n</code></pre>"},{"location":"python/05_Syntax_and_Variables.html#key-features-of-python-syntax","title":"\ud83d\udee0 Key Features of Python Syntax","text":"<ul> <li>No curly braces <code>{}</code> for blocks\u2014indentation matters!</li> <li>No need for semicolons <code>;</code> at the end of statements.</li> <li>Uses <code>#</code> for single-line comments and <code>\"\"\" \"\"\"</code> for multi-line comments.</li> </ul>"},{"location":"python/05_Syntax_and_Variables.html#32-variables-in-python","title":"\ud83d\udd39 3.2 Variables in Python","text":"<p>Variables in Python store data and do not require explicit type declaration. Python is dynamically typed, meaning the data type is determined at runtime.</p>"},{"location":"python/05_Syntax_and_Variables.html#declaring-variables","title":"\u2705 Declaring Variables","text":"<pre><code>name = \"Alice\"       # String\nage = 25            # Integer\nheight = 5.9        # Float\nis_student = True   # Boolean\n</code></pre>"},{"location":"python/05_Syntax_and_Variables.html#rules-for-variable-naming","title":"\ud83d\udd39 Rules for Variable Naming","text":"<p>\u2705 Allowed:</p> <ul> <li>Can start with a letter or underscore <code>_</code></li> <li>Can contain letters, numbers, and underscores</li> <li>Case-sensitive (<code>Age</code> and <code>age</code> are different)</li> </ul> <p>\u274c Not Allowed:</p> <ul> <li>Cannot start with a number (<code>2name \u274c</code>)</li> <li>Cannot use special characters (<code>@name \u274c</code>)</li> </ul>"},{"location":"python/05_Syntax_and_Variables.html#multiple-variable-assignment","title":"\ud83d\udd39 Multiple Variable Assignment","text":"<pre><code>a, b, c = 1, 2, \"Python\"\nprint(a, b, c)  # Output: 1 2 Python\n</code></pre> <p>\u2705 Use Case: Quick assignment of multiple values.</p>"},{"location":"python/05_Syntax_and_Variables.html#33-data-types-in-python","title":"\ud83d\udd39 3.3 Data Types in Python","text":"<p>Python provides built-in data types for handling different kinds of values.</p> Type Example Description <code>int</code> <code>x = 10</code> Whole numbers <code>float</code> <code>y = 3.14</code> Decimal numbers <code>str</code> <code>s = \"Python\"</code> Text/String <code>bool</code> <code>b = True</code> Boolean (True/False) <code>list</code> <code>l = [1,2,3]</code> Ordered, mutable collection <code>tuple</code> <code>t = (1,2,3)</code> Ordered, immutable collection <code>dict</code> <code>d = {\"key\": \"value\"}</code> Key-value pairs <code>set</code> <code>s = {1,2,3}</code> Unordered unique elements <p>\u2705 Use Case: Storing structured data, lists, and key-value mappings.</p>"},{"location":"python/05_Syntax_and_Variables.html#34-type-conversion","title":"\ud83d\udd39 3.4 Type Conversion","text":"<p>Python allows explicit type conversion (casting) when needed.</p> <pre><code>x = 5          # Integer\ny = str(x)     # Convert to string\nz = float(x)   # Convert to float\nprint(y, z)    # Output: '5' 5.0\n</code></pre> <p>\u2705 Use Case: Ensuring correct data formats in ML/DL models and databases.</p>"},{"location":"python/05_Syntax_and_Variables.html#35-string-manipulation","title":"\ud83d\udd39 3.5 String Manipulation","text":"<p>Python strings (<code>str</code>) support multiple operations.</p> <pre><code>name = \"Python\"\nprint(name.upper())   # PYTHON\nprint(name.lower())   # python\nprint(name[0:3])      # Pyt (Slicing)\n</code></pre> <p>\u2705 Use Case: Data cleaning in text processing and NLP.</p>"},{"location":"python/05_Syntax_and_Variables.html#36-user-input","title":"\ud83d\udd39 3.6 User Input","text":"<p>Python allows reading user input using <code>input()</code>.</p> <pre><code>name = input(\"Enter your name: \")\nprint(\"Hello, \" + name + \"!\")\n</code></pre> <p>\u2705 Use Case: Interactive Python applications and CLI tools.</p>"},{"location":"python/05_Syntax_and_Variables.html#37-constants-in-python","title":"\ud83d\udd39 3.7 Constants in Python","text":"<p>Python doesn\u2019t have built-in constants, but by convention, uppercase names are used.</p> <pre><code>PI = 3.1416  # Treated as a constant\n</code></pre> <p>\u2705 Use Case: Defining scientific constants.</p>"},{"location":"python/05_Syntax_and_Variables.html#38-f-strings-for-string-formatting","title":"\ud83d\udd39 3.8 f-Strings for String Formatting","text":"<pre><code>name = \"Alice\"\nage = 25\nprint(f\"My name is {name} and I am {age} years old.\")\n</code></pre> <p>\u2705 Use Case: Readable string interpolation.</p>"},{"location":"python/05_Syntax_and_Variables.html#summary","title":"\ud83d\ude80 Summary","text":"Concept Key Takeaway Python Syntax Uses indentation instead of <code>{}</code> Variables Dynamically typed, no explicit declaration needed Data Types Includes <code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>list</code>, <code>dict</code>, etc. Type Conversion Use <code>str()</code>, <code>int()</code>, <code>float()</code> for casting String Manipulation Supports <code>.upper()</code>, <code>.lower()</code>, slicing, and f-strings User Input <code>input()</code> for user interaction Constants Uppercase variable names conventionally used"},{"location":"python/05_Syntax_and_Variables.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Understanding basic syntax and variables is the foundation for mastering Python. Once comfortable with these, you can move on to data structures, control flow, and advanced programming concepts.</p> <p>Would you like exercises or quizzes to reinforce learning? \ud83d\ude80</p>"},{"location":"python/06_Control_Flow.html","title":"06 \ud83d\udd04 Control Flow","text":"<p>Control flow determines the execution order of statements in a Python program. It includes conditional statements, loops, and exception handling, allowing programs to make decisions and repeat actions efficiently.</p>"},{"location":"python/06_Control_Flow.html#41-conditional-statements-if-elif-else","title":"\ud83d\udd39 4.1 Conditional Statements (if, elif, else)","text":"<p>Conditional statements allow Python to execute different blocks of code based on conditions.</p>"},{"location":"python/06_Control_Flow.html#basic-if-else-statement","title":"\u2705 Basic if-else Statement","text":"<pre><code>x = 10\n\nif x &gt; 0:\n    print(\"Positive number\")\nelif x &lt; 0:\n    print(\"Negative number\")\nelse:\n    print(\"Zero\")\n</code></pre> <p>Output: <code>Positive number</code></p>"},{"location":"python/06_Control_Flow.html#nested-if-statements","title":"\ud83d\udd39 Nested if Statements","text":"<pre><code>age = 20\n\nif age &gt; 18:\n    if age &gt;= 21:\n        print(\"Eligible for full privileges\")\n    else:\n        print(\"Limited privileges\")\nelse:\n    print(\"Not eligible\")\n</code></pre> <p>\u2705 Use Case: Decision trees in ML models, data validation, user authentication.</p>"},{"location":"python/06_Control_Flow.html#42-looping-in-python","title":"\ud83d\udd39 4.2 Looping in Python","text":"<p>Loops allow repeating actions based on conditions.</p>"},{"location":"python/06_Control_Flow.html#for-loop-iterating-over-sequences","title":"\ud83d\udd39 for Loop (Iterating over Sequences)","text":"<pre><code>numbers = [1, 2, 3, 4, 5]\n\nfor num in numbers:\n    print(num)\n</code></pre> <p>Output:</p> <pre><code>1\n2\n3\n4\n5\n</code></pre> <p>\u2705 Use Case: Iterating over lists, tuples, dictionaries, and strings in data processing, ML datasets.</p>"},{"location":"python/06_Control_Flow.html#for-loop-with-range","title":"\ud83d\udd39 for Loop with range()","text":"<pre><code>for i in range(1, 6):\n    print(i)\n</code></pre> <p>Output:  </p> <pre><code>1\n2\n3\n4\n5\n</code></pre> <p>\u2705 Use Case: Creating training epochs in machine learning models.</p>"},{"location":"python/06_Control_Flow.html#while-loop-repeat-until-condition-fails","title":"\ud83d\udd39 while Loop (Repeat Until Condition Fails)","text":"<pre><code>count = 0\n\nwhile count &lt; 5:\n    print(\"Count:\", count)\n    count += 1\n</code></pre> <p>Output:  </p> <pre><code>Count: 0\nCount: 1\nCount: 2\nCount: 3\nCount: 4\n</code></pre> <p>\u2705 Use Case: Keeping a server running, waiting for a user input, or training an ML model until convergence.</p>"},{"location":"python/06_Control_Flow.html#43-loop-control-statements","title":"\ud83d\udd39 4.3 Loop Control Statements","text":"<p>Python provides ways to modify loop behavior using <code>break</code>, <code>continue</code>, and <code>pass</code>.</p>"},{"location":"python/06_Control_Flow.html#break-exit-loop-early","title":"\ud83d\udd39 break (Exit Loop Early)","text":"<pre><code>for num in range(10):\n    if num == 5:\n        break  # Stops at 5\n    print(num)\n</code></pre> <p>Output:  </p> <pre><code>0\n1\n2\n3\n4\n</code></pre> <p>\u2705 Use Case: Stopping an AI model early if a condition is met.</p>"},{"location":"python/06_Control_Flow.html#continue-skip-iteration","title":"\ud83d\udd39 continue (Skip Iteration)","text":"<pre><code>for num in range(5):\n    if num == 2:\n        continue  # Skips 2\n    print(num)\n</code></pre> <p>Output:  </p> <pre><code>0\n1\n3\n4\n</code></pre> <p>\u2705 Use Case: Skipping invalid data points in datasets.</p>"},{"location":"python/06_Control_Flow.html#pass-do-nothing","title":"\ud83d\udd39 pass (Do Nothing)","text":"<pre><code>for i in range(5):\n    if i == 3:\n        pass  # Placeholder\n    print(i)\n</code></pre> <p>\u2705 Use Case: Placeholder for functions, classes, loops.</p>"},{"location":"python/06_Control_Flow.html#44-list-comprehensions-for-loops","title":"\ud83d\udd39 4.4 List Comprehensions for Loops","text":"<p>Python supports one-liner loops with list comprehensions, improving efficiency.</p> <pre><code>numbers = [x * 2 for x in range(5)]\nprint(numbers)\n</code></pre> <p>Output: <code>[0, 2, 4, 6, 8]</code></p> <p>\u2705 Use Case: Feature engineering, transforming datasets, list filtering.</p>"},{"location":"python/06_Control_Flow.html#45-exception-handling-try-except-finally","title":"\ud83d\udd39 4.5 Exception Handling (try-except-finally)","text":"<p>Handling exceptions prevents crashes in programs.</p> <pre><code>try:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\nfinally:\n    print(\"Execution complete.\")\n</code></pre> <p>Output:  </p> <pre><code>Cannot divide by zero!\nExecution complete.\n</code></pre> <p>\u2705 Use Case: Preventing failures in data pipelines, ML model training.</p>"},{"location":"python/06_Control_Flow.html#summary","title":"\ud83d\ude80 Summary","text":"Concept Key Takeaway if-elif-else Executes different blocks based on conditions for loop Iterates over sequences (lists, tuples, etc.) while loop Runs while condition is <code>True</code> break Exits loop early continue Skips current iteration pass Placeholder statement List Comprehensions Shorter syntax for loops Exception Handling Prevents program crashes"},{"location":"python/06_Control_Flow.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Control flow is crucial for decision-making and iteration in Python. Mastering it allows writing efficient, error-free code.</p> <p>Would you like exercises or real-world examples to reinforce these topics? \ud83d\ude80</p>"},{"location":"python/07_Data_Structures.html","title":"07 \ud83c\udf92 Data Structures","text":"<p>Data structures are essential for organizing, storing, and managing data efficiently in Python. Python provides built-in data structures such as Lists, Tuples, Dictionaries, and Sets, each suited for different tasks.  </p> <p>This chapter covers their properties, operations, and use cases in Data Science, Machine Learning, and AI applications.</p>"},{"location":"python/07_Data_Structures.html#071-lists-ordered-mutable-indexed","title":"07.1 \ud83d\udccb Lists (Ordered, Mutable, Indexed)","text":"<p>A list is an ordered, mutable (changeable) collection that allows duplicate values. Lists are widely used for storing and manipulating datasets.</p>"},{"location":"python/07_Data_Structures.html#creating-a-list","title":"\u2705 Creating a List","text":"<pre><code>fruits = [\"apple\", \"banana\", \"cherry\"]\nnumbers = [10, 20, 30, 40]\nmixed = [1, \"Python\", 3.14, True]\n</code></pre>"},{"location":"python/07_Data_Structures.html#list-operations","title":"\ud83d\udd39 List Operations","text":"<pre><code>fruits.append(\"mango\")  # Add element\nfruits.remove(\"banana\") # Remove element\nfruits.insert(1, \"grape\")  # Insert at index\nfruits.pop()  # Remove last item\nprint(fruits)\n</code></pre> <p>\u2705 Use Case: Managing data records, feature lists, training data batches in ML.</p>"},{"location":"python/07_Data_Structures.html#list-slicing","title":"\ud83d\udd39 List Slicing","text":"<pre><code>numbers = [1, 2, 3, 4, 5, 6]\nprint(numbers[1:4])  # Output: [2, 3, 4]\n</code></pre> <p>\u2705 Use Case: Extracting subsets of data in AI and ML models.</p>"},{"location":"python/07_Data_Structures.html#072-tuples-ordered-immutable-indexed","title":"07.2 \ud83d\udccc Tuples (Ordered, Immutable, Indexed)","text":"<p>A tuple is like a list but immutable (cannot be modified). Tuples are used where data should not change.</p>"},{"location":"python/07_Data_Structures.html#creating-a-tuple","title":"\u2705 Creating a Tuple","text":"<pre><code>coordinates = (10.5, 20.3)\ncolors = (\"red\", \"green\", \"blue\")\n</code></pre>"},{"location":"python/07_Data_Structures.html#tuple-operations","title":"\ud83d\udd39 Tuple Operations","text":"<pre><code>print(coordinates[0])  # Access elements\nprint(len(colors))     # Tuple length\n</code></pre> <p>\u2705 Use Case: Storing constant data like color codes, geographic coordinates.</p>"},{"location":"python/07_Data_Structures.html#tuple-packing-unpacking","title":"\ud83d\udd39 Tuple Packing &amp; Unpacking","text":"<pre><code>point = (3, 4)\nx, y = point  # Unpacking\nprint(x, y)   # Output: 3 4\n</code></pre> <p>\u2705 Use Case: Assigning multiple values in one step in AI and data transformations.</p>"},{"location":"python/07_Data_Structures.html#073-dictionaries-key-value-pairs-unordered-mutable","title":"07.3 \ud83d\uddc2\ufe0f Dictionaries (Key-Value Pairs, Unordered, Mutable)","text":"<p>A dictionary (<code>dict</code>) stores data in key-value pairs, making it ideal for fast lookups.</p>"},{"location":"python/07_Data_Structures.html#creating-a-dictionary","title":"\u2705 Creating a Dictionary","text":"<pre><code>student = {\"name\": \"Alice\", \"age\": 21, \"grade\": \"A\"}\n</code></pre>"},{"location":"python/07_Data_Structures.html#dictionary-operations","title":"\ud83d\udd39 Dictionary Operations","text":"<pre><code>print(student[\"name\"])   # Access value\nstudent[\"age\"] = 22      # Modify value\nstudent[\"city\"] = \"NY\"   # Add new key-value pair\ndel student[\"grade\"]     # Remove key-value pair\n</code></pre> <p>\u2705 Use Case: Storing JSON-like data, AI model parameters, ML hyperparameters.</p>"},{"location":"python/07_Data_Structures.html#iterating-over-a-dictionary","title":"\ud83d\udd39 Iterating Over a Dictionary","text":"<pre><code>for key, value in student.items():\n    print(f\"{key}: {value}\")\n</code></pre> <p>\u2705 Use Case: Extracting metadata from datasets, handling API responses.</p>"},{"location":"python/07_Data_Structures.html#074-sets-unordered-unique-elements-fast-lookups","title":"07.4 \ud83d\udd25 Sets (Unordered, Unique Elements, Fast Lookups)","text":"<p>A set is an unordered collection of unique elements, useful for removing duplicates and fast lookups.</p>"},{"location":"python/07_Data_Structures.html#creating-a-set","title":"\u2705 Creating a Set","text":"<pre><code>numbers = {1, 2, 3, 4, 4, 2}  # Duplicates removed automatically\n</code></pre>"},{"location":"python/07_Data_Structures.html#set-operations","title":"\ud83d\udd39 Set Operations","text":"<pre><code>numbers.add(5)  # Add element\nnumbers.remove(3)  # Remove element\n</code></pre>"},{"location":"python/07_Data_Structures.html#set-operations-for-ai-ml","title":"\ud83d\udd39 Set Operations for AI &amp; ML","text":"<pre><code>A = {1, 2, 3, 4}\nB = {3, 4, 5, 6}\n\nprint(A.union(B))    # {1, 2, 3, 4, 5, 6}\nprint(A.intersection(B))  # {3, 4}\nprint(A.difference(B))    # {1, 2}\n</code></pre> <p>\u2705 Use Case: Removing duplicate values in datasets, comparing feature sets.</p>"},{"location":"python/07_Data_Structures.html#summary","title":"\ud83d\ude80 Summary","text":"Data Structure Properties Use Case List  \ud83d\udccb Ordered, Mutable Storing datasets, feature lists Tuple \ud83d\udccc Ordered, Immutable Constants, fixed ML configurations Dictionary \ud83d\uddc2\ufe0f Key-Value Pairs, Mutable Fast lookups, JSON data, ML parameters Set \ud83d\udd25 Unordered, Unique Elements Removing duplicates, comparing data"},{"location":"python/07_Data_Structures.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Understanding Lists, Tuples, Dictionaries, and Sets is crucial for data processing, feature engineering, and AI applications.</p> <p>Would you like real-world coding exercises on these topics? \ud83d\ude80</p>"},{"location":"python/08_Functions_and_Modules.html","title":"08 \ud83c\udfad Functions and Modules","text":"<p>Functions and modules are essential building blocks in Python, promoting code reuse, modularity, and maintainability. Understanding them is crucial for data science, machine learning, and AI, where reusable code improves efficiency and readability.  </p> <p>This chapter covers defining functions, argument handling, lambda functions, recursion, and working with modules to write efficient, modular, and scalable Python code.</p>"},{"location":"python/08_Functions_and_Modules.html#081-functions-the-building-blocks-of-python","title":"08.1 \ud83c\udfaf Functions: The Building Blocks of Python","text":"<p>A function is a reusable block of code that performs a specific task.</p>"},{"location":"python/08_Functions_and_Modules.html#defining-a-function","title":"\u2705 Defining a Function","text":"<pre><code>def greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"Alice\"))  # Output: Hello, Alice!\n</code></pre> <p>\u2705 Use Case: Encapsulating repetitive code, making programs more readable.</p>"},{"location":"python/08_Functions_and_Modules.html#082-function-arguments-and-parameters","title":"08.2 \ud83c\udfad Function Arguments and Parameters","text":"<p>Functions in Python support different types of arguments:</p>"},{"location":"python/08_Functions_and_Modules.html#positional-arguments","title":"\ud83d\udd39 Positional Arguments","text":"<pre><code>def add(a, b):\n    return a + b\n\nprint(add(5, 3))  # Output: 8\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#default-arguments","title":"\ud83d\udd39 Default Arguments","text":"<pre><code>def power(base, exponent=2):\n    return base  exponent\n\nprint(power(3))     # Output: 9 (3\u00b2)\nprint(power(3, 3))  # Output: 27 (3\u00b3)\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#keyword-arguments","title":"\ud83d\udd39 Keyword Arguments","text":"<pre><code>print(power(exponent=3, base=2))  # Output: 8\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#variable-length-arguments-args-kwargs","title":"\ud83d\udd39 Variable-Length Arguments (*args, **kwargs)","text":"<pre><code>def sum_all(*numbers):\n    return sum(numbers)\n\nprint(sum_all(1, 2, 3, 4))  # Output: 10\n\ndef display_info(info):\n    print(info)\n\ndisplay_info(name=\"Alice\", age=25)\n</code></pre> <p>\u2705 Use Case: Handling dynamic data inputs in ML models, APIs, and automation scripts.</p>"},{"location":"python/08_Functions_and_Modules.html#083-lambda-anonymous-functions","title":"08.3 \u26a1 Lambda (Anonymous) Functions","text":"<p>Lambda functions are short, one-line functions often used in data processing.</p>"},{"location":"python/08_Functions_and_Modules.html#lambda-syntax","title":"\u2705 Lambda Syntax","text":"<pre><code>square = lambda x: x  2\nprint(square(5))  # Output: 25\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#lambda-with-map-filter-reduce","title":"\ud83d\udd39 Lambda with map(), filter(), reduce()","text":"<pre><code>numbers = [1, 2, 3, 4, 5]\nsquared = list(map(lambda x: x2, numbers))\nprint(squared)  # Output: [1, 4, 9, 16, 25]\n</code></pre> <p>\u2705 Use Case: Data transformations in pandas, NumPy, and machine learning preprocessing.</p>"},{"location":"python/08_Functions_and_Modules.html#084-recursion-functions-calling-themselves","title":"08.4 \ud83d\udd01 Recursion: Functions Calling Themselves","text":"<p>Recursion is used when a problem can be broken down into smaller subproblems.</p>"},{"location":"python/08_Functions_and_Modules.html#factorial-calculation","title":"\u2705 Factorial Calculation","text":"<pre><code>def factorial(n):\n    if n == 1:\n        return 1\n    return n * factorial(n - 1)\n\nprint(factorial(5))  # Output: 120\n</code></pre> <p>\u2705 Use Case: Tree-based algorithms, graph traversal (DFS), Fibonacci sequence.</p>"},{"location":"python/08_Functions_and_Modules.html#085-python-modules-importing-and-organizing-code","title":"08.5 \ud83d\udce6 Python Modules (Importing and Organizing Code)","text":"<p>Modules allow code organization by grouping related functions and variables.</p>"},{"location":"python/08_Functions_and_Modules.html#importing-a-module","title":"\u2705 Importing a Module","text":"<pre><code>import math\nprint(math.sqrt(25))  # Output: 5.0\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#importing-specific-functions","title":"\ud83d\udd39 Importing Specific Functions","text":"<pre><code>from math import sqrt\nprint(sqrt(16))  # Output: 4.0\n</code></pre>"},{"location":"python/08_Functions_and_Modules.html#creating-a-custom-module","title":"\ud83d\udd39 Creating a Custom Module","text":"<p>\ud83d\udccc Create a file <code>mymodule.py</code> </p> <pre><code>def greet(name):\n    return f\"Hello, {name}!\"\n</code></pre> <p>\ud83d\udccc Import and Use the Module</p> <pre><code>import mymodule\nprint(mymodule.greet(\"Alice\"))\n</code></pre> <p>\u2705 Use Case: Reusing functions in large AI projects, ML models, and APIs.</p>"},{"location":"python/08_Functions_and_Modules.html#086-working-with-built-in-and-third-party-modules","title":"08.6 \ud83d\udcc2 Working with Built-in and Third-Party Modules","text":""},{"location":"python/08_Functions_and_Modules.html#useful-built-in-modules","title":"\ud83d\udd39 Useful Built-in Modules","text":"Module Purpose <code>math</code> Mathematical functions <code>random</code> Random number generation <code>datetime</code> Date and time operations <code>os</code> File and system operations <code>sys</code> System-related functions <code>re</code> Regular expressions"},{"location":"python/08_Functions_and_Modules.html#installing-using-third-party-modules","title":"\ud83d\udd39 Installing &amp; Using Third-Party Modules","text":"<pre><code>pip install numpy pandas\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\n</code></pre> <p>\u2705 Use Case: Data science, AI model training, automation.</p>"},{"location":"python/09_Object-Oriented_Programming.html","title":"09 \ud83c\udfd7\ufe0f Object-Oriented","text":"<p>Object-Oriented Programming (OOP) is a programming paradigm that organizes code into reusable objects. It is widely used in data science, machine learning, and AI to manage models, datasets, and complex systems efficiently.</p> <p>This chapter covers classes, objects, inheritance, polymorphism, encapsulation, and abstraction to help structure Python programs for scalability and maintainability.</p>"},{"location":"python/09_Object-Oriented_Programming.html#091-what-is-oop","title":"09.1 \ud83c\udfd7\ufe0f What is OOP?","text":"<p>OOP is based on the concept of objects that contain data (attributes) and functions (methods). This approach makes programs modular, reusable, and easy to maintain.</p>"},{"location":"python/09_Object-Oriented_Programming.html#key-oop-concepts","title":"\ud83d\udd39 Key OOP Concepts","text":"Concept Description Class A blueprint for creating objects Object An instance of a class Encapsulation Hiding internal details of an object Inheritance A child class inherits attributes and methods from a parent class Polymorphism Different classes can have methods with the same name but different behaviors Abstraction Hiding unnecessary implementation details"},{"location":"python/09_Object-Oriented_Programming.html#092-creating-classes-and-objects","title":"09.2 \ud83d\udce6 Creating Classes and Objects","text":"<p>A class is a blueprint for creating objects.</p>"},{"location":"python/09_Object-Oriented_Programming.html#defining-a-class-and-creating-an-object","title":"\u2705 Defining a Class and Creating an Object","text":"<pre><code>class Car:\n    def __init__(self, brand, model):\n        self.brand = brand  # Attribute\n        self.model = model  # Attribute\n\n    def display_info(self):  # Method\n        return f\"{self.brand} {self.model}\"\n\n# Creating an object\ncar1 = Car(\"Toyota\", \"Corolla\")\nprint(car1.display_info())  # Output: Toyota Corolla\n</code></pre> <p>\u2705 Use Case: Creating machine learning models, database records, or simulation objects.</p>"},{"location":"python/09_Object-Oriented_Programming.html#093-encapsulation-data-protection","title":"09.3 \ud83d\udd10 Encapsulation (Data Protection)","text":"<p>Encapsulation restricts direct access to object attributes, ensuring data integrity.</p>"},{"location":"python/09_Object-Oriented_Programming.html#private-variables-in-a-class","title":"\u2705 Private Variables in a Class","text":"<pre><code>class BankAccount:\n    def __init__(self, balance):\n        self.__balance = balance  # Private attribute\n\n    def deposit(self, amount):\n        self.__balance += amount\n\n    def get_balance(self):\n        return self.__balance\n\n# Usage\naccount = BankAccount(1000)\naccount.deposit(500)\nprint(account.get_balance())  # Output: 1500\n</code></pre> <p>\u2705 Use Case: Protecting sensitive data like user credentials, financial data.</p>"},{"location":"python/09_Object-Oriented_Programming.html#094-inheritance-code-reusability","title":"09.4 \ud83d\udd04 Inheritance (Code Reusability)","text":"<p>Inheritance allows a child class to use the properties and methods of a parent class, reducing redundant code.</p>"},{"location":"python/09_Object-Oriented_Programming.html#single-inheritance","title":"\u2705 Single Inheritance","text":"<pre><code>class Animal:\n    def make_sound(self):\n        return \"Some sound\"\n\nclass Dog(Animal):\n    def make_sound(self):\n        return \"Bark\"\n\n# Usage\ndog = Dog()\nprint(dog.make_sound())  # Output: Bark\n</code></pre> <p>\u2705 Use Case: Extending functionality of ML models, custom layers in deep learning.</p>"},{"location":"python/09_Object-Oriented_Programming.html#multiple-inheritance","title":"\ud83d\udd39 Multiple Inheritance","text":"<pre><code>class A:\n    def method_a(self):\n        return \"Method A\"\n\nclass B:\n    def method_b(self):\n        return \"Method B\"\n\nclass C(A, B):  # Inheriting from A and B\n    pass\n\nobj = C()\nprint(obj.method_a())  # Output: Method A\nprint(obj.method_b())  # Output: Method B\n</code></pre> <p>\u2705 Use Case: Combining functionalities from different modules (e.g., ML models + preprocessing steps).</p>"},{"location":"python/09_Object-Oriented_Programming.html#095-polymorphism-multiple-forms","title":"09.5 \ud83d\udd04 Polymorphism (Multiple Forms)","text":"<p>Polymorphism allows different classes to use the same method name but behave differently.</p>"},{"location":"python/09_Object-Oriented_Programming.html#method-overriding","title":"\u2705 Method Overriding","text":"<pre><code>class Shape:\n    def area(self):\n        return \"Area method not implemented\"\n\nclass Circle(Shape):\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return 3.14 * self.radius  2  # Overriding method\n\n# Usage\ncircle = Circle(5)\nprint(circle.area())  # Output: 78.5\n</code></pre> <p>\u2705 Use Case: Implementing AI models with different training methods.</p>"},{"location":"python/09_Object-Oriented_Programming.html#096-abstraction-hiding-implementation-details","title":"09.6 \ud83c\udfad Abstraction (Hiding Implementation Details)","text":"<p>Abstraction hides complex logic and exposes only relevant details.</p>"},{"location":"python/09_Object-Oriented_Programming.html#using-the-abc-module-for-abstraction","title":"\u2705 Using the <code>ABC</code> Module for Abstraction","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass Payment(ABC):  # Abstract Class\n    @abstractmethod\n    def process_payment(self, amount):\n        pass\n\nclass CreditCardPayment(Payment):\n    def process_payment(self, amount):\n        return f\"Processing credit card payment of ${amount}\"\n\n# Usage\npayment = CreditCardPayment()\nprint(payment.process_payment(100))  # Output: Processing credit card payment of $100\n</code></pre> <p>\u2705 Use Case: Defining AI model architecture, creating frameworks for ML algorithms.</p>"},{"location":"python/09_Object-Oriented_Programming.html#097-oop-in-real-world-ai-and-ml","title":"09.7 \ud83d\udee0\ufe0f OOP in Real-World AI and ML","text":""},{"location":"python/09_Object-Oriented_Programming.html#oop-for-machine-learning-models","title":"\u2705 OOP for Machine Learning Models","text":"<pre><code>class MLModel:\n    def train(self, data):\n        return \"Training the model on data\"\n\nclass NeuralNetwork(MLModel):\n    def train(self, data):\n        return \"Training deep learning model\"\n\nmodel1 = MLModel()\nmodel2 = NeuralNetwork()\n\nprint(model1.train(\"Dataset\"))  # Output: Training the model on data\nprint(model2.train(\"Dataset\"))  # Output: Training deep learning model\n</code></pre> <p>\u2705 Use Case: Modularizing ML models and creating reusable AI components.</p>"},{"location":"python/09_Object-Oriented_Programming.html#summary","title":"\ud83d\ude80 Summary","text":"OOP Concept Description Use Case Class &amp; Object Blueprint and instance of an object AI models, Data structures Encapsulation Restricting direct access to attributes Secure financial transactions Inheritance Child class inherits from parent class Model pipelines, feature engineering Polymorphism Same method, different behavior Different AI models processing inputs Abstraction Hiding unnecessary details AI frameworks, APIs"},{"location":"python/09_Object-Oriented_Programming.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Object-Oriented Programming enhances modularity and reusability, making it ideal for building scalable ML models, AI systems, and large applications.</p> <p>Would you like real-world coding challenges for hands-on practice? \ud83d\ude80</p>"},{"location":"python/10_Python_Standard_Library.html","title":"10 \ud83e\uddf0 Python Standard Library","text":"<p>The Python Standard Library is a collection of built-in modules and functions that provide powerful functionality without needing external dependencies. It includes modules for mathematical operations, file handling, date/time manipulation, data structures, web services, OS interaction, and more.</p> <p>This chapter explores some of the most useful standard library modules, their features, and how they can simplify Python development.</p>"},{"location":"python/10_Python_Standard_Library.html#101-math-mathematical-operations","title":"10.1 \ud83d\udd22 <code>math</code> \u2013 Mathematical Operations","text":"<p>The <code>math</code> module provides mathematical functions such as trigonometry, logarithms, and factorials.</p>"},{"location":"python/10_Python_Standard_Library.html#common-functions-in-math","title":"\u2705 Common Functions in <code>math</code>","text":"<pre><code>import math\n\nprint(math.sqrt(25))    # Output: 5.0\nprint(math.factorial(5))  # Output: 120\nprint(math.log(100, 10))  # Output: 2.0\nprint(math.pi)  # Output: 3.141592653589793\nprint(math.e)   # Output: 2.718281828459045\n</code></pre> <p>\u2705 Use Case: Calculations in machine learning, data science, and AI models.</p>"},{"location":"python/10_Python_Standard_Library.html#102-random-generating-random-numbers","title":"10.2 \ud83c\udfb2 <code>random</code> \u2013 Generating Random Numbers","text":"<p>The <code>random</code> module helps in random sampling, shuffling, and generating random numbers.</p>"},{"location":"python/10_Python_Standard_Library.html#generating-random-numbers","title":"\u2705 Generating Random Numbers","text":"<pre><code>import random\n\nprint(random.randint(1, 10))  # Random integer between 1 and 10\nprint(random.choice([\"apple\", \"banana\", \"cherry\"]))  # Random choice\nprint(random.sample(range(100), 5))  # 5 random numbers from 0-99\n</code></pre> <p>\u2705 Use Case: Random sampling, AI model parameter initialization, data augmentation.</p>"},{"location":"python/10_Python_Standard_Library.html#103-datetime-working-with-dates-and-time","title":"10.3 \ud83d\udcc6 <code>datetime</code> \u2013 Working with Dates and Time","text":"<p>The <code>datetime</code> module provides tools for date and time manipulation.</p>"},{"location":"python/10_Python_Standard_Library.html#getting-current-date-and-time","title":"\u2705 Getting Current Date and Time","text":"<pre><code>from datetime import datetime\n\nnow = datetime.now()\nprint(now.strftime(\"%Y-%m-%d %H:%M:%S\"))  # Output: 2025-03-06 14:30:15\n</code></pre>"},{"location":"python/10_Python_Standard_Library.html#calculating-time-differences","title":"\ud83d\udd39 Calculating Time Differences","text":"<pre><code>from datetime import timedelta\n\nfuture_date = now + timedelta(days=7)\nprint(future_date.strftime(\"%Y-%m-%d\"))  # Output: (7 days ahead)\n</code></pre> <p>\u2705 Use Case: Timestamps in logs, scheduling tasks, time-series analysis.</p>"},{"location":"python/10_Python_Standard_Library.html#104-os-interacting-with-the-operating-system","title":"10.4 \ud83d\udcc2 <code>os</code> \u2013 Interacting with the Operating System","text":"<p>The <code>os</code> module helps manage files, directories, and system operations.</p>"},{"location":"python/10_Python_Standard_Library.html#common-os-operations","title":"\u2705 Common OS Operations","text":"<pre><code>import os\n\nprint(os.getcwd())  # Get current working directory\nos.mkdir(\"new_folder\")  # Create a new folder\nos.rename(\"old_file.txt\", \"new_file.txt\")  # Rename a file\nos.remove(\"new_file.txt\")  # Delete a file\n</code></pre> <p>\u2705 Use Case: File automation, script execution, managing system resources.</p>"},{"location":"python/10_Python_Standard_Library.html#105-sys-system-specific-functions","title":"10.5 \ud83d\udcdc <code>sys</code> \u2013 System-Specific Functions","text":"<p>The <code>sys</code> module provides functions related to the Python interpreter and command-line arguments.</p>"},{"location":"python/10_Python_Standard_Library.html#getting-command-line-arguments","title":"\u2705 Getting Command-Line Arguments","text":"<pre><code>import sys\nprint(sys.argv)  # List of command-line arguments\n</code></pre>"},{"location":"python/10_Python_Standard_Library.html#exiting-the-program","title":"\ud83d\udd39 Exiting the Program","text":"<pre><code>sys.exit(\"Terminating program\")\n</code></pre> <p>\u2705 Use Case: Handling script execution arguments, system interaction.</p>"},{"location":"python/10_Python_Standard_Library.html#106-re-regular-expressions-pattern-matching","title":"10.6 \ud83d\udd0d <code>re</code> \u2013 Regular Expressions (Pattern Matching)","text":"<p>The <code>re</code> module provides powerful text searching and pattern matching.</p>"},{"location":"python/10_Python_Standard_Library.html#searching-for-patterns","title":"\u2705 Searching for Patterns","text":"<pre><code>import re\n\ntext = \"My email is example@gmail.com\"\nmatch = re.search(r\"\\w+@\\w+\\.\\w+\", text)\nif match:\n    print(match.group())  # Output: example@gmail.com\n</code></pre> <p>\u2705 Use Case: Data cleaning, log processing, text extraction in NLP.</p>"},{"location":"python/10_Python_Standard_Library.html#107-urllib-fetching-web-data","title":"10.7 \ud83d\udce1 <code>urllib</code> \u2013 Fetching Web Data","text":"<p>The <code>urllib</code> module allows sending HTTP requests and fetching web content.</p>"},{"location":"python/10_Python_Standard_Library.html#downloading-a-webpage","title":"\u2705 Downloading a Webpage","text":"<pre><code>import urllib.request\n\nresponse = urllib.request.urlopen(\"https://www.python.org\")\nhtml = response.read().decode(\"utf-8\")\nprint(html[:200])  # Prints first 200 characters of HTML\n</code></pre> <p>\u2705 Use Case: Web scraping, downloading datasets, API calls.</p>"},{"location":"python/10_Python_Standard_Library.html#108-json-handling-json-data","title":"10.8 \ud83d\uddc3\ufe0f <code>json</code> \u2013 Handling JSON Data","text":"<p>The <code>json</code> module allows converting Python objects to JSON format and vice versa.</p>"},{"location":"python/10_Python_Standard_Library.html#converting-python-dictionary-to-json","title":"\u2705 Converting Python Dictionary to JSON","text":"<pre><code>import json\n\ndata = {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"}\njson_data = json.dumps(data)\nprint(json_data)  # Output: JSON formatted string\n</code></pre>"},{"location":"python/10_Python_Standard_Library.html#parsing-json-data","title":"\ud83d\udd39 Parsing JSON Data","text":"<pre><code>parsed_data = json.loads(json_data)\nprint(parsed_data[\"name\"])  # Output: Alice\n</code></pre> <p>\u2705 Use Case: Handling API responses, saving structured data.</p>"},{"location":"python/10_Python_Standard_Library.html#109-csv-reading-and-writing-csv-files","title":"10.9 \ud83d\udd04 <code>csv</code> \u2013 Reading and Writing CSV Files","text":"<p>The <code>csv</code> module allows handling comma-separated values (CSV) files, commonly used in data science.</p>"},{"location":"python/10_Python_Standard_Library.html#reading-a-csv-file","title":"\u2705 Reading a CSV File","text":"<pre><code>import csv\n\nwith open(\"data.csv\", newline=\"\") as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)\n</code></pre> <p>\u2705 Use Case: Reading structured datasets for ML training.</p>"},{"location":"python/10_Python_Standard_Library.html#1010-collections-advanced-data-structures","title":"10.10 \ud83d\uddc2\ufe0f <code>collections</code> \u2013 Advanced Data Structures","text":"<p>The <code>collections</code> module provides specialized data structures like deque, Counter, defaultdict.</p>"},{"location":"python/10_Python_Standard_Library.html#counting-elements-with-counter","title":"\u2705 Counting Elements with Counter","text":"<pre><code>from collections import Counter\n\ndata = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\"]\ncounter = Counter(data)\nprint(counter)  # Output: {'apple': 3, 'banana': 2, 'orange': 1}\n</code></pre> <p>\u2705 Use Case: Counting words in NLP, tracking data occurrences.</p>"},{"location":"python/10_Python_Standard_Library.html#summary-of-useful-python-standard-library-modules","title":"\ud83d\ude80 Summary of Useful Python Standard Library Modules","text":"Module Purpose math Mathematical operations (sqrt, factorial, log, pi) random Random number generation, shuffling datetime Date and time handling os OS file management (create, rename, delete) sys System operations, command-line arguments re Regular expressions for pattern matching urllib Web requests and data fetching json JSON data encoding/decoding csv Reading and writing CSV files collections Advanced data structures (Counter, deque)"},{"location":"python/10_Python_Standard_Library.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>The Python Standard Library provides powerful tools for data processing, automation, system operations, and web interactions. Mastering these modules helps in building efficient Python programs.</p> <p>Would you like hands-on exercises on these modules? \ud83d\ude80</p>"},{"location":"python/11_File_Handling_and_IO.html","title":"11 \ud83c\udf10 File Handling","text":"<p>File handling is an essential part of programming, allowing reading, writing, and manipulating files. In data science, machine learning, and AI, working with text files, CSV files, and logs is common for storing and processing large datasets.  </p> <p>This chapter covers file handling operations, including reading, writing, appending, working with different file formats, and error handling.</p>"},{"location":"python/11_File_Handling_and_IO.html#111-working-with-files-in-python","title":"11.1 \ud83d\udcc2 Working with Files in Python","text":"<p>Python provides built-in functions for opening, reading, writing, and closing files.</p>"},{"location":"python/11_File_Handling_and_IO.html#opening-a-file","title":"\u2705 Opening a File","text":"<pre><code>file = open(\"example.txt\", \"r\")  # Open a file in read mode\nprint(file.read())  # Read the entire file content\nfile.close()  # Always close the file after use\n</code></pre> <p>\u2705 Use Case: Reading log files, dataset files, configuration files.</p>"},{"location":"python/11_File_Handling_and_IO.html#112-writing-to-a-file","title":"11.2 \ud83d\udcdd Writing to a File","text":"<p>To write to a file, use write ('w') mode. If the file doesn\u2019t exist, Python creates a new file.</p>"},{"location":"python/11_File_Handling_and_IO.html#writing-data","title":"\u2705 Writing Data","text":"<pre><code>file = open(\"example.txt\", \"w\")  # Open in write mode\nfile.write(\"Hello, Python!\\nWelcome to File Handling.\")\nfile.close()\n</code></pre> <p>\ud83d\udd39 Note: <code>'w'</code> mode overwrites existing content. To append, use <code>'a'</code>.</p>"},{"location":"python/11_File_Handling_and_IO.html#appending-data-to-an-existing-file","title":"\u2705 Appending Data to an Existing File","text":"<pre><code>file = open(\"example.txt\", \"a\")\nfile.write(\"\\nThis is an additional line.\")\nfile.close()\n</code></pre> <p>\u2705 Use Case: Logging data, saving AI/ML model outputs.</p>"},{"location":"python/11_File_Handling_and_IO.html#113-reading-files-efficiently","title":"11.3 \ud83d\udcd6 Reading Files Efficiently","text":"<p>Python provides different ways to read files efficiently.</p>"},{"location":"python/11_File_Handling_and_IO.html#reading-a-file-line-by-line","title":"\u2705 Reading a File Line by Line","text":"<pre><code>file = open(\"example.txt\", \"r\")\n\nfor line in file:\n    print(line.strip())  # Removing extra newlines\nfile.close()\n</code></pre> <p>\u2705 Use Case: Processing large datasets without memory overflow.</p>"},{"location":"python/11_File_Handling_and_IO.html#using-readlines-to-read-all-lines-as-a-list","title":"\ud83d\udd39 Using <code>readlines()</code> to Read All Lines as a List","text":"<pre><code>file = open(\"example.txt\", \"r\")\nlines = file.readlines()\nprint(lines)  # Output: List of lines in the file\nfile.close()\n</code></pre> <p>\u2705 Use Case: Reading structured text data (e.g., logs, reports).</p>"},{"location":"python/11_File_Handling_and_IO.html#114-using-with-statement-for-file-handling","title":"11.4 \ud83d\ude80 Using <code>with</code> Statement for File Handling","text":"<p>Using <code>with open()</code> ensures that the file automatically closes after execution.</p>"},{"location":"python/11_File_Handling_and_IO.html#safe-file-handling-with-with","title":"\u2705 Safe File Handling with <code>with</code>","text":"<pre><code>with open(\"example.txt\", \"r\") as file:\n    content = file.read()\n    print(content)\n</code></pre> <p>\u2705 Use Case: Better memory management, avoiding file lock issues.</p>"},{"location":"python/11_File_Handling_and_IO.html#115-handling-different-file-formats","title":"11.5 \ud83d\udcca Handling Different File Formats","text":""},{"location":"python/11_File_Handling_and_IO.html#working-with-csv-files-csv-module","title":"\ud83d\udd39 Working with CSV Files (<code>csv</code> Module)","text":"<pre><code>import csv\n\nwith open(\"data.csv\", \"r\") as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)  # Output: List of values in each row\n</code></pre> <p>\u2705 Use Case: Reading structured datasets in ML models.</p>"},{"location":"python/11_File_Handling_and_IO.html#writing-to-a-csv-file","title":"\ud83d\udd39 Writing to a CSV File","text":"<pre><code>with open(\"output.csv\", \"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Name\", \"Age\"])\n    writer.writerow([\"Alice\", 25])\n</code></pre> <p>\u2705 Use Case: Saving processed data from AI pipelines.</p>"},{"location":"python/11_File_Handling_and_IO.html#working-with-json-files-json-module","title":"\ud83d\udd39 Working with JSON Files (<code>json</code> Module)","text":"<pre><code>import json\n\ndata = {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"}\n\n# Writing JSON data\nwith open(\"data.json\", \"w\") as file:\n    json.dump(data, file)\n\n# Reading JSON data\nwith open(\"data.json\", \"r\") as file:\n    loaded_data = json.load(file)\nprint(loaded_data)  # Output: {'name': 'Alice', 'age': 25, 'city': 'New York'}\n</code></pre> <p>\u2705 Use Case: Handling API responses, configuration files, AI model metadata.</p>"},{"location":"python/11_File_Handling_and_IO.html#116-error-handling-in-file-operations","title":"11.6 \ud83d\udee0\ufe0f Error Handling in File Operations","text":"<p>It\u2019s important to handle file-related errors to prevent crashes.</p>"},{"location":"python/11_File_Handling_and_IO.html#handling-file-not-found-error","title":"\u2705 Handling File Not Found Error","text":"<pre><code>try:\n    with open(\"non_existent.txt\", \"r\") as file:\n        content = file.read()\nexcept FileNotFoundError:\n    print(\"Error: File not found!\")\n</code></pre> <p>\u2705 Use Case: Ensuring robust scripts in production AI pipelines.</p>"},{"location":"python/11_File_Handling_and_IO.html#117-working-with-directories-os-module","title":"11.7 \ud83d\udcc2 Working with Directories (<code>os</code> Module)","text":"<p>The <code>os</code> module allows working with directories and file management.</p>"},{"location":"python/11_File_Handling_and_IO.html#listing-files-in-a-directory","title":"\u2705 Listing Files in a Directory","text":"<pre><code>import os\n\nprint(os.listdir(\".\"))  # Lists all files in the current directory\n</code></pre>"},{"location":"python/11_File_Handling_and_IO.html#creating-and-deleting-folders","title":"\ud83d\udd39 Creating and Deleting Folders","text":"<pre><code>os.mkdir(\"new_folder\")  # Create a folder\nos.rmdir(\"new_folder\")  # Remove a folder\n</code></pre> <p>\u2705 Use Case: Managing dataset directories in ML projects.</p>"},{"location":"python/11_File_Handling_and_IO.html#summary","title":"Summary","text":"Concept Description Use Case Reading Files <code>open(\"file.txt\", \"r\")</code> Loading datasets, logs Writing Files <code>open(\"file.txt\", \"w\")</code> Saving AI model results Appending to Files <code>open(\"file.txt\", \"a\")</code> Logging incremental data Using <code>with</code> Statement Ensures safe file handling Avoids resource leaks CSV Handling <code>csv.reader()</code>, <code>csv.writer()</code> Working with structured data JSON Handling <code>json.load()</code>, <code>json.dump()</code> API responses, metadata storage Error Handling <code>try-except</code> for missing files Prevents crashes in AI pipelines Directory Management <code>os.listdir()</code>, <code>os.mkdir()</code> Managing datasets and logs"},{"location":"python/11_File_Handling_and_IO.html#final-thoughts","title":"Final Thoughts","text":"<p>Mastering file handling and I/O is essential for data processing, logging, and storage in AI, ML, and automation projects.</p> <p>Would you like real-world coding exercises for practice? \ud83d\ude80</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html","title":"12 \u26a1 Errors and Debugging","text":"<p>Errors are inevitable in programming. Python provides exception handling and debugging tools to ensure programs fail gracefully and help identify issues efficiently. Exception handling improves code reliability, while debugging helps track and fix errors.</p> <p>This chapter covers handling exceptions with <code>try-except-finally</code>, built-in exceptions, raising custom exceptions, logging errors, and debugging techniques.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#121-understanding-errors-in-python","title":"12.1 \u274c Understanding Errors in Python","text":"<p>Python has two main types of errors:  </p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#syntax-errors-parsing-errors","title":"\ud83d\udd39 Syntax Errors (Parsing Errors)","text":"<p>Occurs when Python encounters an incorrectly written statement.</p> <pre><code>print(\"Hello\"  # Missing closing parenthesis -&gt; SyntaxError\n</code></pre> <p>\u2705 Fix: Ensure correct syntax.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#runtime-errors-exceptions","title":"\ud83d\udd39 Runtime Errors (Exceptions)","text":"<p>Occur during execution, such as dividing by zero or accessing an undefined variable.</p> <pre><code>x = 5 / 0  # ZeroDivisionError\n</code></pre> <p>Python provides built-in exception types, such as:  </p> Exception Type Description <code>ZeroDivisionError</code> Division by zero error <code>TypeError</code> Invalid operation on incompatible data types <code>ValueError</code> Function receives incorrect data type <code>IndexError</code> Accessing an invalid index in a list <code>KeyError</code> Accessing a missing key in a dictionary <code>FileNotFoundError</code> Attempting to open a non-existent file"},{"location":"python/12_Exception_Handling_and_Debugging.html#122-handling-exceptions-with-try-except","title":"12.2 \ud83d\udee0\ufe0f Handling Exceptions with <code>try-except</code>","text":"<p>The <code>try-except</code> block catches runtime errors and prevents program crashes.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#basic-exception-handling","title":"\u2705 Basic Exception Handling","text":"<pre><code>try:\n    x = 5 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\n</code></pre> <p>Output:  </p> <pre><code>Cannot divide by zero!\n</code></pre> <p>\u2705 Use Case: Handling invalid user inputs, avoiding crashes in AI applications.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#123-handling-multiple-exceptions","title":"12.3 \ud83d\udd04 Handling Multiple Exceptions","text":"<p>Multiple exceptions can be handled in a single <code>try-except</code> block.</p> <pre><code>try:\n    num = int(\"Python\")  # Causes ValueError\nexcept (ValueError, TypeError):\n    print(\"Invalid input!\")\n</code></pre> <p>\u2705 Use Case: Handling diverse input errors in data processing pipelines.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#124-using-finally-for-cleanup","title":"12.4 \ud83d\udccc Using <code>finally</code> for Cleanup","text":"<p>The <code>finally</code> block executes whether an exception occurs or not.</p> <pre><code>try:\n    file = open(\"example.txt\", \"r\")\n    print(file.read())\nexcept FileNotFoundError:\n    print(\"File not found!\")\nfinally:\n    print(\"Closing the file.\")  # Always runs\n</code></pre> <p>\u2705 Use Case: Ensuring file/database connections are properly closed.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#125-raising-custom-exceptions-raise","title":"12.5 \ud83d\ude80 Raising Custom Exceptions (<code>raise</code>)","text":"<p>Use <code>raise</code> to define custom exceptions for better debugging.</p> <pre><code>def check_age(age):\n    if age &lt; 18:\n        raise ValueError(\"Age must be 18 or above\")\n    return \"Access granted\"\n\nprint(check_age(15))  # Raises ValueError\n</code></pre> <p>\u2705 Use Case: Enforcing validation rules in APIs and ML models.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#126-logging-errors-logging-module","title":"12.6 \ud83d\udcdd Logging Errors (<code>logging</code> Module)","text":"<p>Instead of printing errors, use logging to track them efficiently.</p> <pre><code>import logging\n\nlogging.basicConfig(filename=\"app.log\", level=logging.ERROR)\n\ntry:\n    result = 5 / 0\nexcept ZeroDivisionError as e:\n    logging.error(f\"Error occurred: {e}\")\n</code></pre> <p>\u2705 Use Case: Tracking issues in AI applications, logging errors in production.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#127-debugging-techniques","title":"12.7 \ud83d\udc1b Debugging Techniques","text":"<p>Debugging helps find and fix issues before deployment.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#using-print-for-debugging","title":"\ud83d\udd39 Using <code>print()</code> for Debugging","text":"<pre><code>def add(a, b):\n    print(f\"Adding {a} and {b}\")  # Debugging output\n    return a + b\n\nprint(add(5, 3))\n</code></pre> <p>\u2705 Use Case: Checking function execution and variable values.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#using-assert-for-testing","title":"\ud83d\udd39 Using <code>assert</code> for Testing","text":"<p><code>assert</code> helps validate assumptions in the code.</p> <pre><code>def divide(a, b):\n    assert b != 0, \"Denominator cannot be zero\"\n    return a / b\n\nprint(divide(10, 2))  # Runs fine\nprint(divide(10, 0))  # Raises AssertionError\n</code></pre> <p>\u2705 Use Case: Preventing invalid inputs in ML models.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#debugging-with-pdb-python-debugger","title":"\ud83d\udd39 Debugging with <code>pdb</code> (Python Debugger)","text":"<p><code>pdb</code> allows step-by-step execution.</p> <pre><code>import pdb\n\ndef multiply(x, y):\n    pdb.set_trace()  # Debugging breakpoint\n    return x * y\n\nprint(multiply(3, 4))\n</code></pre> <p>\u2705 Use Case: Investigating code behavior interactively.</p>"},{"location":"python/12_Exception_Handling_and_Debugging.html#128-best-practices-for-exception-handling-debugging","title":"12.8 \ud83d\udee0\ufe0f Best Practices for Exception Handling &amp; Debugging","text":"<p>\u2705 DO:</p> <ul> <li>Use specific exceptions (<code>ZeroDivisionError</code>, <code>FileNotFoundError</code>)</li> <li>Use logging instead of <code>print()</code></li> <li>Use <code>finally</code> for resource cleanup</li> <li>Use assertions for quick debugging</li> </ul> <p>\u274c AVOID:</p> <ul> <li>Using <code>except:</code> without specifying the exception</li> <li>Silencing exceptions (<code>pass</code> inside <code>except</code>)</li> <li>Overusing <code>try-except</code> in simple operations</li> </ul>"},{"location":"python/12_Exception_Handling_and_Debugging.html#summary","title":"\ud83d\ude80 Summary","text":"Concept Description Use Case try-except Catches runtime errors Handling invalid inputs Multiple Exceptions Catches different errors Complex workflows finally Executes cleanup code Closing files/databases raise Throws custom exceptions Enforcing validation logging Saves errors to a log file Debugging production apps assert Ensures correct values Quick testing pdb Interactive debugging Step-by-step execution"},{"location":"python/12_Exception_Handling_and_Debugging.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Mastering exception handling and debugging is essential for writing robust, error-free Python applications.</p> <p>Would you like hands-on debugging exercises? \ud83d\ude80</p>"},{"location":"python/13_Web_Development.html","title":"13 \ud83d\udce1 Web Development","text":"<p>Python is a powerful language for web development, offering a variety of frameworks for building web applications, APIs, and documentation sites. This chapter covers Flask, FastAPI, MkDocs, and Streamlit, widely used for web applications, REST APIs, documentation, and data visualization.</p>"},{"location":"python/13_Web_Development.html#131-overview-of-python-web-development","title":"13.1 \ud83c\udf0d Overview of Python Web Development","text":"<p>Python provides lightweight and scalable web frameworks for different needs:</p> Framework Purpose Flask Lightweight, flexible web framework FastAPI High-performance API framework for modern web applications MkDocs Static site generator for documentation Streamlit Framework for building data-driven web apps"},{"location":"python/13_Web_Development.html#132-flask-lightweight-web-framework","title":"13.2 \ud83c\udfd7\ufe0f Flask \u2013 Lightweight Web Framework","text":"<p>Flask is a minimalistic and easy-to-use web framework, widely used for small to medium-sized web apps.</p>"},{"location":"python/13_Web_Development.html#installing-flask","title":"\u2705 Installing Flask","text":"<pre><code>pip install flask\n</code></pre>"},{"location":"python/13_Web_Development.html#creating-a-simple-flask-app","title":"\ud83d\udd39 Creating a Simple Flask App","text":"<pre><code>from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef home():\n    return \"Hello, Flask!\"\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre> <p>\ud83d\udd39 Run the app:</p> <pre><code>python app.py\n</code></pre> <p>\ud83d\udd39 Visit <code>http://137.0.0.1:5000/</code> in your browser.</p>"},{"location":"python/13_Web_Development.html#handling-routes-url-parameters","title":"\ud83d\udd39 Handling Routes &amp; URL Parameters","text":"<pre><code>@app.route(\"/user/&lt;name&gt;\")\ndef greet_user(name):\n    return f\"Hello, {name}!\"\n</code></pre> <p>\u2705 Use Case: Building web dashboards, small applications, and APIs.</p>"},{"location":"python/13_Web_Development.html#133-fastapi-high-performance-apis","title":"13.3 \ud83d\ude80 FastAPI \u2013 High-Performance APIs","text":"<p>FastAPI is a modern, fast framework for building APIs with automatic documentation and async support.</p>"},{"location":"python/13_Web_Development.html#installing-fastapi","title":"\u2705 Installing FastAPI","text":"<pre><code>pip install fastapi uvicorn\n</code></pre>"},{"location":"python/13_Web_Development.html#creating-a-simple-fastapi-app","title":"\ud83d\udd39 Creating a Simple FastAPI App","text":"<pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef home():\n    return {\"message\": \"Hello, FastAPI!\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"137.0.0.1\", port=8000)\n</code></pre> <p>\ud83d\udd39 Run the app:</p> <pre><code>uvicorn app:app --reload\n</code></pre> <p>\ud83d\udd39 Visit the automatic API docs:  </p> <ul> <li>Swagger UI: <code>http://137.0.0.1:8000/docs</code></li> <li>ReDoc: <code>http://137.0.0.1:8000/redoc</code></li> </ul>"},{"location":"python/13_Web_Development.html#handling-api-requests","title":"\ud83d\udd39 Handling API Requests","text":"<pre><code>@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int):\n    return {\"item_id\": item_id}\n</code></pre> <p>\u2705 Use Case: Building REST APIs, microservices, AI model deployment.</p>"},{"location":"python/13_Web_Development.html#134-mkdocs-documentation-for-python-projects","title":"13.4 \ud83d\udcd6 MkDocs \u2013 Documentation for Python Projects","text":"<p>MkDocs is a static site generator used for creating project documentation.</p>"},{"location":"python/13_Web_Development.html#installing-mkdocs","title":"\u2705 Installing MkDocs","text":"<pre><code>pip install mkdocs\n</code></pre>"},{"location":"python/13_Web_Development.html#creating-a-new-documentation-site","title":"\ud83d\udd39 Creating a New Documentation Site","text":"<pre><code>mkdocs new my_docs\ncd my_docs\nmkdocs serve\n</code></pre> <p>\ud83d\udd39 View your documentation: Visit <code>http://137.0.0.1:8000/</code> in your browser.</p>"},{"location":"python/13_Web_Development.html#adding-markdown-content","title":"\ud83d\udd39 Adding Markdown Content","text":"<p>Edit <code>docs/index.md</code>:</p> <pre><code># Welcome to My Docs\nThis is a test documentation page.\n</code></pre>"},{"location":"python/13_Web_Development.html#building-the-site","title":"\ud83d\udd39 Building the Site","text":"<pre><code>mkdocs build\n</code></pre> <p>\u2705 Use Case: Project documentation, API documentation, internal guides.</p>"},{"location":"python/13_Web_Development.html#135-streamlit-building-data-driven-web-apps","title":"13.5 \ud83d\udcca Streamlit \u2013 Building Data-Driven Web Apps","text":"<p>Streamlit is a simple framework for building interactive web applications for data visualization.</p>"},{"location":"python/13_Web_Development.html#installing-streamlit","title":"\u2705 Installing Streamlit","text":"<pre><code>pip install streamlit\n</code></pre>"},{"location":"python/13_Web_Development.html#creating-a-simple-streamlit-app","title":"\ud83d\udd39 Creating a Simple Streamlit App","text":"<p>Create <code>app.py</code>:</p> <pre><code>import streamlit as st\n\nst.title(\"Hello, Streamlit!\")\nst.write(\"This is a simple web app using Streamlit.\")\n</code></pre> <p>\ud83d\udd39 Run the app:</p> <pre><code>streamlit run app.py\n</code></pre> <p>\ud83d\udd39 View the app in the browser at <code>http://localhost:8501/</code>.</p>"},{"location":"python/13_Web_Development.html#adding-user-input-and-charts","title":"\ud83d\udd39 Adding User Input and Charts","text":"<pre><code>import pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame(\n    np.random.randn(10, 2),\n    columns=['A', 'B']\n)\n\nst.line_chart(data)\n</code></pre> <p>\u2705 Use Case: Building dashboards, AI model visualizations, interactive reports.</p>"},{"location":"python/13_Web_Development.html#summary","title":"\ud83d\ude80 Summary","text":"Framework Purpose Best For Flask Simple web applications Small apps, dashboards FastAPI High-speed APIs REST APIs, ML model deployment MkDocs Documentation generator API and project documentation Streamlit Data-driven web apps ML dashboards, data visualization"},{"location":"python/13_Web_Development.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Python offers powerful tools for web development, APIs, and data visualization. Would you like step-by-step projects on Flask, FastAPI, MkDocs, or Streamlit? \ud83d\ude80</p>"},{"location":"python/14_Data_Science.html","title":"14 \ud83d\udcca Data Science","text":"<p>Data Science involves analyzing, processing, and visualizing data to derive insights. Python is widely used in Data Science due to its rich ecosystem of libraries. This chapter covers NumPy, Pandas, and Matplotlib, the three core libraries for data manipulation, analysis, and visualization.</p>"},{"location":"python/14_Data_Science.html#141-numpy-numerical-computing","title":"14.1 \ud83d\udd22 NumPy \u2013 Numerical Computing","text":"<p>NumPy (Numerical Python) provides fast, efficient array operations and is the foundation for scientific computing in Python.</p>"},{"location":"python/14_Data_Science.html#installing-numpy","title":"\u2705 Installing NumPy","text":"<pre><code>pip install numpy\n</code></pre>"},{"location":"python/14_Data_Science.html#creating-numpy-arrays","title":"\ud83d\udd39 Creating NumPy Arrays","text":"<pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)  # Output: [1 2 3 4 5]\n</code></pre>"},{"location":"python/14_Data_Science.html#numpy-array-operations","title":"\ud83d\udd39 NumPy Array Operations","text":"<pre><code>a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\nprint(a + b)  # Output: [5 7 9]\nprint(a * b)  # Output: [4 10 18]\nprint(np.dot(a, b))  # Dot product: 1*4 + 2*5 + 3*6 = 32\n</code></pre>"},{"location":"python/14_Data_Science.html#generating-random-data","title":"\ud83d\udd39 Generating Random Data","text":"<pre><code>random_numbers = np.random.rand(5)  # 5 random numbers\nprint(random_numbers)\n</code></pre> <p>\u2705 Use Case: Data preprocessing, handling large numerical datasets efficiently.</p>"},{"location":"python/14_Data_Science.html#142-pandas-data-manipulation","title":"14.2 \ud83d\uddc3\ufe0f Pandas \u2013 Data Manipulation","text":"<p>Pandas simplifies working with structured data (tables, CSVs, JSONs, databases).</p>"},{"location":"python/14_Data_Science.html#installing-pandas","title":"\u2705 Installing Pandas","text":"<pre><code>pip install pandas\n</code></pre>"},{"location":"python/14_Data_Science.html#creating-a-dataframe","title":"\ud83d\udd39 Creating a DataFrame","text":"<pre><code>import pandas as pd\n\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"Salary\": [50000, 60000, 70000]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre>"},{"location":"python/14_Data_Science.html#reading-data-from-csv","title":"\ud83d\udd39 Reading Data from CSV","text":"<pre><code>df = pd.read_csv(\"data.csv\")\nprint(df.head())  # Display first 5 rows\n</code></pre>"},{"location":"python/14_Data_Science.html#filtering-data","title":"\ud83d\udd39 Filtering Data","text":"<pre><code>young_employees = df[df[\"Age\"] &lt; 30]\nprint(young_employees)\n</code></pre>"},{"location":"python/14_Data_Science.html#adding-a-new-column","title":"\ud83d\udd39 Adding a New Column","text":"<pre><code>df[\"Bonus\"] = df[\"Salary\"] * 0.10  # 10% bonus\nprint(df)\n</code></pre> <p>\u2705 Use Case: Handling datasets, cleaning and preparing data for ML models.</p>"},{"location":"python/14_Data_Science.html#143-matplotlib-data-visualization","title":"14.3 \ud83d\udcca Matplotlib \u2013 Data Visualization","text":"<p>Matplotlib is the primary library for plotting graphs and visualizing data.</p>"},{"location":"python/14_Data_Science.html#installing-matplotlib","title":"\u2705 Installing Matplotlib","text":"<pre><code>pip install matplotlib\n</code></pre>"},{"location":"python/14_Data_Science.html#plotting-a-line-graph","title":"\ud83d\udd39 Plotting a Line Graph","text":"<pre><code>import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 20, 25, 30]\n\nplt.plot(x, y, marker=\"o\", linestyle=\"-\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.title(\"Simple Line Plot\")\nplt.show()\n</code></pre>"},{"location":"python/14_Data_Science.html#creating-a-bar-chart","title":"\ud83d\udd39 Creating a Bar Chart","text":"<pre><code>categories = [\"A\", \"B\", \"C\", \"D\"]\nvalues = [10, 20, 15, 25]\n\nplt.bar(categories, values, color=\"blue\")\nplt.title(\"Bar Chart Example\")\nplt.show()\n</code></pre>"},{"location":"python/14_Data_Science.html#creating-a-histogram","title":"\ud83d\udd39 Creating a Histogram","text":"<pre><code>data = np.random.randn(1000)\n\nplt.hist(data, bins=30, color=\"green\")\nplt.title(\"Histogram of Random Data\")\nplt.show()\n</code></pre> <p>\u2705 Use Case: Exploratory Data Analysis (EDA), understanding dataset distributions.</p>"},{"location":"python/14_Data_Science.html#summary","title":"\ud83d\ude80 Summary","text":"Library Purpose Best For NumPy Fast numerical computing Arrays, linear algebra, statistics Pandas Data manipulation CSVs, databases, data wrangling Matplotlib Data visualization Graphs, charts, and plots"},{"location":"python/14_Data_Science.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Mastering NumPy, Pandas, and Matplotlib is essential for data science, machine learning, and AI applications. Would you like practical projects to apply these concepts? \ud83d\ude80</p>"},{"location":"python/15_Machine_Learning_and_AI.html","title":"15 \ud83e\udde0 Machine Learning and AI","text":"<p>Machine Learning (ML) and Artificial Intelligence (AI) enable computers to learn from data and make predictions. Python is the leading language for ML &amp; AI due to its rich ecosystem of libraries.  </p> <p>This chapter covers scikit-learn, TensorFlow, and PyTorch, which are widely used for data preprocessing, building ML models, and deep learning applications.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#151-what-is-machine-learning","title":"15.1 \ud83d\udd0d What is Machine Learning?","text":"<p>Machine Learning is a subset of AI that focuses on teaching computers to learn from data. It is classified into:  </p> Type Description Example Supervised Learning Uses labeled data Spam detection, house price prediction Unsupervised Learning Finds hidden patterns Clustering customers, anomaly detection Reinforcement Learning Learns by trial and error Game AI, robotics"},{"location":"python/15_Machine_Learning_and_AI.html#152-scikit-learn-machine-learning-for-beginners","title":"15.2 \ud83d\udee0\ufe0f Scikit-Learn \u2013 Machine Learning for Beginners","text":"<p>Scikit-learn is a simple and powerful ML library for classification, regression, clustering, and preprocessing.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#installing-scikit-learn","title":"\u2705 Installing Scikit-Learn","text":"<pre><code>pip install scikit-learn\n</code></pre>"},{"location":"python/15_Machine_Learning_and_AI.html#loading-a-dataset","title":"\ud83d\udd39 Loading a Dataset","text":"<pre><code>from sklearn.datasets import load_iris\nimport pandas as pd\n\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\nprint(df.head())\n</code></pre> <p>\u2705 Use Case: Loading datasets for training ML models.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#training-a-simple-classifier","title":"\ud83d\udd39 Training a Simple Classifier","text":"<pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre> <p>\u2705 Use Case: Building predictive models for classification problems.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#153-tensorflow-deep-learning-framework","title":"15.3 \ud83e\udd16 TensorFlow \u2013 Deep Learning Framework","text":"<p>TensorFlow is a powerful framework for building deep learning models.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#installing-tensorflow","title":"\u2705 Installing TensorFlow","text":"<pre><code>pip install tensorflow\n</code></pre>"},{"location":"python/15_Machine_Learning_and_AI.html#creating-a-simple-neural-network","title":"\ud83d\udd39 Creating a Simple Neural Network","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\n# Define a simple model\nmodel = keras.Sequential([\n    keras.layers.Dense(16, activation='relu', input_shape=(4,)),\n    keras.layers.Dense(3, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=8)\n</code></pre> <p>\u2705 Use Case: Training neural networks for AI applications.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#154-pytorch-deep-learning-for-research-ai","title":"15.4 \ud83d\udd25 PyTorch \u2013 Deep Learning for Research &amp; AI","text":"<p>PyTorch is widely used in AI research and deep learning experiments.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#installing-pytorch","title":"\u2705 Installing PyTorch","text":"<pre><code>pip install torch torchvision\n</code></pre>"},{"location":"python/15_Machine_Learning_and_AI.html#creating-a-simple-neural-network_1","title":"\ud83d\udd39 Creating a Simple Neural Network","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the model\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(4, 16)\n        self.fc2 = nn.Linear(16, 3)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return torch.softmax(self.fc2(x), dim=1)\n\nmodel = SimpleNN()\n</code></pre> <p>\u2705 Use Case: Developing custom AI models for image recognition, NLP, and robotics.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#155-model-evaluation-and-metrics","title":"15.5 \ud83d\udcca Model Evaluation and Metrics","text":"<p>Measuring model performance is critical in ML &amp; AI.</p> Metric Use Case Accuracy Classification problems Precision &amp; Recall Imbalanced datasets (e.g., fraud detection) Mean Squared Error (MSE) Regression models ROC Curve Evaluating classification models"},{"location":"python/15_Machine_Learning_and_AI.html#evaluating-a-model","title":"\ud83d\udd39 Evaluating a Model","text":"<pre><code>from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))\n</code></pre> <p>\u2705 Use Case: Assessing model performance before deployment.</p>"},{"location":"python/15_Machine_Learning_and_AI.html#summary","title":"\ud83d\ude80 Summary","text":"Library Purpose Best For Scikit-learn Traditional ML models Classification, regression, clustering TensorFlow Deep learning Neural networks, AI applications PyTorch AI research Custom AI models, NLP, computer vision"},{"location":"python/15_Machine_Learning_and_AI.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Machine Learning and AI enable automation, predictions, and decision-making in various industries.  </p> <p>Would you like hands-on projects on ML &amp; AI? \ud83d\ude80</p>"},{"location":"python/16_Predictive_Modeling.html","title":"16 \ud83d\udd2e Predictive Modeling","text":"<p>Predictive modeling is the core of machine learning, where we use historical data to make future predictions. This chapter covers data preprocessing, feature engineering, model selection, evaluation, and deployment for predictive analytics.  </p>"},{"location":"python/16_Predictive_Modeling.html#161-understanding-predictive-modeling","title":"16.1 \ud83d\udcca Understanding Predictive Modeling","text":"<p>Predictive modeling involves training a machine learning model to make predictions based on input data. It follows these steps:</p> <p>1\ufe0f\u20e3 Data Collection \u2013 Gathering relevant data 2\ufe0f\u20e3 Data Preprocessing \u2013 Cleaning and preparing data 3\ufe0f\u20e3 Feature Engineering \u2013 Selecting important attributes 4\ufe0f\u20e3 Model Selection \u2013 Choosing the best algorithm 5\ufe0f\u20e3 Training &amp; Evaluation \u2013 Assessing model accuracy 6\ufe0f\u20e3 Prediction &amp; Deployment \u2013 Using the model for real-world applications  </p>"},{"location":"python/16_Predictive_Modeling.html#162-data-preprocessing","title":"16.2 \ud83d\udee0\ufe0f Data Preprocessing","text":"<p>Before training a model, we clean and transform data for better accuracy.</p>"},{"location":"python/16_Predictive_Modeling.html#handling-missing-values","title":"\u2705 Handling Missing Values","text":"<pre><code>import pandas as pd\n\ndf = pd.read_csv(\"data.csv\")\ndf.fillna(df.mean(), inplace=True)  # Replace missing values with mean\n</code></pre> <p>\u2705 Use Case: Cleaning noisy datasets before modeling.</p>"},{"location":"python/16_Predictive_Modeling.html#encoding-categorical-variables","title":"\ud83d\udd39 Encoding Categorical Variables","text":"<pre><code>df = pd.get_dummies(df, columns=[\"Category\"])\n</code></pre> <p>\u2705 Use Case: Converting text labels into numerical values for ML models.</p>"},{"location":"python/16_Predictive_Modeling.html#163-feature-engineering","title":"16.3 \ud83c\udfd7\ufe0f Feature Engineering","text":"<p>Feature engineering improves model accuracy by creating meaningful input variables.</p>"},{"location":"python/16_Predictive_Modeling.html#scaling-features","title":"\u2705 Scaling Features","text":"<pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df)\n</code></pre> <p>\u2705 Use Case: Normalizing features for models like Logistic Regression &amp; SVM.</p>"},{"location":"python/16_Predictive_Modeling.html#selecting-important-features","title":"\ud83d\udd39 Selecting Important Features","text":"<pre><code>from sklearn.feature_selection import SelectKBest, f_classif\n\nX_new = SelectKBest(score_func=f_classif, k=5).fit_transform(X, y)\n</code></pre> <p>\u2705 Use Case: Choosing the most relevant features for better predictions.</p>"},{"location":"python/16_Predictive_Modeling.html#164-choosing-the-right-model","title":"16.4 \ud83e\udd16 Choosing the Right Model","text":"<p>Different algorithms are suited for different predictive tasks.</p> Model Type Algorithm Use Case Classification Logistic Regression, Random Forest Fraud detection, spam filtering Regression Linear Regression, XGBoost Stock price prediction, sales forecasting Time Series ARIMA, LSTM Weather forecasting, demand prediction"},{"location":"python/16_Predictive_Modeling.html#training-a-predictive-model","title":"\u2705 Training a Predictive Model","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n</code></pre> <p>\u2705 Use Case: Training an AI model to predict future outcomes.</p>"},{"location":"python/16_Predictive_Modeling.html#165-model-evaluation-performance-metrics","title":"16.5 \ud83d\udcca Model Evaluation &amp; Performance Metrics","text":"<p>Evaluating model accuracy ensures reliable predictions.</p>"},{"location":"python/16_Predictive_Modeling.html#checking-accuracy","title":"\u2705 Checking Accuracy","text":"<pre><code>from sklearn.metrics import accuracy_score\n\ny_pred = model.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"python/16_Predictive_Modeling.html#confusion-matrix","title":"\ud83d\udd39 Confusion Matrix","text":"<pre><code>from sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\n</code></pre> <p>\u2705 Use Case: Measuring classification model performance.</p>"},{"location":"python/16_Predictive_Modeling.html#166-making-predictions","title":"16.6 \ud83d\udd2e Making Predictions","text":"<p>After training, we use the model to predict real-world data.</p> <pre><code>new_data = [[5.1, 3.5, 1.4, 0.2]]\nprediction = model.predict(new_data)\nprint(\"Predicted Class:\", prediction)\n</code></pre> <p>\u2705 Use Case: Predicting customer behavior, stock prices, or disease diagnosis.</p>"},{"location":"python/16_Predictive_Modeling.html#167-deploying-the-model","title":"16.7 \ud83d\ude80 Deploying the Model","text":"<p>A trained model can be deployed using Flask, FastAPI, or Streamlit.</p>"},{"location":"python/16_Predictive_Modeling.html#saving-and-loading-the-model","title":"\u2705 Saving and Loading the Model","text":"<pre><code>import joblib\n\njoblib.dump(model, \"model.pkl\")  # Save model\nloaded_model = joblib.load(\"model.pkl\")  # Load model\n</code></pre> <p>\u2705 Use Case: Deploying AI models into production systems.</p>"},{"location":"python/16_Predictive_Modeling.html#summary","title":"\ud83d\ude80 Summary","text":"Step Description Data Preprocessing Cleaning and transforming data Feature Engineering Selecting the most important variables Model Selection Choosing the best ML algorithm Training &amp; Evaluation Assessing model performance Prediction &amp; Deployment Using the model for real-world applications"},{"location":"python/16_Predictive_Modeling.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Predictive modeling is widely used in finance, healthcare, and business intelligence.  </p> <p>Would you like a hands-on project to build a real-world predictive model? \ud83d\ude80</p>"},{"location":"python/17_Databases.html","title":"17 \ud83c\udfdb\ufe0f Databases","text":"<p>Databases are essential for storing, managing, and retrieving structured data. Python provides built-in and third-party libraries for working with SQL and NoSQL databases.</p> <p>This chapter covers SQLite, PostgreSQL, MySQL, and MongoDB, along with how to perform CRUD operations (Create, Read, Update, Delete).</p>"},{"location":"python/17_Databases.html#171-types-of-databases","title":"17.1 \ud83d\uddc2\ufe0f Types of Databases","text":"Database Type Example Best For Relational (SQL) SQLite, MySQL, PostgreSQL Structured data with relationships NoSQL (Document-based) MongoDB, Firebase Unstructured or semi-structured data"},{"location":"python/17_Databases.html#172-working-with-sqlite-lightweight-sql-database","title":"17.2 \ud83c\udfd7\ufe0f Working with SQLite (Lightweight SQL Database)","text":"<p>SQLite is a lightweight, file-based SQL database that comes pre-installed with Python.</p>"},{"location":"python/17_Databases.html#connecting-to-sqlite","title":"\u2705 Connecting to SQLite","text":"<pre><code>import sqlite3\n\nconn = sqlite3.connect(\"database.db\")  # Creates/opens a database file\ncursor = conn.cursor()\n</code></pre>"},{"location":"python/17_Databases.html#creating-a-table","title":"\ud83d\udd39 Creating a Table","text":"<pre><code>cursor.execute('''\n    CREATE TABLE IF NOT EXISTS users (\n        id INTEGER PRIMARY KEY,\n        name TEXT,\n        age INTEGER\n    )\n''')\nconn.commit()\n</code></pre>"},{"location":"python/17_Databases.html#inserting-data","title":"\ud83d\udd39 Inserting Data","text":"<pre><code>cursor.execute(\"INSERT INTO users (name, age) VALUES (?, ?)\", (\"Alice\", 25))\nconn.commit()\n</code></pre>"},{"location":"python/17_Databases.html#reading-data","title":"\ud83d\udd39 Reading Data","text":"<pre><code>cursor.execute(\"SELECT * FROM users\")\nprint(cursor.fetchall())  # Output: [(1, 'Alice', 25)]\n</code></pre> <p>\u2705 Use Case: Small applications, local storage, prototyping.</p>"},{"location":"python/17_Databases.html#173-working-with-postgresql-mysql-sql-databases-for-large-applications","title":"17.3 \ud83c\udfe6 Working with PostgreSQL &amp; MySQL (SQL Databases for Large Applications)","text":""},{"location":"python/17_Databases.html#installing-postgresqlmysql-connector","title":"\u2705 Installing PostgreSQL/MySQL Connector","text":"<pre><code>pip install psycopg2  # PostgreSQL\npip install mysql-connector-python  # MySQL\n</code></pre>"},{"location":"python/17_Databases.html#connecting-to-postgresql","title":"\ud83d\udd39 Connecting to PostgreSQL","text":"<pre><code>import psycopg2\n\nconn = psycopg2.connect(\n    dbname=\"mydb\",\n    user=\"postgres\",\n    password=\"mypassword\",\n    host=\"localhost\"\n)\ncursor = conn.cursor()\n</code></pre>"},{"location":"python/17_Databases.html#querying-a-postgresql-table","title":"\ud83d\udd39 Querying a PostgreSQL Table","text":"<pre><code>cursor.execute(\"SELECT * FROM users\")\nrows = cursor.fetchall()\nfor row in rows:\n    print(row)\n</code></pre> <p>\u2705 Use Case: Enterprise applications, web applications, analytics.</p>"},{"location":"python/17_Databases.html#174-working-with-mongodb-nosql-database","title":"17.4 \ud83c\udf43 Working with MongoDB (NoSQL Database)","text":"<p>MongoDB stores data in JSON-like documents.</p>"},{"location":"python/17_Databases.html#installing-mongodb-driver","title":"\u2705 Installing MongoDB Driver","text":"<pre><code>pip install pymongo\n</code></pre>"},{"location":"python/17_Databases.html#connecting-to-mongodb","title":"\ud83d\udd39 Connecting to MongoDB","text":"<pre><code>import pymongo\n\nclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\ndb = client[\"mydatabase\"]\ncollection = db[\"users\"]\n</code></pre>"},{"location":"python/17_Databases.html#inserting-a-document","title":"\ud83d\udd39 Inserting a Document","text":"<pre><code>user = {\"name\": \"Alice\", \"age\": 25}\ncollection.insert_one(user)\n</code></pre>"},{"location":"python/17_Databases.html#retrieving-data","title":"\ud83d\udd39 Retrieving Data","text":"<pre><code>for user in collection.find():\n    print(user)\n</code></pre> <p>\u2705 Use Case: Big data, real-time analytics, IoT applications.</p>"},{"location":"python/17_Databases.html#175-crud-operations-in-databases","title":"17.5 \ud83d\udd04 CRUD Operations in Databases","text":"Operation SQL Example MongoDB Example Create <code>INSERT INTO users VALUES (1, 'Alice', 25);</code> <code>collection.insert_one({\"name\": \"Alice\", \"age\": 25})</code> Read <code>SELECT * FROM users;</code> <code>collection.find({})</code> Update <code>UPDATE users SET age=30 WHERE name='Alice';</code> <code>collection.update_one({\"name\": \"Alice\"}, {\"$set\": {\"age\": 30}})</code> Delete <code>DELETE FROM users WHERE name='Alice';</code> <code>collection.delete_one({\"name\": \"Alice\"})</code> <p>\u2705 Use Case: Building full-stack web applications, managing structured/unstructured data.</p>"},{"location":"python/17_Databases.html#summary","title":"\ud83d\ude80 Summary","text":"Database Best For SQLite Small applications, local storage PostgreSQL/MySQL Large-scale, structured data MongoDB Flexible, unstructured data"},{"location":"python/17_Databases.html#final-thoughts","title":"\ud83d\udd1a Final Thoughts","text":"<p>Databases are essential for storing and managing data in Python applications. Would you like a real-world project on integrating databases with Python? \ud83d\ude80</p>"},{"location":"python/17x_Examples.html","title":"Real-World Examples","text":""},{"location":"python/best_practice.html","title":"Best Practice","text":""},{"location":"python/best_practice.html#production-grade-python-code-styles-every-python-developer-uses","title":"Production-Grade Python Code Styles Every Python Developer Uses","text":"<p>Python has grown into one of the most popular and versatile programming languages in the world. Its clean and readable syntax has made it a favorite for a wide range of applications, from web development to data science. However, while the language itself is simple, writing production-grade Python code requires attention to detail, style, and best practices to ensure maintainability, scalability, and readability. In this article, we\u2019ll explore 20 Python code styles that every developer should follow to ensure their code meets the high standards required for production environments.</p>"},{"location":"python/best_practice.html#1-follow-pep-8-guidelines","title":"1. Follow PEP 8 Guidelines","text":"<p>The Python Enhancement Proposal 8 (PEP 8) is the official style guide for Python code. It covers conventions such as indentation, line length, blank lines, imports, and naming conventions. Following PEP 8 ensures that your code is consistent and readable for other developers who may work on the same project. Some key points to remember include:</p> <p>Use 4 spaces per indentation level (avoid tabs). Limit lines to 79 characters for code and 72 characters for docstrings. Keep a single blank line between functions and methods. By adhering to PEP 8, you ensure that your Python code follows a universally accepted format, making collaboration and code reviews smoother.</p>"},{"location":"python/best_practice.html#2-use-descriptive-variable-and-function-names","title":"2. Use Descriptive Variable and Function Names","text":"<p>Choosing meaningful and descriptive names for variables and functions is crucial for code readability and maintainability. Avoid vague names like temp, data, or foo. Instead, opt for names that clearly convey their purpose. For example:</p> <p>Use num_items instead of x. Use get_user_info() instead of do_stuff(). A good rule of thumb is to think about how someone unfamiliar with your code would interpret the name.</p>"},{"location":"python/best_practice.html#3-write-clear-and-concise-docstrings","title":"3. Write Clear and Concise Docstrings","text":"<p>Documentation is essential for any code, and Python encourages using docstrings to document functions, classes, and modules. Always write clear, concise docstrings for your functions and methods to explain what they do, what parameters they accept, and what they return.</p> <p>For example:</p> <p>def add(a, b):     \"\"\"     Add two numbers.</p> <pre><code>Args:\n    a (int): The first number.\n    b (int): The second number.\n\nReturns:\n    int: The sum of a and b.\n\"\"\"\nreturn a + b\n</code></pre> <p>Following the standard format and providing comprehensive docstrings can make your code much easier to understand and maintain.</p>"},{"location":"python/best_practice.html#4-use-type-hinting","title":"4. Use Type Hinting","text":"<p>Type hinting allows you to explicitly declare the types of variables, function parameters, and return values. This feature improves code clarity, helps catch errors early, and assists IDEs with better code suggestions. For example:</p> <p>def greet(name: str) -&gt; str:     return f\"Hello, {name}!\" Type hinting improves code readability and reduces the chances of runtime errors.</p>"},{"location":"python/best_practice.html#5-use-list-comprehensions-and-generator-expressions","title":"5. Use List Comprehensions and Generator Expressions","text":"<p>Python\u2019s list comprehensions and generator expressions allow you to write more concise and readable code for generating lists or iterating over items. For example, instead of writing:</p> <p>squared_numbers = [] for num in numbers:     squared_numbers.append(num ** 2) You can use a list comprehension:</p> <p>squared_numbers = [num ** 2 for num in numbers] This approach is more compact, readable, and Pythonic. It can be extended to generator expressions when you want to save memory by not storing the entire list in memory.</p>"},{"location":"python/best_practice.html#6-leverage-pythons-built-in-functions","title":"6. Leverage Python\u2019s Built-in Functions","text":"<p>Python comes with a rich set of built-in functions, such as map(), filter(), reduce(), and zip(), which can make your code more efficient and readable. For example, instead of writing:</p> <p>squared_numbers = [] for num in numbers:     squared_numbers.append(num ** 2) You can use the map() function:</p> <p>squared_numbers = list(map(lambda num: num ** 2, numbers)) These functions often lead to more concise, readable, and functional code.</p>"},{"location":"python/best_practice.html#7-keep-functions-small-and-focused","title":"7. Keep Functions Small and Focused","text":"<p>A good practice in writing production-grade code is to keep functions small and focused on a single task. A function should do one thing and do it well. Avoid making functions too large or performing multiple unrelated tasks, as this can make your code harder to understand, test, and maintain.</p>"},{"location":"python/best_practice.html#8-handle-exceptions-properly","title":"8. Handle Exceptions Properly","text":"<p>Robust error handling is essential in production environments. Use try and except blocks to catch potential exceptions, but make sure to catch specific exceptions rather than using a generic except clause. For example:</p> <p>try:     result = 10 / divisor except ZeroDivisionError:     print(\"Divisor cannot be zero\") Additionally, make sure to log errors so they can be traced later.</p>"},{"location":"python/best_practice.html#9-use-logging-for-debugging-and-monitoring","title":"9. Use Logging for Debugging and Monitoring","text":"<p>Instead of relying on print() statements for debugging, use Python\u2019s built-in logging module. It provides a flexible framework for emitting logs at different severity levels, such as DEBUG, INFO, WARNING, ERROR, and CRITICAL. For example:</p> <p>import logging logging.basicConfig(level=logging.DEBUG) def calculate_sum(a, b):     logging.debug(f\"Adding {a} and {b}\")     return a + b Logging is essential for production environments because it provides better traceability and control over what\u2019s happening in your application.</p>"},{"location":"python/best_practice.html#10-use-virtual-environments","title":"10. Use Virtual Environments","text":"<p>A virtual environment ensures that your project\u2019s dependencies are isolated from other projects. This is crucial in production environments, as it prevents version conflicts between different Python packages. Use venv to create a virtual environment for your project:</p> <p>python3 -m venv venv Activate it with:</p> <p>source venv/bin/activate  # On Mac/Linux venv\\Scripts\\activate     # On Windows</p>"},{"location":"python/best_practice.html#11-use-dependency-management-tools","title":"11. Use Dependency Management Tools","text":"<p>Managing project dependencies is crucial for ensuring consistency and avoiding conflicts. Use tools like pip, pipenv, or poetry to manage your Python dependencies. The requirements.txt file or pyproject.toml helps maintain a record of all dependencies for reproducibility.</p> <p>For example, to freeze dependencies in a requirements.txt file, you can use:</p> <p>pip freeze &gt; requirements.txt</p>"},{"location":"python/best_practice.html#12-write-unit-tests","title":"12. Write Unit Tests","text":"<p>Unit testing is a crucial part of writing production-grade code. Writing automated tests helps ensure your code works as expected and prevents bugs from slipping into production. Python\u2019s built-in unittest module makes it easy to write unit tests for your functions.</p> <p>Example:</p> <p>import unittest class TestMathFunctions(unittest.TestCase):     def test_addition(self):         self.assertEqual(add(1, 2), 3) if name == 'main':     unittest.main() Test-driven development (TDD) can greatly improve the stability and quality of your code.</p>"},{"location":"python/best_practice.html#13-ensure-code-is-modular","title":"13. Ensure Code is Modular","text":"<p>Modular code is easier to maintain and scale. Split your code into smaller, reusable modules with a single responsibility. Each module should have one task and should be able to interact with other modules through well-defined interfaces. This improves readability and helps with testing.</p>"},{"location":"python/best_practice.html#14-use-list-and-dict-unpacking","title":"14. Use List and Dict Unpacking","text":"<p>Python allows you to unpack lists and dictionaries in a concise and readable way. For example, when working with tuples:</p> <p>x, y = (1, 2) For dictionaries:</p> <p>data = {\"name\": \"Alice\", \"age\": 30} name, age = data.values() Unpacking can make your code more compact and expressive.</p>"},{"location":"python/best_practice.html#15-avoid-using-global-variables","title":"15. Avoid Using Global Variables","text":"<p>Global variables can make your code difficult to debug and maintain. Instead, pass data explicitly between functions or use classes to encapsulate related functionality. When global state is absolutely necessary, ensure it\u2019s well-documented and controlled.</p>"},{"location":"python/best_practice.html#16-use-decorators-to-enhance-functionality","title":"16. Use Decorators to Enhance Functionality","text":"<p>Python decorators allow you to add functionality to functions or methods in a clean and reusable way. For example, a logging decorator can be used to log function calls:</p> <p>def log_function_call(func):     def wrapper(args, kwargs):         print(f\"Calling {func.name} with arguments {args} and {kwargs}\")         return func(args,**kwargs)     return wrapper Decorators enhance the readability and reusability of your code by abstracting common logic.</p>"},{"location":"python/best_practice.html#17-use-f-strings-for-string-formatting","title":"17. Use f-strings for String Formatting","text":"<p>f-strings, introduced in Python 3.6, are the recommended way to format strings due to their readability and performance. Instead of using the older format() method or % operator, use:</p> <p>name = \"Alice\" greeting = f\"Hello, {name}!\" This is not only more concise but also faster than the alternatives.</p>"},{"location":"python/best_practice.html#18-avoid-premature-optimization","title":"18. Avoid Premature Optimization","text":"<p>While it\u2019s tempting to optimize your code for performance, premature optimization can lead to complex, unreadable code. Focus on writing clean, correct code first, and optimizing only when necessary, using profiling tools to identify bottlenecks.</p>"},{"location":"python/best_practice.html#19-write-code-with-scalability-in-mind","title":"19. Write Code with Scalability in Mind","text":"<p>When writing production code, always think about scalability. This includes considering database query efficiency, memory usage, concurrency, and network performance. Always aim for code that can scale as traffic, data, or users increase.</p>"},{"location":"python/best_practice.html#20-adopt-continuous-integration-ci-and-continuous-delivery-cd","title":"20. Adopt Continuous Integration (CI) and Continuous Delivery (CD)","text":"<p>Continuous Integration (CI) and Continuous Delivery (CD) pipelines automate the process of integrating code changes, testing them, and deploying them. Use tools like GitHub Actions, Jenkins, or Travis CI to automate the testing and deployment of your code. This ensures that bugs are caught early and your code is always ready for production.</p>"},{"location":"python/best_practice.html#conclusion","title":"Conclusion","text":"<p>Writing production-grade Python code requires adherence to best practices that prioritize readability, maintainability, and performance. By following these 20 coding styles, you can ensure that your Python code is high-quality, scalable, and easy to work with in a professional setting. Whether you\u2019re working solo or in a team, these practices will make your development process smoother and your codebase more reliable.</p>"},{"location":"python/pandas.html","title":"Pandas","text":"<p>A beginner-friendly guide to Pandas for data analysis.</p>"},{"location":"python/yfinance.html","title":"Yfinance","text":"<p>How to use <code>yfinance</code> to fetch and analyze stock market data.</p>"},{"location":"services/data_science.html","title":"Data Science","text":"<p>An overview of data science techniques and methodologies used in Dbnostix.</p>"},{"location":"services/database_infrastructure.html","title":"Database Infrastructure","text":"<p>Details on database infrastructure solutions and best practices.</p>"},{"location":"services/large_language_model.html","title":"Large Language Model","text":"<p>An explanation of large language models and services related to them.</p>"},{"location":"services/predictive_modeling.html","title":"Predictive Modeling","text":"<p>Insights into predictive modeling and its applications.</p>"}]}